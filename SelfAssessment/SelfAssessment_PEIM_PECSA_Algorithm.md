# Concepts, Features, Types and Pros and Cons

Organize concepts, features, types and Pros and Cons

## Algorithm

- 알고리즘 정의와 특징을 설명
    - 정의: 알고리즘(Algorithm)은 주어진 문제를 해결하기 위한 일련의 절차나 규칙의 집합
    - 특징
        - 명확성 (Definiteness): 각 단계는 정확하고 모호함이 없어야 한다
        - 입력 (Input): 0개 이상의 입력이 존재한다
        - 출력 (Output): 최소한 1개 이상의 결과(출력)가 있어야 한다
        - 유한성 (Finiteness): 유한한 단계 내에 종료해야 한다.
        - 효율성 (Effectiveness): 모든 연산이 기계적으로 수행 가능해야 한다.

- 알고리즘의 성능을 평가하는 기준
    - 시간 복잡도(Time Complexity)
        - 알고리즘이 수행하는 연산의 개수를 측정하여 입력 크기에 따른 실행 시간 증가율을 평가한다.
        - 일반적으로 Big-O 표기법을 사용해 표현한다.
    - 공간 복잡도(Space Complexity)
        - 알고리즘이 실행될 때 필요한 메모리 사용량을 평가한다.
        - 추가적인 데이터 구조(배열, 리스트, 스택 등)가 필요한 경우 공간 복잡도가 증가할 수 있다.

- 시간 복잡도(Time Complexity)와 공간 복잡도(Space Complexity)의 차이점 설명
    - 시간 복잡도는 알고리즘이 실행되는 시간의 효율성을 평가하는 개념
        - 입력 크기(n)가 증가할 때 연산 횟수가 어떻게 변하는지 분석한다.
    - 공간 복잡도는 알고리즘이 실행되는 동안 사용하는 메모리 공간을 분석하는 개념이다.
    - 예시
        - 선형 탐색(Linear Search)
            - 시간복잡도: O(n), 공간복잡도: O(1)
        - 이진 탐색(Binary Search)
            - 시간복잡도: O(logn), 공간복잡도: O(1)
        - 병합 정렬(Merge Sort)
            - 시간복잡도: O(nlogn), 공간복잡도: O(n)
        - 퀵 정렬(Quick Sort)
            - 시간복잡도: O(nlogn)(평균), 공간복잡도: O(logn)

- Big-O 표기법과 주요 Big-O 표기법 예제 설명
    - Big-O 표기법: 알고리즘의 시간 복잡도를 표현하는 수학적 표기법으로, 입력 크기(n)에 따른 실행 시간 증가율을 나타낸다.
    - 주요 Big-O 표기법 예제
        - O(1) / 상수 시간 / 배열 인덱스 접근
        - O(log n) / 로그 시간 / 이진 탐색
        - O(n) / 선형 시간 / 선형 탐색
        - O(n log n) / 로그 선형 시간 / 병합 정렬, 퀵 정렬(평균)
        - O(n²) / 이차 시간 / 버블 정렬, 삽입 정렬
        - O(2ⁿ) / 지수 시간 / 피보나치 수열(재귀)

- Best case, Worst case, Average case의 개념과 차이점을 설명
    - Best Case
        - 가장 이상적인 상황에서 알고리즘이 실행되는 경우
        - 예: 이진 탐색(Binary Search)에서 찾으려는 값이 첫 번째 요소에 있는 경우 → O(1)
            - 정렬된 배열에서 임의의 중간값을 찾고자 하는 값과 비교하여 좌측 또는 우측을 대상으로 탐색을 하는 알고리즘
            - 임의의 중간값 선정 후 찾고자 하는 값 비교 과정을 계속 반복한다
    - Worst Case
        - 가장 나쁜 상황에서 알고리즘이 실행되는 경우
        - 예: 선형 탐색(Linear Search)에서 찾는 값이 배열의 마지막 요소에 있는 경우 → O(n)
            - 배열이나 리스트에서 특정 값을 찾기 위해 처음부터 끝까지 하나씩 순차적으로 확인하는 탐색 알고리즘
    - Average Case
        - 모든 가능한 입력에 대한 평균적인 수행 시간
        - 예: 퀵 정렬(Quick Sort)에서 랜덤한 피벗 선택 → O(n log n)

- 시간 복잡도가 O(n)인 알고리즘과 O(n²)인 알고리즘의 차이를 실제 사례와 함께 설명
    - O(n) 알고리즘 예시 (선형 탐색)
        ```python
        def linear_search(arr, target):
        for i in range(len(arr)):
            if arr[i] == target:
                return i
        return -1
        ```
    - O(n²) 알고리즘 예시 (버블 정렬)
        ```python
        def bubble_sort(arr):
            n = len(arr) # example: [5, 4, 1, 2, 9]
            for i in range(n): # n = 5
                for j in range(0, n - i - 1): # 0, 4, 0, 3, 0, 2
                    if arr[j] > arr[j + 1]: #arr[0] > arr[1]
                        arr[j], arr[j + 1] = arr[j + 1], arr[j] #arr[0], arr[1] = arr[1], arr[0], swap
        ```
        - 알고리즘 분석
            - 예: [5,4,1,2,9]라 가정할 때 전체 아이템 개수만큼 반복문을 돈다.
            - Set 1 (0,4), 1세트가 끝나면 가장 큰 수는 배열의 맨뒤에 위치하게 된다.
                - 1) [4,5,1,2,9]
                - 2) [4,1,5,2,9]
                - 3) [4,1,2,5,9]
                - 4) [4,1,2,5,9] 
            - Set 2 (0,3)
                - 1) [1,4,2,5,9]
                - 2) [1,2,4,5,9]
                - 3) [1,2,4,5,9]
            - Set 3 (0,2)
            - Set 4 (0,1)
    - 차이점
        - O(n) 알고리즘은 데이터 크기가 증가해도 비교적 빠르게 실행되지만, O(n²) 알고리즘은 입력 크기가 커질수록 실행 속도가 매우 느려진다.

- 분할 정복(Divide and Conquer) 기법과 주요 예제 제시
    - 개요
        - 큰 문제를 작은 문제로 나누어 해결한 후, 이를 합쳐서 최종 결과를 얻는 알고리즘 설계 기법
    - 핵심 개념
        - 분할(Divide): 문제를 더 작은 부분 문제(subproblems)로 나눈다
        - 정복(Conquer): 나눠진 작은 문제를 해결한다
        - 병합(Combine): 해결한 작은 문제들을 합쳐서 최종 해결책을 도출한다
        - 이 방식은 재귀(Recursion)를 많이 활용하며, 복잡한 문제를 해결하는 데 자주 사용됨
    - 분할정복 사용되는 대표적인 알고리즘
        - 병합 정렬 (Merge Sort) / O(n log n) / 배열을 반으로 나눈 후 정렬하여 합침
        - 퀵 정렬 (Quick Sort) / 평균 O(n log n) / 피벗을 기준으로 분할 후 정렬
        - 이진 탐색 (Binary Search)	/ O(log n) / 정렬된 배열을 반씩 나누며 탐색
        - 최근접 점 쌍 문제 (Closest Pair of Points) / O(n log n) / 좌표평면에서 가장 가까운 두 점을 찾음
        - 스트라센 행렬 곱셈 (Strassen’s Matrix Multiplication) / O(n².⁸) / 일반 행렬 곱셈보다 빠름
        - FFT(Fast Fourier Transform) / O(n log n) / 푸리에 변환을 빠르게 수행
    - 분할 정복(DC) 기법의 장점과 단점
        - 장점
            - 복잡한 문제 해결 가능
                - 정렬, 탐색, 행렬 연산 등에서 효율적인 해결 방법을 제공함.
            - 병렬 처리(Parallel Processing)에 유리함
                - 작은 문제를 독립적으로 해결할 수 있어 병렬 연산이 가능함.
            - 빠른 실행 속도
                - 정렬 알고리즘(병합 정렬, 퀵 정렬) 등에서 O(n log n) 성능을 제공.
        - 단점
            - 재귀(Recursion)로 인한 함수 호출 비용 발생
                - 스택 오버플로우(Stack Overflow) 위험 있음
            - 추가적인 메모리 사용
                - 병합 정렬처럼 새로운 배열을 사용하면 O(n) 추가 공간이 필요할 수도 있음.
            - 피벗 선택이 중요한 경우(퀵 정렬)
                - 잘못된 피벗을 선택하면 최악의 경우 O(n²) 성능이 될 수 있음.
    - 분할 정복(DC) 주요 예제
        - 병합정렬
            - 정의: 병합 정렬은 배열을 계속 반으로 나눈 후(Divide), 각각 정렬한 후 병합(Conquer & Combine)하는 정렬 알고리즘
            - 병합 정렬의 과정
                - 배열을 절반으로 계속 나눈다.
                - 크기가 1이 되면 정렬된 상태로 간주한다.
                - 나눠진 두 개의 정렬된 배열을 하나로 합친다(병합).
            - 병합 정렬 코드 예시
                ```python
                def merge_sort(arr):
                    if len(arr) <= 1:
                        return arr  # 원소가 1개 이하이면 그대로 반환

                    # 1. 분할 (Divide)
                    mid = len(arr) // 2
                    # 0부터 mid-1까지의 요소, 0,1,2
                    left_half = merge_sort(arr[:mid])
                    # mid부터 끝까지의 요소, 3,4,5,6
                    right_half = merge_sort(arr[mid:])
                    # 2. 정복 & 병합 (Conquer & Combine)
                    return merge(left_half, right_half)

                def merge(left, right):
                    result = []
                    i = j = 0

                    # 두 리스트를 정렬된 순서대로 병합
                    while i < len(left) and j < len(right):
                        if left[i] < right[j]:
                            result.append(left[i])
                            i += 1
                        else:
                            result.append(right[j])
                            j += 1

                    # 남은 원소 추가
                    result.extend(left[i:])
                    result.extend(right[j:])
                    return result

                    # 사용 예제 (Entry Point)
                    arr = [38, 27, 43, 3, 9, 82, 10]
                    sorted_arr = merge_sort(arr)
                    print(sorted_arr)
                ```
        - 퀵정렬
            - 정의: 퀵 정렬은 피벗(Pivot)을 설정하고, 이를 기준으로 작은 값과 큰 값으로 나눈 뒤 각각 정렬하는 방식
            - 퀵 정렬의 과정
                - 피벗(Pivot)을 설정한다.
                - 피벗보다 작은 값은 왼쪽, 큰 값은 오른쪽으로 정렬한다.
                - 각각을 다시 퀵 정렬로 정렬한다.
            - 퀵 정렬 코드 예시
                ```python
                def quick_sort(arr):
                if len(arr) <= 1:
                    return arr  # 원소가 1개 이하이면 그대로 반환

                pivot = arr[len(arr) // 2]  # 피벗 설정 (중간값)
                left = [x for x in arr if x < pivot]  # 피벗보다 작은 값
                middle = [x for x in arr if x == pivot]  # 피벗과 같은 값
                right = [x for x in arr if x > pivot]  # 피벗보다 큰 값

                return quick_sort(left) + middle + quick_sort(right)  # 재귀 호출

                # 사용 예제 (Entry Point)
                arr = [38, 27, 43, 3, 9, 82, 10]
                sorted_arr = quick_sort(arr)
                print(sorted_arr)

                # 참고 List Comprehension 다른 표현
                [x for x in arr if x < pivot]

                result = []
                for x in arr:
                    if x < pivot:
                        result.append(x)
                ```
        - 이진탐색
            - 정의: 이진 탐색은 정렬된 배열에서 탐색 범위를 반씩 줄여가며 값을 찾는 알고리즘 (반씩 줄이는 과정을 반복)
            - 이진 탐색의 과정
                - 배열의 중간값과 찾으려는 값을 비교한다.
                - 같으면 찾은 것이고, 크면 왼쪽 부분을 탐색, 작으면 오른쪽 부분을 탐색한다.
                - 이 과정을 찾을 때까지 반복한다.
            - 이진탐색 코드 예시
                ```python
                # 기준 인덱스를 계속 변화시키면서 값을 찾는 방식
                def binary_search(arr, target):
                    left, right = 0, len(arr) - 1
                    while left <= right:
                        mid = (left + right) // 2  # 중간값 찾기
                        if arr[mid] == target:
                            return mid  # 찾음
                        elif arr[mid] < target:
                            left = mid + 1  # 오른쪽 탐색
                        else:
                            right = mid - 1  # 왼쪽 탐색

                    return -1  # 못 찾음

                # 사용 예제 (정렬된 배열 필요, Entry Point)
                arr = [3, 9, 10, 27, 38, 43, 82]
                target = 27
                result = binary_search(arr, target)
                ```

- 동적 계획법(Dynamic Programming) 정의와 대표적 예제 제시
    - 정의: 큰 문제를 작은 문제로 나누어 해결한 후, 그 결과를 저장하여 동일한 문제를 다시 풀지 않도록 하는 최적화 기법
    - 핵심 개념
        - 최적 부분 구조 (Optimal Substructure)
            - 큰 문제를 작은 문제로 나누어 해결할 수 있어야 한다.
            - 예: 피보나치 수열에서 F(n) = F(n-1) + F(n-2)
        - 중복되는 부분 문제 (Overlapping Subproblems)
            - 동일한 작은 문제가 여러 번 반복해서 나타난다.
            - 예: F(5)를 구할 때 F(3)을 여러 번 계산하는 경우, F(5) = F(4) + F(3), F(4) 구할때도 F(3)에 대한 계산이 또 이루어짐
        - 메모이제이션 (Memoization) 또는 반복(Tabulation) 사용
            - 이미 계산한 결과를 저장하여 다시 계산하지 않도록 한다.
            - Dictionary 등에 이전 결과값 저장 후 사용
    - 동적 계획법 접근 방식
        - 탑다운(Top-Down) (재귀 + 메모이제이션)
            - 큰 문제를 작은 문제로 나누고, 결과를 저장하며 푸는 방식
        - 바텀업(Bottom-Up) (반복문 + 테이블 사용)
            - 작은 문제부터 차근차근 해결하여 큰 문제를 해결하는 방식
    - 대표적 동적 계획법 문제 예시
        - 피보나치 수열 (Fibonacci Sequence)
            - 피보나치 수열은 다음과 같이 정의: F(n) = F(n-1) + F(n-2)
            - 조건: F(0) = 0, F(1) = 1
            - 재귀 (일반적인 방법, 중복 계산이 많아 시간 복잡도 O(2ⁿ)로 매우 비효율적)
                ```python
                def fib(n):
                    if n <= 1: # 0, 1
                        return n
                    return fib(n-1) + fib(n-2)
                print(fib(10))  # 55
                ```
            - 동적 계획법 (탑다운: 메모이제이션 활용, 이전 결과를 memo에 저장하여 중복 계산을 방지하고 시간 복잡도: O(n) (한 번씩만 계산))
                ```python
                # 딕셔너리(해시 테이블)를 사용한 메모이제이션
                memo = {}
                def fib(n):
                    # 딕셔너리 내 값이 존재하면 재연산 없이 그대로 반환
                    if n in memo:
                        return memo[n]
                    # 0 또는 1이면 그대로 반환
                    if n <= 1:
                        return n
                    # 재귀 연산 후 반환되는 값 저장
                    memo[n] = fib(n-1) + fib(n-2)
                    return memo[n]
                print(fib(10))  # 55
                ```
            - 동적 계획법 (바텀업: 반복문 활용)
                - 코드 설명 및 추후 개선점 설명
                    - dp 테이블을 활용해 작은 문제부터 차근차근 해결 (반복문 사용)
                    - 메모리를 줄이려면 dp 배열 없이 변수 2개만 사용 가능 (O(1) 공간 사용 가능)
                    - 시간 복잡도: O(n) (반복문 한 번만 사용)
                ```python
                def fib(n):
                    dp = [0] * (n + 1)  # DP 테이블 초기화, 0으로 초기화
                    dp[1] = 1   # 0: 0(초기화), 1: 1
                    for i in range(2, n + 1):
                        dp[i] = dp[i-1] + dp[i-2]
                    return dp[n]
                print(fib(10))  # 55
                ```
            - 그외 대표적 동적 계획법(DP) 문제
                - 0-1 배낭 문제 (Knapsack Problem)
                    - 문제: 무게 제한이 있는 배낭에 최대 가치를 갖는 물건을 선택하여 넣는 문제
                    - 동적 계획법을 사용하여 해결 가능 (O(nW) 시간 복잡도)
                - 최장 공통 부분 수열 (LCS: Longest Common Subsequence)
                    - 문제: 두 문자열이 주어질 때, 가장 긴 공통 부분 수열의 길이를 구하는 문제
                    - 동적 계획법을 사용하여 해결 가능 (O(nm) 시간 복잡도)
    - DP vs DC (분할 정복(Divide and Conquer)) 비교
        - 동적 계획법 (DP)
            - 중복되는 부분 문제 해결 결과를 저장하여 재사용
            - 피보나치 수열, 배낭 문제, LCS
        - 분할 정복 (DC)
            - 문제를 더 작은 문제로 나누어 해결 후 병합
            - 병합 정렬, 퀵 정렬, 이진 탐색
        - DP는 "중복 계산이 많을 때" 분할 정복보다 효율적이다

- 알고리즘을 설계할 때 고려해야 할 요소 설명
    - 개요
      - 효율성, 정확성, 확장성 등의 다양한 요소를 고려해야 함
      - 이 요소들을 잘 고려해야 성능이 뛰어나고 유지보수가 쉬운 최적의 알고리즘 코딩 가능
    - 고려 필요 요소
      - 정확성 (Correctness)
        - 알고리즘이 모든 입력에 대해 정확한 결과를 도출해야 함
        - 모든 경우의 수를 고려하고, 예외 상황에서도 올바르게 동작하는지 확인해야 함
        - 예제 테스트 및 경계값 테스트를 통해 검증 필요
        - 예시
            - 정렬 알고리즘이 입력 배열을 항상 오름차순(또는 내림차순)으로 정렬하는지 확인
            - 숫자를 더하는 알고리즘에서 음수, 0, 큰 수 등 다양한 입력값에 대해 올바르게 동작하는지 테스트
      - 시간 복잡도 (Time Complexity)
        - 알고리즘이 실행되는 속도를 나타내는 지표
        - 입력 크기 n이 커질수록 실행 시간이 어떻게 변하는지 분석
        - Big-O 표기법으로 성능 평가
        - 예: O(1) < O(log n) < O(n) < O(n log n) < O(n²) < O(2ⁿ) < O(n!)
        - 예시
            - O(n²)인 버블 정렬보다 O(n log n)인 퀵 정렬을 사용하는 것이 효율적
            - 탐색할 데이터가 많을 경우 이진 탐색(O(log n))을 사용하면 선형 탐색(O(n))보다 빠름
      - 공간 복잡도 (Space Complexity)
        - 알고리즘이 실행될 때 필요한 메모리 사용량
        - 추가적인 데이터 구조(배열, 리스트, 해시맵 등)를 사용할 경우 메모리 효율성 고려
        - 인플레이스 알고리즘(in-place algorithm)을 사용하면 추가 공간 사용을 줄일 수 있음
        - 예시
            - 퀵 정렬은 평균적으로 O(log n)의 추가 공간이 필요하지만, 병합 정렬은 O(n)의 추가 공간이 필요
            - 피보나치 수열을 재귀(O(2ⁿ))로 계산하면 메모리 사용이 많아지므로 동적 계획법(DP, O(n))을 활용해 최적화 가능
      - 단순성 (Simplicity, Readability)
        - 코드는 명확하고 직관적으로 설계해야 함
        - 복잡한 알고리즘보다는 이해하기 쉽고 유지보수하기 쉬운 알고리즘이 바람직
        - 가독성이 좋은 변수명과 명확한 주석 작성
        - 예시
            - 한 줄로 압축한 복잡한 코드보다, 단계별로 설명이 잘 된 명확한 코드가 유지보수에 유리
            - 너무 많은 중첩된 반복문 대신 함수 분리 및 재사용성 고려
      - 확장성 (Scalability)
        - 입력 데이터가 커질 때 성능 저하 없이 확장 가능한지 고려
        - 데이터 크기가 100일 때와 1,000,000일 때의 성능 차이를 분석
        - 병렬 처리(멀티스레딩, GPU 가속 등) 가능성도 고려
        - 예시
            - 단일 서버에서 동작하는 알고리즘이 아닌, 분산 처리(Distributed Computing) 가능한 구조로 설계
            - 트래픽이 증가할 경우 부하 분산(Load Balancing)을 고려하여 알고리즘을 최적화
      - 재사용성 (Reusability)
        - 알고리즘이 다양한 문제에서도 재사용 가능하도록 일반화
        - 특정 문제에 한정된 알고리즘보다는 모듈화하여 여러 곳에서 활용 가능하도록 설계
        - 예시
            - 정렬 알고리즘을 특정 데이터 유형에 한정하지 않고, 범용적으로 사용할 수 있도록 제너릭(Generic) 함수로 구현
            - 재귀 알고리즘을 반복문으로 변환하여 재사용성을 높임
      - 경계값 & 예외 처리 (Edge Cases & Error Handling)
        - 알고리즘이 비정상적인 입력값에서도 안정적으로 동작하는지 확인
        - 입력값이 너무 크거나 작을 때, 음수, 빈 값 등을 고려하여 예외 처리 추가
        - 예시
            - 리스트가 비어 있을 때 오류 발생 방지 (if len(arr) == 0: 처리)
            - 입력값이 음수일 경우 처리 (if n < 0: 예외 처리)
      - 최적화 가능성 (Optimization)
        - 처음부터 최적의 알고리즘을 설계하기 어려우므로, 점진적인 최적화 과정 필요
        - 알고리즘이 실제 환경에서 병목현상(Bottleneck)이 발생하는 부분을 분석하고 개선
        - 예시
            - 정렬된 배열에서 선형 탐색(O(n)) 대신 이진 탐색(O(log n))을 활용
            - 메모이제이션(Memoization) 기법을 적용해 중복 계산 최소화

- 메모이제이션(Memoization) 셜명과 예제 기술
    - 설명
        - 이전에 계산한 결과를 저장하여, 동일한 연산을 반복하지 않도록 최적화하는 기법
        - 중복 계산을 줄여 알고리즘의 성능을 개선하는 방식으로, 주로 재귀 함수에서 사용
        - 핵심 개념
            - 이전에 계산한 값을 저장하여 중복 계산 방지
            - 재귀적 알고리즘(피보나치, 팩토리얼)에서 성능 최적화
            - 공간을 사용하여 시간을 절약 (시간 복잡도 감소)

- 다이나믹 프로그래밍(DP)과 분할 정복(D&C)의 차이점 서술
  - 차이점 개요
    - 모두 재귀적 접근 방식을 사용하는 알고리즘 기법
    - 문제 해결 방식과 중간 결과의 활용 방식에서 중요한 차이점 존재
  - 동적 계획법(Dynamic Programming, DP)
    - 정의
      - 큰 문제를 작은 문제로 나누어 해결하고, 중복되는 부분 문제의 결과를 저장하여 재사용하는 기법
      - 최적 부분 구조(Optimal Substructure)와 중복 부분 문제(Overlapping Subproblems) 특성을 가짐
    - 특징
	  - 중복된 계산을 피하기 위해 메모이제이션(Memoization) 또는 테이블(Tabulation) 기법 사용
      - 부분 문제의 해결 결과를 저장하여 재사용하는 것이 핵심.
	  - 주로 최적화 문제(Optimization Problems)에 사용됨
        - 최단 경로, 배낭 문제, LIS 등
  - 분할 정복(Divide and Conquer, D&C)
    - 정의
	  - 문제를 작은 문제로 나누고, 각각을 독립적으로 해결한 후, 결과를 조합하여 최종 결과를 얻는 방식
      - 최적 부분 구조(Optimal Substructure) 특성을 가짐
    - 특징
	  - 각 하위 문제가 독립적(Independent Subproblems)으로 해결됨
	  - 중복된 계산을 피하지 않음 → DP와 달리, 같은 부분 문제가 여러 번 계산될 수 있음
	  - 재귀적으로 하위 문제를 해결한 후, 최종적으로 합침(Merge 과정이 중요)
  - 결론
    - DP는 “중복 문제를 저장하고 재사용할 때”, D&C는 “독립적인 문제를 나눠 해결할 때” 사용한다

- 탐욕 알고리즘(Greedy Algorithm) 정의 및 장점과 단점 설명
    - 정의
        - 각 단계에서 가장 최선의 선택(현재 최적의 해)을 하는 방식으로 문제를 해결하는 알고리즘
        - 현재 상태에서 가장 최적인 선택을 반복적으로 수행하여 전체 문제의 해를 구하는 방식
    - 대표적인 특징
        - 현재 순간(Locally)에서 가장 좋은 선택(최적해)을 찾음
        - 미래의 선택에 대한 고려 없이 즉각적인 최적해를 구함
        - "한 번 선택한 해를 변경하지 않음" (백트래킹 없음)
        - 최적해가 항상 보장되지 않지만, 일부 문제에서는 매우 효과적
    - 탐욕 알고리즘의 동작 방식
        - 현재 상태에서 가장 최선의 선택을 수행
        - 해당 선택이 전체 최적해에 포함되는지 검토
        - 모든 선택이 끝날 때까지 반복
        - 최종적으로 최적해를 도출
    - 탐욕 알고리즘의 예제
        - 거스름돈 문제
            - 문제: 거스름돈으로 줄 동전 개수를 최소화해야 한다.
            - 입력: 동전 단위 [500원, 100원, 50원, 10원], 목표 금액: 1260원
            - 출력: 최소 동전 개수
                ```python
                def min_coins_greedy(money):
                    coins = [500, 100, 50, 10]  # 동전 단위 (큰 단위부터 사용)
                    count = 0
                    for coin in coins:
                        count += money // coin  # 현재 동전으로 거슬러 줄 개수
                        money %= coin  # 남은 금액 업데이트
                    return count

                print(min_coins_greedy(1260))  # 출력: 6 (500x2, 100x2, 50x1, 10x1)
                ```
                - 코드 설명
                    - 큰 단위의 동전부터 차례로 사용하여 최소 개수를 만들도록 함.
                    - 현재 상태에서 최선의 선택을 계속 반복하면서 문제를 해결.
    - 탐욕 알고리즘의 대표적인 활용 사례
        - 거스름돈 문제: 가장 큰 단위부터 사용하여 동전 개수 최소화
        - 최소 신장 트리(MST): 크루스칼 알고리즘(Kruskal), 프림 알고리즘(Prim)
        - 다익스트라 최단 경로: 현재 최단 거리를 가진 노드를 선택하여 확장
        - 허프만 코딩(Huffman Coding): 압축 알고리즘, 빈도 기반 최적 이진 트리 생성
        - 회의실 배정 문제: 종료 시간이 가장 빠른 회의를 먼저 선택
        - 배낭 문제(Knapsack Problem): 0-1 Knapsack과 Fractional Knapsack (부분적으로 적용 가능)
    - 탐욕 알고리즘의 장점
        - 빠른 실행 속도 (O(n log n) ~ O(n))
            - 매 단계에서 최적의 해를 선택하기 때문에 빠르게 해결됨
            - 동적 계획법(DP)보다 공간 복잡도가 낮아 효율적
        - 간결한 코드
            - 복잡한 백트래킹 없이 직관적으로 작성 가능
        - 특정 문제에서 최적해 보장
            - "탐욕 선택 속성(Greedy Choice Property)"이 성립하면 최적해를 보장.
            - "최적 부분 구조(Optimal Substructure)"가 존재해야 함.
    - 탐욕 알고리즘의 단점
        - 최적해를 항상 보장하지 않음
            - 모든 문제에서 항상 최적해를 찾을 수 없음
            - 배낭 문제 (0-1 Knapsack) 같은 경우, 탐욕법으로 풀면 최적해가 나오지 않음.
            - DP(동적 계획법)나 백트래킹이 더 적합할 수 있음.
        - 미래를 고려하지 않음
            - 현재 최적의 해가 미래에 최악의 선택이 될 수도 있음.
            - 예: 그래프에서 DFS보다 BFS(다익스트라)가 유리한 경우.
        - 탐욕법이 적용 가능한 문제인지 검토 필요
            - "탐욕 선택 속성"과 "최적 부분 구조"가 만족되는지 분석해야 함.
    - 탐욕 알고리즘이 최적해를 찾지 못하는 경우
        - 0-1 배낭 문제(Knapsack Problem)
            - 배낭의 최대 무게는 10kg
            - 물건:
                - A (무게 5kg, 가치 30)
                - B (무게 4kg, 가치 28)
                - C (무게 6kg, 가치 35)
        - 코드
            ```python
            items = [(5, 30), (4, 28), (6, 35)]  # (무게, 가치)
            items.sort(key=lambda x: x[1] / x[0], reverse=True)  # 단위 무게당 가치로 정렬

            capacity = 10
            total_value = 0

            for weight, value in items:
                if capacity >= weight:
                    capacity -= weight
                    total_value += value

            print(total_value)  # 출력: 58 (B + A 선택)
            ```
            - 코드 설명
                - 탐욕 알고리즘은 단위 무게당 가치가 높은 물건을 선택하지만,
                    - B(4kg, 28) + A(5kg, 30) = 58 (탐욕법)
                    - C(6kg, 35) + B(4kg, 28) = 63 (최적해, DP로 풀어야 가능)
                - 탐욕법으로는 최적해를 구하지 못함
                    - 이 문제는 DP(동적 계획법)가 더 적합
    - 결론
        - 탐욕 알고리즘은 현재 상태에서 가장 최적의 선택을 반복하여 문제를 해결하는 방식
        - 일부 문제에서는 최적해를 보장하지만, 항상 보장되는 것은 아님
        - "탐욕 선택 속성(Greedy Choice Property)"과 "최적 부분 구조(Optimal Substructure)"가 성립해야 탐욕법이 유효
        - 백트래킹, DP, DFS/BFS 등과 비교하여 문제 유형에 맞게 선택해야 함
        - 탐욕 알고리즘은 빠르고 간결하지만, 항상 최적해를 찾는 것은 아님
        - 적용할 수 있는 문제인지 검토 후 사용

- 백트래킹(Backtracking) 기법 개념과 예제 설명
    - 백트래킹(Backtracking) 개념
        - 모든 가능한 경우의 수를 탐색하면서, 불가능한 경우가 발생하면 되돌아(백트랙) 가는 방식으로 문제를 해결하는 기법
    - 백트래킹 특징
        - 재귀(Recursion) 기반 탐색 기법
        - 가능성이 없는 경로를 조기에 차단(Pruning, 가지치기)하여 탐색 범위 축소
        - DFS(깊이 우선 탐색, Depth-First Search)와 유사하지만, 불가능한 경로를 빨리 배제하는 것이 차이점
    - 백트래킹과 완전 탐색(Brute Force) 차이
        - 탐색 방식
            - 백트래킹: 조건을 만족하는 경우만 탐색 (가지치기)
            - 완전 탐색: 모든 경우의 수 탐색
        - 성능 최적화
            - 백트래킹: 불필요한 탐색 배제 (Pruning)
            - 완전 탐색: 모든 경우를 시도 (비효율적)
        - 예제
            - 백트래킹: N-Queen, 순열(Permutation)	
            - 완전 탐색: 단순한 경우의 수 계산
    - 백트래킹을 적용하는 대표적인 문제 유형
        - N-Queen 문제 → 퀸을 놓을 수 있는 유효한 경우만 탐색
        - 부분 집합(Subset) 생성 → 특정 조건을 만족하는 부분 집합 찾기
        - 순열(Permutation) 생성 → 중복 없이 모든 순열 생성
        - 조합(Combination) 생성 → 특정 개수의 원소를 선택하는 경우 찾기
        - 미로 탐색(Maze Solving) → 경로 찾기
    - 결론
        - 가능한 모든 경우를 탐색하지만, 불가능한 경로는 조기에 배제하는 최적화된 DFS 기법
        - N-Queen, 순열, 조합, 부분 집합, 미로 탐색 등 다양한 문제에 적용 가능
        - DFS와 유사하지만, “가지치기(Pruning)“를 통해 불필요한 탐색을 줄여 성능을 개선
        - 재귀 호출 후 pop()을 사용하여 이전 상태로 되돌리는 것이 핵심
        - 백트래킹은 가능한 경우의 수를 탐색하면서도, 불필요한 계산을 줄이는 스마트한 탐색 기법

- 브루트 포스(Brute Force) 알고리즘의 개념과 장점, 단점을 설명
    - 브루트 포스(Brute Force) 알고리즘 개념
        - 가능한 모든 경우를 전부 탐색하여 정답을 찾는 알고리즘 기법
        - 무식하게(Brute Force) 하나씩 다 해보는 방식으로, 가장 단순하고 직관적인 방법
    - 브루트 포스의 특징
        - 모든 경우의 수를 다 탐색 → 완전 탐색(Exhaustive Search) 기법
        - 단순하고 구현이 쉬움 → 가장 기본적인 알고리즘
        - 시간이 오래 걸릴 수 있음 → 최적화가 필요할 수도 있음
    - 브루트 포스 알고리즘의 예제
	    - 비밀번호 해킹(무차별 대입 공격, Brute Force Attack)
	    - 문자열 검색(모든 위치에서 일치하는지 확인)
	    - 조합 생성(모든 가능한 조합을 탐색)
	    - 순열 생성(모든 가능한 순서를 확인)
    - 브루트 포스의 장점과 단점
        - 장점
            - 구현이 단순하고 직관적
	            - 모든 경우를 확인하는 방식이므로 코드가 간단함.
            - 정확한 해를 찾을 수 있음
	            - 가능한 모든 경우를 확인하므로 최적해(Optimal Solution)를 찾을 수 있음
            - 특별한 알고리즘적 사고 없이도 적용 가능
	            - 문제 해결을 위한 복잡한 이론이 필요하지 않음
        - 단점
            - 비효율적 (시간 복잡도가 매우 큼)
	            - 경우의 수가 많아지면 O(N!), O(2^N) 같은 지수 시간이 걸릴 수 있음
            - 큰 입력값에 대해 실행 시간이 길어짐
	            - 입력 크기가 증가하면 실행 시간이 급격히 증가하여 실용성이 떨어짐.
            - 최적화가 어렵고, 개선이 필요함
	            - 일부 문제에서는 탐색 공간을 줄이는 백트래킹(Backtracking), 동적 프로그래밍(DP) 같은 최적화 기법이 필요함
    - 브루트 포스가 비효율적인 경우
	    - N이 100만 이상인 경우 → 지수 시간 복잡도 O(N!)은 비효율적
	    - 최적해를 빨리 찾아야 하는 문제 → 다른 최적화 기법 사용 필요
    - 브루트 포스 예제 코드
        - 배열에서 특정 숫자 찾기 ( 시간 복잡도: O(N) (배열 크기만큼 탐색) )
            ```python
            def brute_force_search(arr, target):
                for i in range(len(arr)):
                    if arr[i] == target:
                        return i  # 찾은 위치 반환
                return -1  # 못 찾으면 -1 반환

            arr = [1, 3, 7, 5, 9]
            target = 7
            print(brute_force_search(arr, target))  # 출력: 2
            ```
        - 문자열 패턴 매칭 (Brute Force 방식)
            ```python
            def brute_force_pattern_search(text, pattern):
                n, m = len(text), len(pattern)
                for i in range(n - m + 1):  # 시작 위치
                    j = 0
                    while j < m and text[i + j] == pattern[j]:
                        j += 1
                    if j == m:
                        return i  # 패턴이 일치하는 위치 반환
                return -1  # 패턴이 없으면 -1 반환

            text = "ABABABC"
            pattern = "ABABC"
            print(brute_force_pattern_search(text, pattern))  # 출력: 2
            ```
             - 코드 설명
                 - 시간 복잡도: O(NM) (텍스트 길이 * 패턴 길이만큼 탐색)
                 - 최적화 방법: KMP 알고리즘(O(N)으로 개선 가능)
    - 유용한 케이스
        - 입력 크기 작을때
        - 최적화 알고리즘 찾기 어려울 때
        - 정확한 해답 필요 시
    - 최적화 기법
        - 가지치기
        - 메모이제이션
        - 비트마스크
        - 정렬 후 탐색 최적화

- NP-문제 정의와 P vs NP 문제를 설명
    - NP-문제(NP Problem) 정의
        - NP(Non-deterministic Polynomial time) 
        - 문제 해결(정답을 찾는 것)은 어려울 수 있지만, 주어진 해답을 검증하는 것은 다항시간(Polynomial Time) 안에 할 수 있는 문제를 의미.
    - NP의 핵심 개념
        - 해를 찾는 것은 어려울 수 있지만, 주어진 해가 맞는지 검증하는 것은 빠르게 가능
        - 퍼즐을 푸는 것은 어려울 수 있지만, 정답이 맞는지 확인하는 것은 쉽다
    - NP 문제 예시
	    - 여행하는 외판원 문제(TSP, Traveling Salesman Problem): 모든 도시를 한 번씩 방문하는 최적 경로 찾기 → 어렵다.
	    - 만약 주어진 경로가 최적인지 검증하는 것 → 쉽다 (단순히 거리 계산하면 됨)
    - P(Polynomial Time) 문제 개념
        - P(Polynomial Time) 문제는 해를 찾는 것도 다항시간 내에 해결 가능한 문제
        - P 문제는 일반적인 알고리즘으로 빠르게 풀 수 있는 문제를 의미
    - P 문제 예시
	    - 정렬(Sorting): 퀵 정렬(O(n log n)), 병합 정렬(O(n log n)) 등 → 빠르게 해결 가능
	    - 최단 경로 문제(Dijkstra, Floyd-Warshall) → 다항시간 내에 해결 가능
	    - 소수 판별(에라토스테네스의 체, O(n log log n))
    - P 문제는 해결하는 것도 쉽고, 검증하는 것도 쉬운 문제
    - P vs NP 문제
        - P 문제: 다항시간(Polynomial Time) 내에 해결 가능, 쉽게 풀 수 있는 문제
        - NP 문제: 해답 검증은 다항시간 내 가능하지만, 해결이 어려울 수 있음, 풀기는 어렵지만, 답이 맞는지 확인은 쉬움
    - P와 NP의 관계
	    - 모든 P 문제는 NP 문제이지만, 모든 NP 문제가 P 문제인지는 미지수
	    - 즉, P ⊆ NP이지만, NP ⊆ P인지는 아직 증명되지 않음
    - P = NP인가?
        - 만약 P = NP라면, NP 문제도 다항시간 내에 풀 수 있는 알고리즘이 존재한다는 의미
        - 현재까지 어떤 NP 문제도 다항시간 내에 풀리는 P 문제로 변환되지 않음
        - 즉, P ≠ NP라고 생각되지만, 아직 증명되지 않은 미해결 문제
    - P vs NP 문제는 컴퓨터 과학에서 가장 중요한 난제 중 하나이며, 이를 해결하면 100만 달러 상금이 걸려 있음(클레이 수학 연구소 문제)

- NP-완전(NP-Complete) 문제와 NP-난해(NP-Hard) 문제의 차이점을 설명
    - NP 완전 문제(NP-Complete, NPC)
        - 정의: NP 문제 중에서도 가장 어려운 문제로, 만약 이 문제를 다항시간 내에 해결할 수 있다면, 모든 NP 문제가 P 문제가 됨.
            - NP에 속하면서, NP-난해(NP-Hard)한 문제
            - 즉, NP-완전 문제를 다항시간 내에 해결할 수 있으면, 모든 NP 문제를 다항시간 내에 해결할 수 있음.
            - 대표적인 예제: SAT(만족도 문제), 여행하는 외판원 문제(TSP)
        - NP-완전 문제의 특징
            - 해당 문제는 NP에 속해야 함 (즉, 주어진 해답을 다항시간 내에 검증할 수 있어야 함)
            - NP-난해(NP-Hard) 문제 중 하나여야 함
    - NP 난해 문제(NP-Hard)
        - 정의: 모든 NP 문제를 다항시간 내에 해결할 수 있는 문제로 변환 가능하지만, 반드시 NP에 속하지는 않는 문제.
            - 즉, 해답 검증이 다항시간 내에 가능할 필요가 없음
            - 일반적으로 NP-완전 문제보다 더 어려운 문제도 포함됨
        - NP-난해 문제의 특징
            - 해답 검증이 NP에 속하지 않을 수도 있음
            - 모든 NP 문제를 다항시간 내에 변환 가능
            - 결정 문제뿐만 아니라 최적화 문제도 포함할 수 있음
            - 대표적인 예제: 여행하는 외판원 문제(TSP)의 최적화 버전, 할당 문제(Assignment Problem), 체스 게임의 최적 해 찾기.
    - 결론
        - NP-완전(NP-Complete) 문제는 NP 문제이면서, NP-난해 문제
        - NP-난해(NP-Hard) 문제는 NP에 속하지 않을 수도 있음
        - P vs NP 문제를 해결하기 위해 NP-완전과 NP-난해 문제의 특성을 분석하는 것이 중요
        - 모든 NP-완전 문제는 NP-난해 문제이지만, 모든 NP-난해 문제가 NP-완전 문제는 아니다


- 트레이드오프(Trade-off) 정의와 알고리즘 설계에서의 예제를 설명
    - 트레이드오프(Trade-off) 정의
        - 두 가지 이상의 상반된 요소가 있을 때, 하나를 선택하면 다른 하나를 포기해야 하는 상황을 의미
        - 즉, 하나를 얻으면 다른 것을 희생해야 하는 관계
    - 일반적인 트레이드오프 예시
	    - 성능 vs 비용 → 더 빠른 컴퓨터를 원하면 비용이 증가
        - 화질 vs 저장 공간 → 고화질 영상은 저장 공간을 더 많이 차지
	    - 안정성 vs 속도 → 시스템 안정성을 높이면 속도가 느려질 수 있음
    - 알고리즘 설계에서의 트레이드 오프
        - 시간 복잡도 vs 공간 복잡도
            - 빠른 실행 속도를 원하면 메모리를 더 사용해야 하고, 메모리를 절약하면 실행 속도가 느려질 수 있음
            - 예제: 피보나치 수열 계산, 일반재귀, 메모이제이션(동적계획법)
        - 정확성(Accuracy) vs 속도(Speed)
            - 머신러닝 모델 트레이드오프
                - 딥러닝 모델 (정확성 ↑, 속도 ↓)
	                - 신경망(Deep Neural Networks)은 높은 정확도를 제공하지만, 훈련(training) 시간이 오래 걸림.
                - 경량 모델 (정확성 ↓, 속도 ↑)
	                - 단순한 선형 회귀(Linear Regression) 모델은 빠르게 예측할 수 있지만, 복잡한 패턴을 제대로 학습하지 못할 수도 있음
        - 데이터 구조 선택: 검색 속도 vs 메모리 사용
            - 특정 작업(검색, 삽입, 삭제 등)에 맞춰 적절한 데이터 구조를 선택해야 함.
            - 배열(Array) vs 해시맵(HashMap)
                - 배열 (메모리 절약, 검색 속도 느림)
                 - 해시맵 (메모리 사용 증가, 검색 속도 빠름)
        - 압축(Compression) vs 성능(Performance)
            - 데이터를 압축하면 저장 공간을 줄일 수 있지만, 압축을 풀 때 시간이 오래 걸릴 수 있음.
            - 이미지 압축
	            - PNG (압축률 높음, 로딩 속도 느림) → 저장 공간을 절약하지만, 렌더링 속도가 느릴 수 있음.
	            - JPEG (압축률 낮음, 로딩 속도 빠름) → 화질을 조금 희생하지만, 로딩 속도가 빠름
        - 보안(Security) vs 성능(Performance)
            - 보안을 강화하면 성능이 저하될 수 있음.
            - 예를 들어, 웹사이트에서 SSL/TLS 암호화를 사용하면 보안이 강화되지만, 네트워크 속도가 느려질 수 있음.
            - HTTPS vs HTTP
                - HTTP (보안 ↓, 속도 ↑)
	                - 데이터를 암호화하지 않아 보안에 취약하지만 속도는 빠름
                - HTTPS (보안 ↑, 속도 ↓)
	                - 데이터를 암호화하여 보안을 강화하지만, 암호화/복호화 과정 때문에 속도가 느려질 수 있음

- 버블 정렬(Bubble Sort)의 동작 방식과 시간 복잡도
    - 버블 정렬 정의
        - 버블 정렬(Bubble Sort)은 인접한 두 요소를 비교하면서 정렬하는 가장 단순한 정렬 알고리즘
        - 큰 값이 거품(Bubble)처럼 배열 끝으로 이동하는 방식
        - 안정 정렬(Stable Sort)로 같은 값이 있는 경우 기존 순서가 유지
    - 버블 정렬의 특징
	    - 제자리 정렬(In-place Sort) → 추가적인 메모리 공간 필요 없음
	    - 비교 기반 정렬(Comparison Sort) → 원소를 직접 비교하여 정렬 수행
	    - 느린 정렬 방식 → 실행 속도가 매우 느려 대규모 데이터 정렬에는 비효율적
        - 최악: O(N²), 최선: O(N), 공간: O(1)
    - 버블 정렬 동작 방식
        - 서로 인접한 두 요소를 비교하여, 크기가 작은 값을 앞으로 이동하는 방식으로 진행
        - N번 반복하면서 정렬이 완료될 때까지 수행
    - 버블 정렬 예시 ([5, 3, 8, 4, 2]를 오름차순 정렬)
        - 첫 번째 패스 (Pass 1)
	        - (5, 3) 비교 → 3이 더 작으므로 교환 → [3, 5, 8, 4, 2]
	        - (5, 8) 비교 → 순서 유지 → [3, 5, 8, 4, 2]
	        - (8, 4) 비교 → 4가 더 작으므로 교환 → [3, 5, 4, 8, 2]
	        - (8, 2) 비교 → 2가 더 작으므로 교환 → [3, 5, 4, 2, 8]
            - 첫 번째로 큰 값: 가장 큰 값(8)이 마지막으로 이동.
        - 두 번째 패스 (Pass 2)
	        - (3, 5) 비교 → 순서 유지 → [3, 5, 4, 2, 8]
	        - (5, 4) 비교 → 4가 더 작으므로 교환 → [3, 4, 5, 2, 8]
	        - (5, 2) 비교 → 2가 더 작으므로 교환 → [3, 4, 2, 5, 8]
            - 두 번째로 큰 값(5)이 정렬 완료.
        - 세 번째 패스 (Pass 3)
	        - (3, 4) 비교 → 순서 유지 → [3, 4, 2, 5, 8]
	        - (4, 2) 비교 → 2가 더 작으므로 교환 → [3, 2, 4, 5, 8]
            - 세 번째로 큰 값(4)이 정렬 완료.
        - 네 번째 패스 (Pass 4)
	        - (3, 2) 비교 → 2가 더 작으므로 교환 → [2, 3, 4, 5, 8]
            - 모든 정렬 완료
    - 최적화된 버블 정렬 (스왑 감지)
        - 이미 정렬된 경우 즉시 종료하여 불필요한 반복을 줄이는 최적화 적용
        - 이미 정렬된 경우 O(N)으로 최적화 가능
            ```python
            def optimized_bubble_sort(arr):
                n = len(arr)
                for i in range(n - 1):
                    swapped = False  # 스왑 발생 여부 체크
                    for j in range(n - 1 - i):
                        if arr[j] > arr[j + 1]:
                            arr[j], arr[j + 1] = arr[j + 1], arr[j]
                            swapped = True
                    if not swapped:  # 더 이상 스왑이 발생하지 않으면 정렬 완료
                        break
                return arr

            arr = [2, 3, 4, 5, 8]  # 이미 정렬된 배열
            print(optimized_bubble_sort(arr))  # O(N)으로 빠르게 종료
            ```

- 선택 정렬(Selection Sort)의 동작 원리와 특징을 설명
    - 정의
        - 배열에서 가장 작은(또는 큰) 값을 선택하여 정렬하는 방식의 정렬 알고리즘
        - 첫 번째 원소부터 시작하여, 남아있는 데이터 중 최소값을 찾아 교환하는 방식으로 동작
        - 제자리 정렬(In-place Sort)이므로, 추가적인 메모리 공간을 거의 사용하지 않음
    - 선택 정렬의 특징
	    - 단순한 구조의 비교 정렬 알고리즘
	    - 매 반복마다 최솟값(또는 최댓값)을 찾아 첫 번째 위치로 이동
	    - 시간 복잡도는 O(N²)로, 성능이 좋지 않음
	    - 추가적인 메모리 공간이 필요 없는 정렬 방식
        - 공간 복잡도 O(1) (추가 메모리 사용 없음)
    - 선택 정렬 동작 방식
        - 배열에서 최소값을 찾아 맨 앞의 값과 교환 → 다음 탐색에서는 두 번째 위치부터 반복
        - N-1번 반복하여 정렬 완료
    - 선택 정렬의 시간 복잡도
        - 최악(Worst Case) & 평균(Average Case) 시간 복잡도
            - 선택 정렬은 항상 N-1번 선택하고, N-1번 비교 연산을 수행
            - 최선, 평균, 최악 모두 O(N²)
            - 데이터가 정렬되어 있어도 전체 탐색이 필요하기 때문에 최적화 효과 없음
            - 최악/평균 시간 복잡도: O(N^2)
        - 최선(Best Case) 시간 복잡도
            - 배열이 이미 정렬된 경우에도 O(N²)
            - 단순 비교 연산이 줄어들 뿐, 전체 탐색은 필요함
            - 최선 시간 복잡도: O(N^2)
     - 선택 정렬 구현 (최적화된 선택 정렬 (정렬 여부 체크))
         - 데이터가 이미 정렬된 경우, 불필요한 연산을 줄이도록 개선 가능
            ```python
            def optimized_selection_sort(arr):
                n = len(arr)
                for i in range(n - 1):
                    min_idx = i
                    swapped = False  # 정렬 여부 확인
                    for j in range(i + 1, n):
                        if arr[j] < arr[min_idx]:
                            min_idx = j # 최소값 찾기
                            swapped = True
                    if not swapped:  # 정렬 완료된 경우 종료
                        break
                    # 찾은 최소값을 설정
                    arr[i], arr[min_idx] = arr[min_idx], arr[i]
                return arr

            arr = [2, 3, 4, 5, 8]  # 이미 정렬된 배열
            print(optimized_selection_sort(arr))  # O(N)으로 빠르게 종료
            ```
            - 코드 설명
                - 이미 정렬된 경우 O(N)으로 최적화 가능!
    - 선택 정렬의 장단점
        - 장점
	        - 구현이 매우 간단
	            - 코드가 직관적이고 이해하기 쉬움
	        - 추가적인 메모리가 필요 없음 (In-place 정렬)
	            - O(1) 공간 복잡도
	        - 제자리 정렬 방식 (Stable Sort 가능)
	            - 최적화하면 안정 정렬이 가능
	        - 비교적 작은 데이터에 적합
	            - N이 작을 때는 버블 정렬보다 빠름
        - 단점
	        - 느린 성능 (O(N²))
	            - 데이터 크기가 크면 성능이 매우 저하됨
	        - 대규모 데이터 정렬에 비효율적
	            - 퀵 정렬, 병합 정렬(O(N log N))과 비교하면 매우 비효율적
	        - 교환 연산이 많음
	            - 스왑이 빈번하게 발생하여 성능이 저하될 수 있음
     
- 삽입 정렬(Insertion Sort)의 과정과 시간 복잡도를 설명
    - 삽입 정렬(Insertion Sort) 정의
        - 배열을 정렬된 부분과 정렬되지 않은 부분으로 나누고, 정렬되지 않은 요소를 하나씩 꺼내 적절한 위치에 삽입하는 정렬 알고리즘
        - 제자리 정렬(In-place Sort)로, 추가적인 메모리 공간이 거의 필요하지 않음
        - 거의 정렬된 배열에서 매우 효율적인 성능(O(N))을 보임
    - 삽입 정렬의 특징:
	    - 단순하고 구현이 쉬운 알고리즘
	    - 거의 정렬된 데이터에서 빠른 성능 (O(N))
	    - 제자리 정렬로 추가적인 메모리 공간 필요 없음 (O(1))
	    - 느린 정렬 방식 (O(N²))으로 대규모 데이터에는 적합하지 않음
    - 삽입 정렬 과정
        - 배열을 앞부분(정렬된 부분)과 뒷부분(정렬되지 않은 부분)으로 나누고, 정렬되지 않은 요소를 적절한 위치에 삽입
        - N-1번 반복하며, 각 요소를 적절한 위치로 이동하여 정렬
        - 예제: [5, 3, 8, 4, 2]를 오름차순 정렬
        - 첫 번째 패스 (Pass 1, 두 번째 요소 비교)
	        - 정렬된 부분: [5]
	        - 현재 비교 대상: 3
	        - 3을 5 앞에 삽입
	        - 결과: [3, 5, 8, 4, 2]
        - 두 번째 패스 (Pass 2, 세 번째 요소 비교)
	        - 정렬된 부분: [3, 5]
	        - 현재 비교 대상: 8
	        - 8은 이미 제자리에 있음
	        - 결과: [3, 5, 8, 4, 2]
        - 세 번째 패스 (Pass 3, 네 번째 요소 비교)
	        - 정렬된 부분: [3, 5, 8]
	        - 현재 비교 대상: 4
	        - 4를 적절한 위치(5 앞)에 삽입
	        - 결과: [3, 4, 5, 8, 2]
        - 네 번째 패스 (Pass 4, 다섯 번째 요소 비교)
	        - 정렬된 부분: [3, 4, 5, 8]
	        - 현재 비교 대상: 2
	        - 2를 적절한 위치(3 앞)에 삽입
	        - 결과: [2, 3, 4, 5, 8] 정렬 완료
    - 삽입 정렬의 시간 복잡도
        - 최악(Worst Case) & 평균(Average Case) 시간 복잡도
            - 정렬되지 않은 데이터가 역순(내림차순)으로 정렬되어 있는 경우 → 모든 요소를 한 칸씩 이동해야 함
            - 모든 요소를 비교하고 이동해야 하므로 O(N²)
        - 최악/평균 시간 복잡도: O(N^2)
        - 최선(Best Case) 시간 복잡도
            - 배열이 이미 정렬된 경우, 각 요소가 한 번만 비교됨
            - 따라서 O(N)까지 최적화 가능
        - 최선 시간 복잡도: O(N)
    - 삽입 정렬 구현
        - 기본 삽입 정렬 코드
        ```python
        def insertion_sort(arr):
            n = len(arr)    # 배열 크기
            for i in range(1, n):  # 두 번째 요소부터 시작 (인덱스 1부터 시작)
                key = arr[i]
                j = i - 1
                while j >= 0 and arr[j] > key:  # key보다 큰 값들은 한 칸씩 이동
                    arr[j + 1] = arr[j]
                    j -= 1
                arr[j + 1] = key  # 적절한 위치에 key 삽입
            return arr

        arr = [5, 3, 8, 4, 2]
        print(insertion_sort(arr))  # 출력: [2, 3, 4, 5, 8]
        ```
        - 코드 설명
            - 시간 복잡도 O(N²), 공간 복잡도 O(1) (추가 메모리 사용 없음)
    - 최적화된 삽입 정렬 (이미 정렬된 경우 빠른 종료)
        - 이미 정렬된 경우 불필요한 비교 연산을 줄이도록 개선 가능
        ```python
        def optimized_insertion_sort(arr):
            n = len(arr)
            for i in range(1, n):
                key = arr[i]
                j = i - 1
                if arr[j] <= key:  # 이미 정렬된 경우 이동 불필요
                    continue
                while j >= 0 and arr[j] > key:
                    arr[j + 1] = arr[j]
                    j -= 1
                arr[j + 1] = key
            return arr

        arr = [2, 3, 4, 5, 8]  # 이미 정렬된 배열
        print(optimized_insertion_sort(arr))  # O(N)으로 빠르게 종료
        ```
        - 코드 설명
            - 이미 정렬된 경우 O(N)으로 최적화 가능
    - 삽입 정렬의 장단점
        - 장점
	        - 거의 정렬된 데이터에 대해 매우 빠름 (O(N))
	            - 대부분 정렬된 데이터에서는 버블 정렬, 선택 정렬보다 훨씬 빠름
	        - 추가적인 메모리 사용이 없음 (In-place 정렬, O(1))
	            - 별도의 추가 배열이 필요하지 않음
	        - 안정 정렬 (Stable Sort)
	            - 동일한 값이 있는 경우 원래 순서가 유지됨.
        - 단점
	        - 큰 데이터 정렬에는 비효율적 (O(N²))
	            - 퀵 정렬, 병합 정렬(O(N log N))에 비해 성능이 떨어짐
	        - 많은 이동 연산이 필요함
	            - 정렬되지 않은 요소가 많을 경우, 많은 데이터 이동이 필요하여 비효율적
    - 결론
        - 배열을 정렬된 부분과 정렬되지 않은 부분으로 나누고, 정렬되지 않은 요소를 적절한 위치에 삽입하는 방식
        - 거의 정렬된 데이터에서는 O(N)으로 매우 빠름, 일반적으로 O(N²)으로 비효율적
        - 메모리 사용이 적고(제자리 정렬), 안정 정렬이지만, 대규모 데이터 정렬에는 적합하지 않음

- 병합 정렬(Merge Sort) 알고리즘의 개념과 적용 사례
    - 병합 정렬(Merge Sort)의 개념
        - 병합 정렬(Merge Sort)은 분할 정복(Divide and Conquer) 전략을 기반으로 하는 정렬 알고리즘
        - 주어진 배열을 계속해서 두 개의 작은 배열로 분할하고, 더 이상 분할할 수 없는 크기(즉, 원소가 1개 이하)가 되면, 이를 다시 병합하면서 정렬하는 방식으로 동작
    - 알고리즘 과정
	    - 분할(Divide)
            - 배열을 중간을 기준으로 두 개의 부분 배열로 나눔
	    - 정복(Conquer)
            - 나눈 부분 배열을 각각 다시 병합 정렬을 적용하여 정렬된 상태로 만듬
	    - 병합(Merge)
            - 정렬된 두 개의 부분 배열을 하나의 정렬된 배열로 합침
    - 시간 복잡도
	    - 최선, 평균, 최악의 경우: O(n log n)
	    - 공간 복잡도: O(n) (추가적인 보조 배열이 필요함)
    - 병합 정렬의 적용 사례
	    - 대용량 데이터 정렬
	        - 퀵 정렬(Quick Sort)은 최악의 경우 O(n²)이 될 수 있어, 안정적인 성능이 필요한 경우 병합 정렬을 사용
	        - 내부 정렬(메모리 내 정렬)보다 외부 정렬(디스크에서 데이터를 읽고 정렬하는 경우)에서 자주 사용됨
	    - 링크드 리스트 정렬
	        - 배열 기반 정렬은 랜덤 액세스(random access)가 빠른 배열에서 효과적이지만, 링크드 리스트는 연속된 메모리 할당이 없기 때문에 퀵 정렬보다 병합 정렬이 더 적합
	    - 멀티스레딩 환경에서 정렬
	        - 병합 정렬은 재귀적으로 정렬할 부분을 나누므로, 여러 개의 스레드에서 병렬로 처리하기 용이
	    - 외부 정렬(External Sorting)
	        - 대용량 데이터가 메모리에 한 번에 들어오지 않는 경우(예: DBMS에서 테이블을 정렬할 때), 병합 정렬을 사용하여 디스크에서 데이터를 부분적으로 읽고 정렬 후 합치는 방식으로 사용됨.
    - 병합 정렬 구현 예시
        ```python
        def merge_sort(arr):
            if len(arr) <= 1:
                return arr
            
            # 중간 지점 찾기
            mid = len(arr) // 2
            left_half = merge_sort(arr[:mid])
            right_half = merge_sort(arr[mid:])
            
            return merge(left_half, right_half)

        def merge(left, right):
            sorted_arr = []
            i = j = 0
            
            # 두 배열을 병합하는 과정
            while i < len(left) and j < len(right):
                if left[i] < right[j]:
                    sorted_arr.append(left[i])
                    i += 1
                else:
                    sorted_arr.append(right[j])
                    j += 1
            
            # 남은 요소 추가
            sorted_arr.extend(left[i:])
            sorted_arr.extend(right[j:])
            
            return sorted_arr

        # 테스트 실행
        arr = [5, 3, 8, 4, 2, 7, 1, 6]
        sorted_arr = merge_sort(arr)
        print(sorted_arr)
        ```
    - 결론
	    - 병합 정렬은 분할 정복(Divide and Conquer) 개념을 활용한 정렬 알고리즘
	    - 시간 복잡도 O(n log n) 으로 비교적 안정적인 정렬 속도를 보장함
	    - 추가적인 공간(O(n)) 이 필요하기 때문에 메모리 사용량이 많을 수 있음
	    - 대용량 데이터 처리, 링크드 리스트 정렬, 외부 정렬 등에 유용함

- 퀵 정렬(Quick Sort)의 피벗 선택 방법과 성능에 미치는 영향 설명
    - 개요
        - 분할 정복(Divide and Conquer) 기법을 사용하는 효율적인 정렬 알고리즘
        - 피벗(Pivot)을 기준으로 배열을 두 개의 부분 배열로 나누고, 각각을 재귀적으로 정렬하는 방식으로 동작
    - 피벗 선택 방법 및 성능 영향
        - 개요
            - 피벗 선택 방식은 성능에 중요한 영향
            - 피벗을 잘못 선택하면 성능이 급격히 저하될 가능성 존재
        - 피벗 선택 방식
            - 첫 번째 원소 선택
                - 장점: 구현 간단
                - 단점
                    - 이미 정렬된 배열에서는 최악의 성능 발생(𝑂(𝑛²)) 발생
                    - 불균형 분할 가능성 높음
                - 적용 추천 상황
                    - 입력 데이터가 랜덤한 경우(정렬되지 않은 상태)
            - 마지막 원소 선택
                - 장점: 구현 간단
                - 단점: 첫 번째 원소 선택과 동일한 문제(정렬된 배열에서의 최악의 성능) 발생
                - 적용 추천 상황: 랜덤하게 섞인 데이터
            - 중앙 원소 선택
                - 장점: 일부 정렬된 데이터에서도 균형 잡힌 분할 가능
                - 단점: 특정 입력에 대해 여전히 불균형 분할 발생 가능
                - 적용 추천 상황: 정렬된 데이터가 일부 포함된 경우
            - 랜덤 피벗 선택
                - 장점
                    - 특정 패턴의 데이터에 영향을 덜 받음
                    - 평균적인 성능이 우수 (𝑂(𝑛 log 𝑛))
                - 단점: 난수 생성 비용 추가
                - 적용 추천 상황: 정렬 상태를 알 수 없는 다양한 데이터에서 안정적 성능 제공
            - 세 개의 값 중 중앙값 선택
                - 설명: 배열에서 첫 번째, 중앙, 마지막 요소를 선택하여 그중 중앙값을 피벗으로 사용
                - 장점
                    - 분할 균형을 맞추는 데 효과적
                    - 최악의 경우(𝑂(𝑛²))를 피할 확률이 높음
                - 단점: 비교 연산이 추가됨
                - 적용 추천 상황: 정렬된 데이터 또는 유사 정렬 데이터에서 사용 (일반적으로 가장 효율적)
    - 퀵 정렬의 성능 분석
        - 최선의 경우 (균형 분할): 𝑂(𝑛 log 𝑛)
        - 평균적인 경우: 𝑂(𝑛 log 𝑛)
            - 랜덤 피벗 또는 세 개의 중앙값 기법 사용하면 최악의 경우를 줄이고 평균적인 𝑂(𝑛 log 𝑛) 성능을 보장 가능
        - 최악의 경우 (불균형 분할): 𝑂(𝑛²)
            - 정렬된 배열에서 첫 번째 / 마지막 원소를 피벗으로 선택하면 발생
    - 결론: 최적의 피벗 선택 방법
        - 일반적인 경우: 랜덤 피벗 선택 또는 세 개의 중앙값(Median of Three) 방법 추천
        - 정렬된 데이터 가능성이 높다면: 세 개의 중앙값 선택
        - 데이터가 랜덤하게 섞여 있다면: 랜덤 피벗 선택
        - "Median of Three" 기법이 가장 안정적이며, 다양한 입력 데이터에서 성능이 가장 우수한 방법
  
- 힙 정렬(Heap Sort)의 개념과 구현 방법 설명
    - 개념
        - 완전 이진트리(Complete Binary Tree) 구조를 기반으로 하는 선택 정렬(Selection Sort)의 일종
        - 힙(Heap)을 활용하여 데이터를 정렬하는 알고리즘
            - 힙은 최대 힙 또는 최소 힙을 사용하여 데이터를 정렬하는 자료구조
            - 최대 힙: 부모 노드의 값이 자식 노드보다 크거나 같은 완전 이진 트리
            - 최소 힙: 부모 노드의 값이 자식 노드보다 작거나 같은 완전 이진 트리
            - 힙 정렬은 제자리 정렬(In-Place Sorting)이며 불안정 정렬(Unstable Sorting)에 해당
        - 참고: 완전 이진트리란 각 노드가 최대 2개의 자식 노드를 갖는 트리 형태의 자료구조, 마지막 레벨을 제외한 모든 노드는 완전히 채워져 있어야 함
    - 힙 정렬의 과정
        - 힙 생성(Build Heap)
            - 주어진 배열을 최대 힙 또는 최소 힙으로 변환
            - O(n)의 시간 복잡도
        - 정렬 과정(Sorting Process)
            - 최대 힙(또는 최소 힙)에서 루트 노드(최대값 또는 최소값)를 추출하고 마지막 원소와 교환한 후 다시 힙을 정리한다
            - 이 과정을 반복하여 정렬을 수행
            - O(n log n)의 시간 복잡도
    - 힙 정렬 구현 방법
        - 힙을 만드는 과정 (Heapify)
            ```python
            # 힙의 성질을 만족하도록 정리하는 과정 수행
            def heapify(arr, n, i):
                largest = i  # 현재 노드를 가장 큰 값으로 가정
                left = 2 * i + 1  # 왼쪽 자식 노드 인덱스
                right = 2 * i + 2  # 오른쪽 자식 노드 인덱스

                # 왼쪽 자식이 존재하고, 현재 노드보다 크다면 largest 변경
                if left < n and arr[left] > arr[largest]:
                    largest = left

                # 오른쪽 자식이 존재하고, 현재 노드보다 크다면 largest 변경
                if right < n and arr[right] > arr[largest]:
                    largest = right

                # largest가 변경되었으면 스왑 후 재귀적으로 heapify 수행
                if largest != i:
                    arr[i], arr[largest] = arr[largest], arr[i]
                    heapify(arr, n, largest)
            ```
        - 힙 정렬 구현
            ```python
            def heap_sort(arr):
                n = len(arr)

                # 1. 최대 힙을 구성 (Build Max Heap)
                for i in range(n // 2 - 1, -1, -1):
                    heapify(arr, n, i)

                # 2. 힙에서 요소를 하나씩 꺼내서 정렬
                for i in range(n - 1, 0, -1):
                    arr[i], arr[0] = arr[0], arr[i]  # 최댓값(루트 노드)과 마지막 요소를 교환
                    heapify(arr, i, 0)  # 힙 속성을 유지하도록 다시 heapify 수행

            # 실행 예제
            arr = [4, 10, 3, 5, 1]
            heap_sort(arr)
            print("정렬된 배열:", arr)
            ```
    - 시간 복잡도: 최선, 평균, 최악의 경우 모두 O(n log n)으로 일정
    - 힙 정렬의 장점과 단점
        - 장점
            - 시간 복잡도가 안정적(O(n log n)): 데이터의 정렬 여부에 관계없이 성능이 일정
            - 제자리 정렬(In-place Sort): 추가적인 메모리 공간이 거의 필요 없음 (O(1))
            - 느리지만 안정적인 성능: 최악의 경우에도 O(n log n) 성능을 보장
        - 단점
            - 불안정 정렬(Unstable Sort): 동일한 값을 가진 요소들의 상대적인 순서가 유지되지 않음
            - 캐시 친화적이지 않음: 다른 정렬 알고리즘(예: 퀵 정렬)에 비해 메모리 접근이 불규칙하여 실제 성능이 다소 떨어질 수 있음.
    - 결론
        - 힙 구조를 이용하여 O(n log n)의 시간 복잡도를 가지는 정렬 알고리즘
        - 힙을 구성한 후, 최댓값(또는 최솟값)을 루트에서 제거하며 정렬을 수행
        - 제자리 정렬이지만, 불안정 정렬이며 캐시 효율이 낮아 실제 성능은 퀵 정렬보다 다소 떨어질 수 있다.
        - 최악의 경우에도 O(n log n) 성능을 유지하므로 안정적인 성능이 필요한 경우 사용

- 기수 정렬(Radix Sort)과 카운팅 정렬(Counting Sort)의 차이점
    - 기수 정렬(Radix Sort)
        - 숫자의 각 자릿수를 기준으로 정렬을 수행
        - 비교 연산 없이 특정 범위 내에서 빠르게 정렬할 수 있음
        - 비교 정렬보다 효율적일 수 있지만, 정렬할 데이터가 크거나 자릿수가 많으면 성능이 저하될 수 있음.
	- 카운팅 정렬(Counting Sort)
        - 값의 개수를 세어 정렬하는 방법
        - 범위(k)가 작을 때 O(n) 시간 복잡도로 매우 빠르게 정렬할 수 있음.
        - 하지만 데이터 값의 범위가 너무 크면 메모리 사용량이 급증하는 단점이 있음.
    - 사용할 정렬 선택 기준
	    - 데이터의 범위가 작고 정수가 많으면 → 카운팅 정렬(Counting Sort)
	    - 데이터의 자릿수가 정해진 정수(예: 전화번호, 날짜)일 때 → 기수 정렬(Radix Sort)

- 정렬 알고리즘의 안정성(Stable Sort)
    - 개요
        - 동일한 값을 가진 요소들이 정렬 후에도 기존의 상대적인 순서를 유지하는 성질을 의미
        - 즉, 정렬 전후에 같은 값을 가진 데이터들의 순서가 유지되면 안정적인 정렬(Stable Sort)
        - 순서가 바뀔 가능성이 있으면 불안정한 정렬(Unstable Sort)
    - 안정적인 정렬의 예시
        - 다음과 같은 리스트를 키(key)를 기준으로 정렬한다고 가정
            - 원래 데이터 (정렬 전)
                - (5, A)
                - (3, B)
                - (3, C)
                - (8, D)
                - (5, E)
                    - (키, 값) 형태로 주어졌으며, 키(Key) 값이 동일한 경우, 기존 순서를 유지해야 안정적인 정렬
            - 안정적인 정렬 결과
                - 정렬 후 (Stable Sort)
                    - (3, B)
                    - (3, C)
                    - (5, A)
                    - (5, E)
                    - (8, D)
                - (5, A)가 (5, E)보다 앞에 있었으므로, 정렬 후에도 순서가 유지됨 → 안정적인 정렬
	            - (3, B)가 (3, C)보다 앞에 있었으므로, 정렬 후에도 순서가 유지됨
    - 불안정한 정렬의 예시
        - 만약 정렬 과정에서 같은 값(키)의 요소들의 순서가 바뀌면, 불안정한 정렬(Unstable Sort)
    - 안정적인 정렬과 불안정한 정렬 알고리즘
        - 안정적인 정렬 알고리즘 (Stable Sort)
        	- 버블 정렬 (Bubble Sort)
        	- 합병 정렬 (Merge Sort)
        	- 삽입 정렬 (Insertion Sort)
        	- 계수 정렬 (Counting Sort)
        	- 기수 정렬 (Radix Sort)
        - 불안정한 정렬 알고리즘 (Unstable Sort)
        	- 선택 정렬 (Selection Sort)
        	- 힙 정렬 (Heap Sort)
	        - 퀵 정렬 (Quick Sort, 일반적인 구현 방식)
	        - 쉘 정렬 (Shell Sort)
    - 정렬 안정성이 중요한 경우
    	- 데이터가 여러 개의 속성을 가지는 경우 (예: 키(Key) 값이 동일한 레코드)
	    - 예를 들어, 학생 데이터를 성적(점수)로 정렬하되, 같은 점수일 경우 원래 입력 순서를 유지하고 싶다면 안정적인 정렬을 사용해야 함.
	    - 멀티 단계 정렬 (Multi-pass Sorting)
	    - 예를 들어, 먼저 이름으로 정렬하고, 같은 이름이면 나이로 정렬하는 경우, 첫 번째 정렬의 순서를 유지하면서 두 번째 정렬을 수행해야 함.
    - 결론
	    - 안정적인 정렬(Stable Sort): 같은 값을 가진 요소들의 상대적인 순서가 유지됨.
	    - 불안정한 정렬(Unstable Sort): 같은 값을 가진 요소들의 순서가 바뀔 수 있음.
	    - 안정성이 중요한 경우, Merge Sort, Insertion Sort 등을 사용해야 하며, 불안정한 정렬도 적절한 방식으로 구현하면 안정적으로 만들 수 있음.

- O(n log n) 정렬 알고리즘 비교 및 특징 설명
    - 개요
        - 정렬 알고리즘 중 시간 복잡도가 O(n log n)인 알고리즘들은 효율적인 정렬 알고리즘으로 분류되며, 대부분의 실용적인 정렬에서 사용
        - 대표적 알고리즘
            - 병합 정렬 (Merge Sort): 평균 O(n log n) / 최악 O(n log n) / 공간 O(n) / 안정적 / 분할 정복, 추가 공간 필요
            - 퀵 정렬 (Quick Sort):	평균 O(n log n)	/ 최악 O(n²) (이미 정렬된 경우)	/ 공간 O(log n) (재귀 깊이)	/ 불안정 / 분할 정복, 피벗 선택 중요
                - 피벗 선택 중요함 
            - 힙 정렬 (Heap Sort): 평균 O(n log n) / 최악 O(n log n) / 공간	O(1) / 불안정 / 힙을 이용한 선택 정렬
            - 셸 정렬 (Shell Sort):	평균 O(n log n) / 최악 O(n²) / 공간 O(1) / 불안정 /	삽입 정렬의 개선형
                - 삽입 정렬의 개선 타입 
    - 병합 정렬 (Merge Sort)
        - 특징
	        - 분할 정복(Divide and Conquer) 방식 사용
	        - 배열을 절반으로 나누고, 각각을 정렬한 후 병합(Merge)
        	- 추가적인 메모리 공간 O(n) 필요
	        - 안정적인 정렬(Stable Sort)
	        - 입력 데이터의 초기 상태에 관계없이 O(n log n) 보장
        - 작동 방식
	        - 배열을 반으로 나눈다.
	        - 각각을 재귀적으로 정렬
	        - 두 개의 정렬된 배열을 병합(merge)
        - 장점
            - 항상 O(n log n) 보장 (최악의 경우에도 성능 유지)
            - 안정적인 정렬(Stable Sort)
            - 리스트가 연결 리스트인 경우 매우 효율적
        - 단점
            - 추가 메모리 O(n) 필요
            - 작은 배열에서는 비효율적 (Insertion Sort보다 느릴 수 있음)
    - 퀵 정렬 (Quick Sort)
        - 특징
	        - 분할 정복(Divide and Conquer) 방식 사용
	        - 피벗(Pivot)을 기준으로 작은 값과 큰 값을 분할한 후 재귀적으로 정렬
	        - 평균적으로 O(n log n)이지만, 최악의 경우 O(n²)
	        - 불안정한 정렬(Unstable Sort)
	        - 추가 메모리 필요 없음 (제자리 정렬, In-Place Sorting)
        - 작동 방식
	        - 피벗(Pivot)을 선택 (중앙값, 랜덤 선택 등)
	        - 피벗을 기준으로 작은 값과 큰 값으로 분할
	        - 각각을 재귀적으로 정렬
        - 장점
            - 제자리 정렬(In-Place Sort) → 추가 메모리 필요 없음
            - 빠른 평균 성능 (O(n log n))
            - 캐시 친화적, 실제 실행 속도가 빠름
        - 단점
            - 최악의 경우 O(n²) 발생 가능 (이미 정렬된 배열에서 피벗을 첫 번째 원소로 선택할 경우)
            - 불안정한 정렬 (동일한 값의 상대 순서 유지 불가)
            - 재귀 호출이 많아 깊은 재귀 호출 시 스택 오버플로우 발생 가능
    - 힙 정렬 (Heap Sort)
        - 특징
	        - 완전 이진 트리(Heap) 구조를 이용한 정렬
	        - 최대 힙(Max-Heap) 또는 최소 힙(Min-Heap)을 활용하여 정렬
	        - 항상 O(n log n) 보장
	        - 불안정한 정렬(Unstable Sort)
	        - 추가 메모리 사용이 거의 없음 (제자리 정렬, In-Place Sorting)
        - 작동 방식
	        - 주어진 배열을 최대 힙(Max Heap) 또는 최소 힙(Min Heap)으로 변환
	        - 루트 노드(최대/최소값)를 제거하고, 다시 힙을 조정
	        - 모든 요소가 정렬될 때까지 반복
        - 장점
            - 최악의 경우에도 O(n log n) 보장
            - 제자리 정렬 (In-Place Sorting), 추가 메모리 필요 없음
            - 배열이 거의 정렬된 상태에서도 일정한 성능 유지
        - 단점
            - 불안정한 정렬(Unstable Sort)
            - 캐시 친화적이지 않아 실제 실행 속도가 퀵 정렬보다 느림
            - 힙을 유지하는 연산이 많아 정렬 속도가 느릴 수 있음
    - 셸 정렬 (Shell Sort, 삽입 정렬의 개선 버전)
        - 특징
	        - 삽입 정렬(Insertion Sort)의 개선 버전
	        - 간격(Gap)을 점진적으로 줄여가며 부분 정렬을 수행
	        - 최선의 경우 O(n log n), 최악의 경우 O(n²)
	        - 불안정한 정렬(Unstable Sort)
	        - 추가 메모리 필요 없음 (제자리 정렬, In-Place Sorting)
        - 작동 방식
	        - 배열을 일정한 간격(Gap)으로 나눠 부분 정렬 수행
	        - 간격을 줄여가며 정렬 반복
	        - 간격이 1이 될 때까지 반복하여 완전한 정렬 수행
        - 장점
            - 삽입 정렬보다 빠르며, 거의 정렬된 배열에 유리
            - 제자리 정렬(In-Place Sorting), 추가 메모리 필요 없음
            - 간격 조정을 잘하면 퀵 정렬보다 빠를 수도 있음
        - 단점
            - 불안정한 정렬(Unstable Sort)
            - 최악의 경우 O(n²) 발생 가능
            - 최적의 간격(Gap) 설정이 어렵고 경험적으로 결정해야 함
    - 결론
        - 최선의 선택
	        - 병합 정렬: 안정성이 필요한 경우
	        - 퀵 정렬: 실제 실행 속도가 가장 빠름 (캐시 친화적)
	        - 힙 정렬: 최악의 경우에도 성능이 일정해야 하는 경우
	        - 셸 정렬: 거의 정렬된 데이터에서 삽입 정렬보다 효율적
    - 주의할 점
	    - 퀵 정렬은 피벗 선택을 잘못하면 O(n²) 성능 저하 가능
	    - 병합 정렬은 추가 메모리 필요
	    - 힙 정렬은 실제 실행 속도가 느릴 수 있음
    - 정리: 실제 구현에서는 퀵 정렬이 가장 빠른 경우가 많으며, 안정성이 필요한 경우 병합 정렬을 사용

- 비교 기반 정렬과 비비교 기반 정렬의 차이
    - 개요
        - 비교 기반 정렬 (Comparison-based Sorting) vs 비비교 기반 정렬 (Non-comparison-based Sorting)
    - 비교 기반 정렬 (Comparison-based Sorting)
        - 정의
            - 요소들을 서로 비교하여 정렬을 수행하는 알고리즘
            - 일반적으로 비교 연산(>, <)을 사용하여 정렬 순서를 결정
        - 시간 복잡도
            - 평균적으로 O(n log n)의 복잡도를 가짐 (최적 알고리즘 기준)
            - 최악의 경우 O(n²)까지 증가할 수도 있음 (예: 삽입 정렬, 선택 정렬)
        - 예제 알고리즘
            - O(n log n) 알고리즘
                - 병합 정렬 (Merge Sort)
                - 힙 정렬 (Heap Sort)
                - 퀵 정렬 (Quick Sort)
            - O(n²) 알고리즘
                - 삽입 정렬 (Insertion Sort)
                - 선택 정렬 (Selection Sort)
                - 버블 정렬 (Bubble Sort)
        - 한계점
            - 비교 연산을 수행하기 때문에 정렬의 하한(bound)인 Ω(n log n)이 존재
            - 비교 기반 정렬의 최적 시간 복잡도는 O(n log n) 이상일 수 없음
    - 비비교 기반 정렬 (Non-comparison-based Sorting)
        - 정의
            - 요소들을 비교 연산 없이 정렬하는 알고리즘
            - 일반적으로 데이터의 특성을 활용하여 직접 위치를 결정하는 방식
        - 시간 복잡도
            - 최적의 경우 O(n) 시간 복잡도로 정렬 가능
            - 특정 조건을 만족할 때만 효율적으로 동작
        - 예제 알고리즘
            - O(n) 알고리즘
                - 계수 정렬 (Counting Sort)
                - 기수 정렬 (Radix Sort)
                - 버킷 정렬 (Bucket Sort)
            - 한계점
                - 데이터의 크기나 분포에 따라 성능이 제한됨
                    - Counting Sort → 데이터의 값 범위가 너무 크면 메모리 낭비가 심함
                    - Radix Sort → 숫자 또는 특정 구조를 가진 데이터에만 효과적
                    - Bucket Sort → 특정 분포(예: 균등 분포)를 가정해야 효과적
    - 결론
        - 비교 기반 정렬은 일반적으로 범용적이고 정렬할 수 있는 데이터의 유형이 제한되지 않음
            - 단, 최적 시간 복잡도가 O(n log n) 이상으로 제한됨.
        - 비비교 기반 정렬은 특정한 조건(예: 정수 데이터, 작은 범위)에 적합하며 O(n)으로 정렬 가능
            - 메모리 사용량이 크거나, 데이터에 따라 효과적이지 않을 수 있음
        - 실제 사용 예시
            - 일반적인 정렬 → 퀵 정렬, 병합 정렬 (비교 기반)
            - 숫자만 포함된 데이터 정렬 → 기수 정렬, 계수 정렬 (비비교 기반)

- 선형 탐색(Linear Search)의 개념과 시간 복잡도
    - 선형 탐색(Linear Search) 개념
        - 배열이나 리스트에서 원하는 값을 찾을 때 처음부터 끝까지 차례대로 탐색하는 방법
        - 순차 탐색(Sequential Search)이라고도 하며, 정렬되지 않은 배열에서도 사용할 수 있는 가장 기본적인 탐색 알고리즘
    - 선형 탐색 알고리즘 동작 방식
        - 배열의 첫 번째 요소부터 마지막 요소까지 순차적으로 확인
        - 찾고자 하는 값과 같은 값이 나오면 해당 위치(인덱스) 반환
        - 끝까지 탐색했는데 값이 없으면 -1 또는 null 반환
    - 시간 복잡도 분석
        - 최악 시간 복잡도: O(n)
	        - 최악의 경우, 배열의 마지막 요소(혹은 없음)까지 탐색해야 함 → O(n)
	        - 예: [10, 20, 30, 40, 50]에서 60을 찾는 경우 → 모든 요소를 탐색해야 함
        - 최선 시간 복잡도: O(1)
	        - 첫 번째 요소가 정답일 경우, 한 번의 비교만으로 찾을 수 있음 → O(1)
        - 평균 시간 복잡도: O(n)
	        - 평균적으로 배열의 절반(n/2) 정도를 탐색해야 하므로 O(n/2)이지만, 빅오 표기법에서는 O(n)으로 표현
    - 결론
        - 정렬되지 않은 데이터에서는 유용하지만, 정렬된 데이터에서는 이진 탐색(O(log n))이 더 효율적

- 이진 탐색(Binary Search)의 개념과 구현 방법
    - 이진 탐색(Binary Search) 개념
        - 정렬된 배열에서 특정 값을 찾을 때, 중간값을 기준으로 탐색 범위를 반씩 줄여가며 검색하는 알고리즘
    - 선형 탐색(Linear Search)과의 차이점
	    - 선형 탐색(Linear Search): O(n) → 처음부터 끝까지 순차적으로 탐색
	    - 이진 탐색(Binary Search): O(log n) → 탐색 범위를 절반씩 줄이며 검색
    - 이진 탐색의 조건:
	    - 반드시 정렬된 배열이어야 함 (오름차순 또는 내림차순)
    - 이진 탐색(Binary Search) 알고리즘 동작 원리
        - 배열의 가운데 값(mid)을 선택
        - 찾는 값(target)과 비교
	        - target == mid 값 → 찾았으므로 해당 인덱스 반환
	        - target < mid 값 → 왼쪽 절반에서 다시 탐색 (오른쪽은 버림)
	        - target > mid 값 → 오른쪽 절반에서 다시 탐색 (왼쪽은 버림)
        - 위 과정을 반복하며 탐색 범위를 계속 반으로 줄여 나감
        - 찾지 못하면 -1 반환
    - 구현 방식: Iterative or Recursive
    - 이진 탐색의 시간 복잡도 분석
        - 시간 복잡도
	        - 최악/평균 시간 복잡도: O(log n)
	        - 탐색 범위를 절반으로 줄여가므로 log₂ n만큼만 비교
	        - 최선 시간 복잡도: O(1) (첫 번째 중간 값이 정답일 경우)
        - 공간 복잡도
	        - 반복문 방식: O(1) (추가적인 메모리 사용 없음)
	        - 재귀 방식: O(log n) (재귀 호출 스택 사용)
    - 결론
        - 이진 탐색(Binary Search)은 정렬된 배열에서만 사용 가능하지만, O(log n)의 빠른 검색 속도를 제공
	    - 반복문 방식이 재귀 방식보다 메모리 효율적이므로 보통 반복문 방식이 더 선호됨
	    - 대량의 데이터에서는 이진 탐색이 선형 탐색보다 훨씬 효율적

- 깊이 우선 탐색(DFS)과 너비 우선 탐색(BFS)의 차이점
    - 개요
        - DFS(Depth-First Search)와 BFS(Breadth-First Search)는 그래프 탐색 알고리즘
        - 주어진 그래프에서 특정 노드를 방문하는 방법을 정의
    - 차이점
        - 탐색 방식	한 방향으로 끝까지 탐색 후 백트래킹	모든 인접 노드를 먼저 탐색
    - DFS, BFS 차이점 상세
        - 자료 구조
            - DFS: 스택(Stack) 또는 재귀(Recursive)
            - BFS: 큐(Queue)
        - 메모리 사용량
            - DFS: 경로를 저장하는 데 적은 메모리 사용
            - BFS: 방문해야 할 모든 노드를 저장하므로 메모리 사용량이 큼
        - 속도
            - DFS: 깊게 탐색 후 백트래킹으로 비효율적일 수 있음
            - BFS: 항상 최단 경로를 보장하므로 빠를 수 있음
        - 최단 경로 보장 여부
            - DFS: 최단 경로를 보장하지 않음
            - BFS: 최단 경로를 보장함(가중치가 없는 그래프에서)
        - 순환(사이클) 탐색
            - DFS: 사이클 존재 여부를 쉽게 판별 가능
            - BFS: 사이클 판별에 불리함
        - 트리 탐색 방식
            - DFS: 전위 순회(Preorder Traversal) 방식과 유사
            - BFS: 레벨 순회(Level Order Traversal) 방식과 유사

- 전위 순회(Preorder Traversal)과 레벨 순회(Level Order Traversal) 방식
    - 전위 순회 (Preorder Traversal)
        - 개요
            - 방문 순서: 루트 → 왼쪽 서브트리 → 오른쪽 서브트리
            - 순회 방식: 깊이 우선 탐색(DFS, Depth-First Search) 방식 중 하나
        - 전위 순회 순서 (좌측 먼저 다 돌고 우측 순회)
            - A → B → D → E → C → F
        - 코드 (Python)
            ```python
            class Node:
                def __init__(self, value):
                    self.value = value
                    self.left = None
                    self.right = None

            # 전위 순회 함수 (Preorder Traversal)
            def preorder_traversal(node):
                if node:
                    print(node.value, end=" ")      # 1. 루트 방문
                    preorder_traversal(node.left)   # 2. 왼쪽 서브트리 탐색
                    preorder_traversal(node.right)  # 3. 오른쪽 서브트리 탐색

            # 트리 생성
            root = Node('A')
            root.left = Node('B')
            root.right = Node('C')
            root.left.left = Node('D')
            root.left.right = Node('E')
            root.right.right = Node('F')

            # 실행
            preorder_traversal(root)
            # 출력: A B D E C F
            ```
    - 레벨 순회 (Level-order Traversal)
        - 개요
            - 방문 순서: 트리의 각 레벨(층) 을 왼쪽에서 오른쪽으로 차례대로 탐색
            - 순회 방식: 너비 우선 탐색(BFS, Breadth-First Search)
        - 레벨 순회 순서
            - A → B → C → D → E → F
        - 코드 (Python, BFS 활용)
            ```python
            # 레벨 순회 함수 (Level-order Traversal)
            def level_order_traversal(root):
                if not root:
                    return

                queue = deque([root])  # 큐에 루트 노드 삽입

                while queue:
                    node = queue.popleft()  # 큐에서 노드 제거
                    print(node.value, end=" ")

                    if node.left:
                        queue.append(node.left)  # 왼쪽 자식 노드 추가
                    if node.right:
                        queue.append(node.right) # 오른쪽 자식 노드 추가

            # 실행
            level_order_traversal(root)
            # 출력: A B C D E F
            ```
    - 전위 순회 vs 레벨 순회 비교
        - 전위 순회 (Preorder Traversal): 루트 → 왼쪽 → 오른쪽 / DFS 방식, 재귀적
        - 레벨 순회 (Level-order Traversal): 각 층을 왼쪽 → 오른쪽 순서 / BFS 방식, 큐 사용
    - 정리
        - 전위 순회 (Preorder Traversal): 루트 먼저 방문, 깊이 우선 탐색(DFS)
        - 레벨 순회 (Level-order Traversal): 층별로 방문, 너비 우선 탐색(BFS)

- DFS와 BFS가 각각 유리한 문제 유형을 설명
    - DFS가 유리한 문제 유형
        - 문제 유형
            - 그래프가 깊거나 가지 수가 많을 때 (메모리를 절약할 수 있음)
            - 모든 경로를 탐색해야 하는 경우 (예: 백트래킹 문제)
            - 재귀적으로 탐색할 수 있는 문제 (예: 퍼즐, 미로 탐색)
            - 사이클 탐색이 필요한 경우 (예: 강결합 요소(SCC) 찾기)
            - 위상 정렬 (Topological Sorting)
        - 예제 문제
	        - 미로 탐색 (한 경로를 끝까지 탐색하는 방식)
	        - 경로의 모든 조합을 찾는 문제 (백트래킹: N-Queen, 순열/조합)
	        - 게임 맵 탐색 (예: 체스의 나이트 이동 가능성 확인)
	        - 사이클 탐색 (예: 유니온-파인드를 활용한 사이클 탐지)
    - BFS가 유리한 문제 유형
        - 문제 유형
	        - 최단 경로를 찾아야 하는 문제 (가중치 없는 그래프에서)
	        - 계층적 탐색이 필요한 경우 (예: SNS 친구 추천, 네트워크 탐색)
	        - 모든 가능한 상태를 탐색해야 하는 문제 (예: 최소 이동 횟수)
	        - 가중치가 없는 최단 거리 문제 (예: 전염병 확산 시뮬레이션)
        - 예제 문제
	        - 최단 경로 찾기 (예: 미로에서 출구까지 최소 거리)
	        - SNS 친구 추천 (예: 페이스북의 친구 추천 시스템)
	        - 최소 횟수로 이동하는 문제 (예: 체스판 나이트 이동 최소 횟수)
	        - 토마토 익히기 문제 (BFS를 이용한 퍼지는 문제)
    - 결론
        - DFS: 경로 탐색, 백트래킹, 모든 경우의 수를 따지는 문제에서 유리함
	    - BFS: 최단 거리, 레벨 탐색, 최적 경로를 찾는 문제에서 유리함

- 해시 테이블(Hash Table)의 구조와 충돌 해결 방법
    - 해시 테이블(Hash Table) 구조
        - 해시 테이블은 키(Key) 를 해시 함수(Hash Function)를 사용하여 특정한 해시 값(Hash Value, 인덱스) 로 변환한 후, 해당 위치에 데이터를 저장하는 자료 구조
	        - 해시 함수(Hash Function): 입력된 키를 특정한 인덱스로 변환하는 함수
	        - 버킷(Bucket): 해시 함수에 의해 결정된 저장 공간 (배열의 특정 위치)
	        - 충돌(Collision): 서로 다른 키가 같은 해시 값(인덱스)을 가질 때 발생하는 문제
        - 예제
            - hash("apple") -> index 3
            - hash("banana") -> index 5
            - hash("grape") -> index 3  # 충돌 발생
    - 해시 충돌(Collision) 해결 방법
        - 체이닝(Chaining) (Separate Chaining)
            - 개념
                - 같은 인덱스에 여러 개의 데이터를 연결 리스트(Linked List) 또는 트리(Tree) 로 저장하는 방법
                - 새로운 키가 동일한 해시 값을 가지면 리스트의 끝에 추가됨
            - 장점: 충돌이 많아도 테이블의 크기를 변경하지 않아도 됨
            - 단점: 연결 리스트 사용으로 인해 메모리 오버헤드 발생 가능
            - 예제
                ```python
                class HashTable:
                    def __init__(self, size):
                        self.size = size
                        self.table = [[] for _ in range(size)]

                    def hash_function(self, key):
                        return hash(key) % self.size

                    def insert(self, key, value):
                        index = self.hash_function(key)
                        self.table[index].append((key, value))  # 체이닝 방식 사용

                    def search(self, key):
                        index = self.hash_function(key)
                        for k, v in self.table[index]:
                            if k == key:
                                return v
                        return None  # 데이터가 없으면 None 반환
                ```
        - 개방 주소법(Open Addressing)
            - 개요
                - 충돌이 발생하면 다른 빈 슬롯을 찾아 데이터를 저장하는 방법
                - 해시 테이블 내의 공간을 최대한 활용할 수 있음
            - 방법
	            - 선형 탐사(Linear Probing)
	                - 충돌이 발생하면 다음 버킷(인덱스 + 1)을 검사하여 빈 공간을 찾음
	                - 단점: 연속된 데이터 저장으로 인해 클러스터링(Cluster)이 발생하여 성능 저하
                    - 해시 충돌 해결 예시
                        - 해시 테이블 크기: 5
                        - hash("apple") -> index 2
                        - hash("banana") -> index 2 (충돌 발생) -> index 3로 저장
                - 이차 탐사(Quadratic Probing)
	                - 충돌이 발생하면 1², 2², 3², … 형태로 탐색하여 빈 공간을 찾음
	                - 선형 탐사보다 클러스터링이 줄어들지만, 저장 공간이 제한될 수 있음
	            - 이중 해싱(Double Hashing)
	                - 해시 함수를 두 개 사용하여 충돌이 발생할 경우 두 번째 해시 값을 이용하여 새로운 인덱스를 찾음
	                - 예: index = (hash1(key) + i * hash2(key)) % table_size
	                - 클러스터링 문제를 줄이면서도 공간 활용을 최적화
                - 예제
                    ```python
                    class OpenAddressingHashTable:
                        def __init__(self, size):
                            self.size = size
                            self.table = [None] * size

                        def hash_function(self, key):
                            return hash(key) % self.size

                        def insert(self, key, value):
                            index = self.hash_function(key)
                            while self.table[index] is not None:  # 선형 탐사 방식
                                index = (index + 1) % self.size
                            self.table[index] = (key, value)

                        def search(self, key):
                            index = self.hash_function(key)
                            while self.table[index] is not None:
                                if self.table[index][0] == key:
                                    return self.table[index][1]
                                index = (index + 1) % self.size
                            return None
                    ```
    - 해시 테이블 성능 분석
	    - 탐색 평균 시간 복잡도: O(1)
	    - 충돌 발생 시 최악의 경우: O(n)
	    - 부하율(Load Factor, α) = (저장된 요소 개수) / (테이블 크기)
	    - α가 너무 크면 충돌이 많이 발생 → 성능 저하
	    - 보통 α < 0.7 정도로 유지하며, 필요 시 해시 테이블 크기를 늘려 리해싱(Rehashing) 수행
    - 해시 테이블의 활용 사례
	    - 데이터베이스 인덱싱
	    - 캐싱 시스템 (LRU Cache)
	    - 컴파일러 (심볼 테이블)
	    - DNS 조회
	    - 비밀번호 저장 (해시 함수 적용 후 저장)
    - 결론
        - 해시 테이블은 빠른 데이터 검색이 가능한 효율적인 자료 구조
        - 충돌(Collision) 해결 방식에 따라 성능이 달라짐
        - 일반적으로 체이닝(Chaining) 방식과 개방 주소법(Open Addressing) 이 많이 사용되며, 상황에 맞게 적절한 충돌 해결 기법을 선택 필요

- 해싱(Hashing)에서 충돌(Collision) 해결 방법
    - 해싱과 충돌(Collision) 개념
        - 해싱(Hashing)
            - 키(Key) 를 해시 함수(Hash Function)를 사용하여 특정 해시 값(Hash Value, 인덱스) 으로 변환한 후, 해당 위치에 데이터를 저장하는 방식
            - 해시 함수가 서로 다른 키에 대해 동일한 해시 값을 반환하는 경우 충돌(Collision) 이 발생할 수 있음
    - 충돌 해결 방법
        - 체이닝(Chaining, Separate Chaining)
            - 개요
                - 체이닝 방식은 같은 해시 값을 갖는 데이터를 연결 리스트(Linked List) 또는 트리(Tree) 로 저장하는 방법
                - 즉, 충돌이 발생하면 해당 인덱스의 리스트에 데이터를 추가하여 해결
            - 장점
	            - 해시 테이블의 크기를 초과하여 데이터를 저장할 수 있음
	            - 충돌이 많아도 테이블 크기를 변경할 필요가 없음
            - 단점
                - 연결 리스트로 인해 추가적인 메모리 사용
                - 탐색 시 리스트의 길이가 길어지면 성능 저하 발생 가능 (O(n))
            - 체이닝 방식 예제 (파이썬)
                ```python
                class HashTable:
                    def __init__(self, size):
                        self.size = size
                        self.table = [[] for _ in range(size)]  # 각 버킷에 리스트 생성

                    def hash_function(self, key):
                        return hash(key) % self.size

                    def insert(self, key, value):
                        index = self.hash_function(key)
                        self.table[index].append((key, value))  # 체이닝 방식으로 저장

                    def search(self, key):
                        index = self.hash_function(key)
                        for k, v in self.table[index]:  # 리스트 내에서 탐색
                            if k == key:
                                return v
                        return None  # 데이터가 없으면 None 반환
                ```
        - 개방 주소법(Open Addressing)
            - 개요
                - 충돌이 발생했을 때 다른 빈 슬롯을 찾아 데이터를 저장하는 방법
                - 해시 테이블 자체에서 충돌을 해결
                - 주요 방식은 선형 탐사(Linear Probing), 이차 탐사(Quadratic Probing), 이중 해싱(Double Hashing)
            - 주요 방식
                - 선형 탐사(Linear Probing)
                    - 개요
                        - 충돌이 발생하면 다음 버킷(인덱스 + 1)을 검사 하여 빈 공간을 찾음
                        - index = (해시 값 + 1) % 테이블 크기 를 반복
                    - 장점
                        - 구현이 간단하고 캐시 성능이 좋음
                    - 단점
                        - 연속적인 데이터 저장으로 인해 1차 클러스터링(Primary Clustering) 문제 발생
                        - (충돌이 많아지면 빈 공간을 찾는 시간이 길어짐)
                    - 예제
                        ```python
                        class LinearProbingHashTable:
                            def __init__(self, size):
                                self.size = size
                                self.table = [None] * size

                            def hash_function(self, key):
                                return hash(key) % self.size

                            def insert(self, key, value):
                                index = self.hash_function(key)
                                while self.table[index] is not None:  # 충돌 발생 시 다음 위치 탐색
                                    index = (index + 1) % self.size
                                self.table[index] = (key, value)

                            def search(self, key):
                                index = self.hash_function(key)
                                while self.table[index] is not None:
                                    if self.table[index][0] == key:
                                        return self.table[index][1]
                                    index = (index + 1) % self.size
                                return None
                        ```
                - 이차 탐사(Quadratic Probing)
                    - 개요
                        - 충돌이 발생하면 탐색 위치를 1², 2², 3², … 형태로 증가시켜 빈 공간을 찾음
                        - index = (해시 값 + i²) % 테이블 크기
                    - 장점
                        - 선형 탐사보다 클러스터링이 덜 발생 (2차 클러스터링(Secondary Clustering) 해결)
                    - 단점
                        - 저장할 공간이 부족할 경우 무한 루프 발생 가능
                        - 해시 테이블 크기가 소수(Prime Number) 여야 효과적
                - 이중 해싱(Double Hashing)
                    - 개요
                        - 두 개의 해시 함수를 사용하여 충돌을 해결하는 방식
                        - 충돌 발생 시 두 번째 해시 함수를 적용하여 이동 간격을 결정
                        - index = (해시 값 + i * hash2(key)) % 테이블 크기
                    - 장점
                        - 클러스터링 문제를 최소화
                        - 탐색 성능이 우수
                    - 단점
                        - 추가적인 해시 함수 연산이 필요하여 성능이 다소 느릴 수 있음
                    - 예제
                        ```python
                        class DoubleHashingHashTable:
                            def __init__(self, size):
                                self.size = size
                                self.table = [None] * size

                            def hash_function1(self, key):
                                return hash(key) % self.size

                            def hash_function2(self, key):
                                return 1 + (hash(key) % (self.size - 1))  # 두 번째 해시 값 계산

                            def insert(self, key, value):
                                index = self.hash_function1(key)
                                step = self.hash_function2(key)

                                while self.table[index] is not None:  # 충돌 발생 시 이중 해싱 적용
                                    index = (index + step) % self.size
                                self.table[index] = (key, value)

                            def search(self, key):
                                index = self.hash_function1(key)
                                step = self.hash_function2(key)

                                while self.table[index] is not None:
                                    if self.table[index][0] == key:
                                        return self.table[index][1]
                                    index = (index + step) % self.size
                                return None
                        ```
    - 충돌 해결 방법 비교
        - 체이닝 (Chaining)
            - 장점: 테이블 크기 초과 저장 가능, 동적 크기 조정 가능
            - 단점: 추가 메모리 사용 (연결 리스트)
        - 선형 탐사 (Linear Probing)
            - 장점: 구현이 간단하고 캐시 성능이 좋음	
            - 단점: 1차 클러스터링 발생
        - 이차 탐사 (Quadratic Probing)
            - 장점: 클러스터링 감소
            - 단점: 무한 루프 가능성 존재
        - 이중 해싱 (Double Hashing)	
            - 장점: 클러스터링 최소화, 탐색 성능 우수	
            - 단점: 추가 해시 연산 필요
    - 결론
        - 충돌이 많을 경우: 체이닝(Chaining) 방식이 유리함
        - 메모리를 절약하고 싶을 경우: 개방 주소법(Open Addressing)
        - 빠른 탐색이 필요할 경우: 이중 해싱(Double Hashing)

- 그래프의 정의와 주요 용어(정점, 간선, 가중치 등)
    - 개요
        - 그래프(Graph)는 정점(Vertex, 노드)과 간선(Edge, 엣지) 으로 구성된 자료 구조
        - 정점 간의 관계를 표현할 수 있어, 네트워크, 경로 탐색, 최단 경로 문제 등 다양한 분야에서 활용
    - 정의
	    - 그래프 G(V, E) 는 정점(Vertex)들의 집합 V 와 간선(Edge)들의 집합 E 로 이루어짐
	    - 정점 V 는 그래프의 개별 요소 (노드)
	    - 간선 E 는 정점 간의 연결 (관계, 경로)
    - 예제
        - V = {A, B, C, D, E}
        - E = {(A, B), (A, C), (B, D), (C, D), (D, E)}
    - 그래프의 주요 용어
        - 정점(Vertex, Node)
            - 그래프에서 데이터를 저장하는 기본 단위
            - 보통 V 로 표현됨
            - 예: V = {A, B, C, D, E}

        - 간선(Edge)
            - 정점 간의 관계를 나타내는 연결선
            - 보통 E 로 표현됨
            - 예: E = {(A, B), (A, C), (B, D), (C, D), (D, E)}

        - 가중치(Weight)
            - 간선에 할당된 값(거리, 비용 등)
            - 가중 그래프(Weighted Graph)에서 사용됨
            - 예: (A, B, 4), (B, D, 2)

        - 방향성(Direction)
            - 무방향 그래프(Undirected Graph): 간선에 방향이 없음 (A, B) == (B, A)
            - 방향 그래프(Directed Graph, Digraph): 간선에 방향이 있음 (A → B) ≠ (B → A)

        - 차수(Degree)
            - 진입 차수(In-degree): 특정 정점으로 들어오는 간선 개수
            - 진출 차수(Out-degree): 특정 정점에서 나가는 간선 개수

        - 경로(Path)
            - 한 정점에서 다른 정점으로 가는 연결된 간선들의 집합
            - 예: A → C → D → E

        - 사이클(Cycle)
            - 같은 정점으로 돌아오는 경로가 존재하는 경우
            - 예: A → B → D → C → A

        - 연결 그래프(Connected Graph)
            - 모든 정점이 하나 이상의 경로로 연결된 그래프
            - 비연결 그래프는 일부 정점이 연결되지 않음

        - 트리(Tree)
            - 사이클이 없는 연결 그래프
            - 루트(Root) 노드를 기준으로 계층 구조를 가짐
    - 그래프의 종류
        - 무방향 그래프 (Undirected Graph)
	        - 간선에 방향이 없는 그래프
            - 예: {A, B}, {B, C}, {C, A}
        - 방향 그래프 (Directed Graph)
	        - 간선에 방향이 있는 그래프
            - 예: A → B, B → C
        - 가중 그래프 (Weighted Graph)
	        - 간선마다 가중치(비용, 거리, 시간)가 있는 그래프
            - 예: A → B (4), B → C (2)
    - 그래프의 표현 방법
        - 인접 행렬(Adjacency Matrix)
	        - 2차원 배열을 사용하여 정점 간의 연결 정보를 저장
	        - 간선이 존재하면 1 또는 가중치, 없으면 0
	        - 공간 복잡도 O(V²) (정점이 많을수록 메모리 낭비)
            - 예제
                ```python
                graph = [
                    [0, 1, 1, 0, 0],  # A
                    [1, 0, 0, 1, 0],  # B
                    [1, 0, 0, 1, 0],  # C
                    [0, 1, 1, 0, 1],  # D
                    [0, 0, 0, 1, 0]   # E
                ]
                ```
        - 인접 리스트(Adjacency List)
            - 딕셔너리 또는 리스트를 사용하여 정점과 연결된 노드를 저장
            - 공간 복잡도 O(V + E) (메모리 절약)
            - 예제
                ```python
                graph = {
                    'A': ['B', 'C'],
                    'B': ['A', 'D'],
                    'C': ['A', 'D'],
                    'D': ['B', 'C', 'E'],
                    'E': ['D']
                }
                ```
    - 그래프의 활용
        - 네트워크 연결 분석 (SNS, 통신망)
        - 지도 및 경로 탐색 (네비게이션, GPS)
        - 웹 크롤링 (웹페이지 간 연결 구조)
        - 인공지능 (지식 그래프, 그래프 신경망)
        - 추천 시스템 (유사한 사용자 또는 제품 연결)
    - 결론
        - 그래프는 정점(Vertex)과 간선(Edge)으로 구성됨
        - 방향성, 가중치에 따라 다양한 그래프 형태가 존재
        - 인접 행렬, 인접 리스트 방식으로 저장
        - SNS, 경로 탐색, AI 등 다양한 분야에서 사용됨

- 인접 행렬(Adjacency Matrix)과 인접 리스트(Adjacency List)의 차이점
    - 개요
        - 그래프를 표현하는 방식에는 인접 행렬(Adjacency Matrix) 과 인접 리스트(Adjacency List) 두 가지 대표적인 방법
        - 각 방법은 메모리 사용량, 탐색 속도, 공간 복잡도, 구현 방식에서 차이가 있음
    - 인접 행렬(Adjacency Matrix)
        - 설명
	        - 2차원 배열(행렬) 을 사용하여 그래프의 연결 관계를 저장하는 방식
	        - 노드 간 연결 여부를 0(없음)과 1(있음) 또는 가중치 값으로 표현
        - 특징
            - 장점
                - 노드 간 간선 여부를 O(1) 시간에 빠르게 확인 가능
                - 행렬이므로 구현이 간단
                - 가중 그래프에서 간선의 가중치를 쉽게 표현 가능
            - 단점
                - 메모리 사용량이 O(V²) 로 큼 (간선이 적은 희소 그래프에서는 비효율적)
                - 연결된 노드를 찾을 때 O(V) 시간이 필요함
                - 동적 크기 조정이 어렵고, 불필요한 공간 낭비가 발생
    - 인접 리스트(Adjacency List)
        - 설명
	        - 연결 리스트(리스트, 딕셔너리) 를 사용하여 그래프의 인접 정점을 저장하는 방식
	        - 연결된 노드만 저장하므로 희소 그래프(Sparse Graph)에 적합
        - 특징
            - 장점
                - 메모리 효율적 (간선 개수가 적을 때 O(V + E) 로 공간 절약)	
                - 연결된 노드 탐색이 빠름 (노드의 개수와 비례)
                - 동적 크기 조정이 쉬움
            - 단점
                - 노드 간 연결 여부를 확인하려면 O(E) 시간이 필요
                - 구현이 상대적으로 복잡
                - 밀집 그래프(Dense Graph)에서는 탐색 속도가 느릴 수 있음
        - 예제
            ```python
            # 인접 리스트 구현 (딕셔너리 사용)
            graph = {
                'A': ['B', 'C'],
                'B': ['A', 'C', 'D'],
                'C': ['A', 'B'],
                'D': ['B']
            }

            # A와 연결된 노드 확인
            print(graph['A'])  # ['B', 'C']

            # A와 D가 연결되어 있는지 확인
            print('D' in graph['A'])  # False (연결되지 않음)
            ```
    - 언제 사용해야 하는가
        - 인접 행렬(Adjacency Matrix) 사용 경우:
	        - 밀집 그래프(Dense Graph) (간선 개수가 많음)
	        - 노드 간 연결 여부를 빠르게 확인해야 할 때
	        - 가중 그래프에서 가중치 저장이 필요할 때
        - 인접 리스트(Adjacency List) 사용 경우:
	        - 희소 그래프(Sparse Graph) (간선 개수가 적음)
	        - 연결된 노드만 저장하여 메모리를 절약해야 할 때
	        - 연결된 노드 탐색이 많은 경우 (DFS, BFS 등)
    - 결론
	    - 그래프의 밀집도에 따라 적절한 표현 방법을 선택해야 함
	    - 인접 행렬: 노드 수가 많고 간선이 많은 경우 (빠른 접근 O(1))
	    - 인접 리스트: 간선이 적고 메모리를 절약해야 하는 경우 (효율적 저장 O(V + E))
        - 일반적으로 희소 그래프에서는 인접 리스트를 사용하고, 밀집 그래프에서는 인접 행렬이 더 유리

- 최단 경로 문제란 무엇인가? 대표적인 알고리즘
    - 최단 경로 문제 정의
        - 최단 경로 문제(Shortest Path Problem)는 그래프(Graph)에서 한 정점에서 다른 정점으로 이동하는 가장 짧은 경로를 찾는 문제
        - 네트워크 경로 최적화, GPS 네비게이션, 통신 라우팅 등 다양한 분야에서 활용
    - 최단 경로 알고리즘의 종류
        - 그래프의 특성(가중치, 음수 간선 포함 여부 등)에 따라 단일 출발점(single-source) 또는 전체 쌍(all-pairs) 최단 경로로 나뉨
        - 종류
            - 다익스트라(Dijkstra): 양수 가중치 그래프 / O(V²) (우선순위 큐 사용 시 O(E log V)) / 음수 가중치 X, 빠른 탐색
            - 벨만-포드(Bellman-Ford): 음수 가중치 가능	/ O(VE)	/ 최단 경로 + 음수 사이클 감지
            - 플로이드-워셜(Floyd-Warshall): 전체 쌍 최단 경로 / O(V³) / 모든 정점 간 최단 거리 계산
            - 존슨(Johnson): 전체 쌍 최단 경로 + 음수 간선 / O(V² log V + VE) / 효율적 (음수 가중치 포함 가능)
        - 다익스트라(Dijkstra) 알고리즘
            - 특징
                - 음수 가중치가 없는 그래프에서 최단 경로를 찾는 알고리즘
                - 우선순위 큐(Priority Queue) 를 활용하면 O(E log V) 의 시간 복잡도를 가짐
                - 네트워크 경로 탐색, 지도 서비스(GPS) 등에 활용됨
            - 다익스트라 동작 방식
                - 출발 노드에서 거리를 0 으로 설정하고, 나머지 노드는 무한대(∞)로 초기화
                - 가장 가까운 노드(최단 거리)를 선택하여, 인접한 노드의 최단 거리를 갱신
                - 모든 노드를 방문할 때까지 반복
            - 다익스트라 구현 (파이썬, 우선순위 큐 사용)
                ```python
                import heapq

                def dijkstra(graph, start):
                    distances = {node: float('inf') for node in graph}
                    distances[start] = 0
                    pq = [(0, start)]  # (거리, 노드)

                    while pq:
                        current_distance, current_node = heapq.heappop(pq)
                        if distances[current_node] < current_distance:
                            continue  # 이미 처리된 경우 건너뜀

                        for neighbor, weight in graph[current_node].items():
                            distance = current_distance + weight

                            if distance < distances[neighbor]:  # 더 짧은 경로 발견 시 갱신
                                distances[neighbor] = distance
                                heapq.heappush(pq, (distance, neighbor))
                                
                    return distances

                # 그래프 정의 (딕셔너리)
                graph = {
                    'A': {'B': 1, 'C': 4},
                    'B': {'A': 1, 'C': 2, 'D': 5},
                    'C': {'A': 4, 'B': 2, 'D': 1},
                    'D': {'B': 5, 'C': 1}
                }

                print(dijkstra(graph, 'A'))  # {'A': 0, 'B': 1, 'C': 3, 'D': 4}
                ```
        - 벨만-포드(Bellman-Ford) 알고리즘
            - 특징
                - 음수 가중치를 포함한 그래프에서도 최단 경로 계산 가능
                - 시간 복잡도 O(VE) 로 다익스트라보다 느림
                - 음수 사이클(negative cycle) 검출 가능
            - 벨만-포드 동작 방식
                - 출발 노드에서 모든 노드까지의 거리를 무한대(∞) 로 초기화, 시작 노드는 0
                - 모든 간선을 V-1번 반복 하면서 거리 갱신
                - V번째 반복에서도 갱신이 발생하면 음수 사이클 존재
            - 벨만-포드 구현 (파이썬)
                ```python
                def bellman_ford(graph, start):
                    distances = {node: float('inf') for node in graph}
                    distances[start] = 0

                    for _ in range(len(graph) - 1):
                        for node in graph:
                            for neighbor, weight in graph[node].items():
                                if distances[node] + weight < distances[neighbor]:
                                    distances[neighbor] = distances[node] + weight

                    # 음수 사이클 검출
                    for node in graph:
                        for neighbor, weight in graph[node].items():
                            if distances[node] + weight < distances[neighbor]:
                                return "음수 사이클 발견"

                    return distances

                graph = {
                    'A': {'B': 1, 'C': 4},
                    'B': {'C': -2, 'D': 5},
                    'C': {'D': 1},
                    'D': {}
                }

                print(bellman_ford(graph, 'A'))  # {'A': 0, 'B': 1, 'C': -1, 'D': 0}
                ```
        - 플로이드-워셜(Floyd-Warshall) 알고리즘
            - 특징
                - 모든 정점 쌍의 최단 경로 를 구하는 알고리즘
                - 시간 복잡도 O(V³) 로 큰 그래프에서는 비효율적
                - 동적 프로그래밍(DP) 기반 알고리즘
            - 플로이드-워셜 동작 방식
                - 2차원 배열을 초기화하여, 자기 자신으로 가는 경로는 0, 나머지는 ∞로 설정
                - 모든 노드를 중간 경로로 사용하여 최단 거리 갱신 (i → k → j)
            - 플로이드-워셜 구현 (파이썬)
                ```python
                def floyd_warshall(graph):
                    nodes = list(graph.keys())
                    dist = {node: {n: float('inf') for n in nodes} for node in nodes}

                    for node in nodes:
                        dist[node][node] = 0

                    for node in graph:
                        for neighbor, weight in graph[node].items():
                            dist[node][neighbor] = weight

                    for k in nodes:
                        for i in nodes:
                            for j in nodes:
                                dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])

                    return dist

                graph = {
                    'A': {'B': 3, 'C': 7},
                    'B': {'A': 3, 'C': 1, 'D': 5},
                    'C': {'A': 7, 'B': 1, 'D': 2},
                    'D': {'B': 5, 'C': 2}
                }

                print(floyd_warshall(graph))
                ```
        - 존슨(Johnson) 알고리즘
            - 특징
                - 모든 정점 간 최단 경로 + 음수 가중치 가능
                - 벨만-포드 + 다익스트라 결합 (O(V² log V + VE))
                - 효율적이지만, 복잡도가 높아 큰 그래프에 적합
    - 결론
        - 음수 가중치가 없고 단일 출발점 → 다익스트라
        - 음수 가중치 존재 → 벨만-포드
        - 전체 쌍 최단 경로 → 플로이드-워셜
        - 희소 그래프에서 전체 쌍 최단 경로 → 존슨 알고리즘

- 음수 가중치(음수 간선, Negative Weight)의 필요성
    - 개요
        - 그래프에서 간선(Edge)에 음수 가중치(Weight) 를 사용하는 이유는 특정한 의미를 가지는 비용을 표현하기 위해서임
        - 주로 비용 감소, 할인 개념, 시간 차감, 신용 시스템 등을 나타낼 때 사용
    - 음수 가중치를 사용하는 이유
        - 비용 감소를 나타내기 위해
            - 음수 가중치는 비용이 줄어드는 상황을 나타낼 수 있음
            - 예: 상품 할인, 세금 감면, 비용 절감 등
            - 예제: 항공권 최적화
                - A → B 비행기 티켓 가격이 200달러
                - A → C → B 경유하면 할인(-50달러) 적용 → C를 경유하는 것이 더 저렴
                - 음수 가중치를 이용하면 최적의 경로(할인 포함) 를 찾을 수 있음
        - 시간 단축 또는 보너스를 표현
	        - 어떤 작업을 수행할 때, 음수 가중치를 사용하면 특정 작업이 완료될 때 보너스(시간 단축 등) 가 적용되는 것을 표현할 수 있음
	        - 예: 프로젝트 일정 단축, 공정 효율화, 생산성 증가
            - 예제: 프로젝트 일정 관리
                - A → B 작업은 10일 소요
                - A → C → B 경로를 선택하면 자동화 시스템(-3일) 적용 → 더 빠른 경로 선택 가능
        - 금융 거래 시스템에서 잔고 계산
            - 은행 거래, 신용 거래에서 음수 가중치를 사용하여 부채(빚) 또는 크레딧(할인, 캐시백) 을 표현 가능
            - 예: 마일리지 시스템, 신용 카드 포인트 적립
            - 예제: 은행 계좌 시스템
                - A 계좌에서 B 계좌로 100달러 송금 (기본 수수료 5달러)
                - VIP 고객은 수수료 할인(-2달러) 적용 → 음수 가중치 사용 가능
        - 게임에서 버프/디버프 시스템 표현
            - 게임에서 음수 가중치를 이용하면, 이동 속도 감소(디버프), 체력 감소, 공격력 증가(버프) 등을 표현 가능
            - 예: 특정한 아이템을 장착하면 이동 속도가 -10% 감소 (디버프)
            - 예제: 게임 속 이동 경로
                - A → B까지 기본 이동 속도 10초
                - 특정 능력(음수 가중치 -3초)을 사용하면 7초에 이동 가능
    - 음수 가중치를 다룰 때의 주의점
        - 다익스트라(Dijkstra) 알고리즘에서는 사용 불가능
            - 다익스트라는 한 번 방문한 노드의 최단 경로가 확정되므로, 음수 간선이 있으면 잘못된 결과를 낼 수 있음
            - 음수 가중치가 있는 경우 → 벨만-포드(Bellman-Ford) 알고리즘 사용
        - 음수 사이클(Negative Cycle) 문제
            - 경로를 반복할수록 무한히 비용이 감소하는 루프가 생기면 최단 경로를 찾을 수 없음
            - 벨만-포드 알고리즘을 사용하면 음수 사이클이 존재하는지 검출 가능
        - 금융 시스템에서는 오버플로우 위험
            - 음수 가중치를 잘못 적용하면, 무한한 크레딧이 발생하는 등 금융 시스템에서 오류 발생 가능
            - 신용 한도 설정, 부채 상한 설정 등의 추가 검증 필요
    - 결론
        - 음수 가중치는 비용 절감, 시간 단축, 할인, 금융 거래 시스템, 게임 버프/디버프 등에서 활용되며, 현실 세계에서 중요한 개념
        - 그러나, 다익스트라 알고리즘은 음수 가중치를 처리하지 못하며, 음수 사이클 문제를 조심해야 될 필요
        - 따라서, 음수 가중치가 포함된 그래프에서는 벨만-포드 알고리즘을 활용하여 최단 경로를 찾고, 음수 사이클을 탐지하는 것이 중요

- 다익스트라(Dijkstra) 알고리즘의 개념과 적용 사례
    - 개념
        - 가중 그래프(Weighted Graph) 에서 한 정점에서 다른 모든 정점까지의 최단 경로를 찾는 알고리즘
        - 그래프에서 간선(Edge)의 가중치가 음수가 아닌 경우에 사용할 수 있으며, 탐욕적(Greedy) 방법을 기반으로 동작
    - 동작 원리
	    - 출발 노드를 설정하고, 해당 노드의 최단 거리를 0으로 설정한다. 다른 모든 노드는 무한대(∞)로 설정한다.
	    - 방문하지 않은 노드 중에서 최단 거리가 가장 짧은 노드를 선택한다.
	    - 해당 노드를 기준으로 인접한 노드들의 최단 거리를 갱신한다.
	    - 위 과정을 모든 노드를 방문할 때까지 반복한다.
    - 핵심 아이디어
        - 방문하지 않은 정점 중에서 가장 작은 비용의 정점을 선택하고, 이를 바탕으로 다른 정점의 최단 경로를 갱신하는 방식
    - 구현 방법
	    - 배열(Array) 사용 (시간 복잡도: O(V²))
	        - 매 단계에서 방문하지 않은 정점 중 최단 거리를 찾기 위해 순차 탐색을 사용
	        - 정점 개수가 많으면 비효율적
	    - 우선순위 큐(Priority Queue) 사용 (시간 복잡도: O(E log V))
	        - 힙(Heap) 자료구조를 활용하여 최단 거리를 갱신할 때 우선순위 큐를 사용
	        - 일반적으로 최소 힙(Min-Heap) 을 이용해 최단 거리가 가장 짧은 정점을 빠르게 찾는다.
	        - V는 정점(Vertex)의 개수, E는 간선(Edge)의 개수이며, 이 방식이 더 효율적
    - Python 코드 예제 (우선순위 큐 사용)
        ```python
        import heapq

        def dijkstra(graph, start):
            INF = float('inf')
            # 최단 거리 테이블 초기화
            distances = {node: INF for node in graph}
            distances[start] = 0
            pq = [(0, start)]  # (거리, 노드)

            while pq:
                current_distance, current_node = heapq.heappop(pq)

                # 이미 처리된 노드라면 무시
                if current_distance > distances[current_node]:
                    continue

                for neighbor, weight in graph[current_node].items():
                    distance = current_distance + weight

                    # 더 짧은 경로를 발견하면 갱신
                    if distance < distances[neighbor]:
                        distances[neighbor] = distance
                        heapq.heappush(pq, (distance, neighbor))

            return distances

        # 그래프 예제 (딕셔너리 사용)
        graph = {
            'A': {'B': 6, 'D': 1},
            'B': {'A': 6, 'D': 2, 'E': 2, 'C': 5},
            'C': {'B': 5, 'E': 5},
            'D': {'A': 1, 'B': 2, 'E': 1},
            'E': {'D': 1, 'B': 2, 'C': 5}
        }

        start_node = 'A'
        shortest_paths = dijkstra(graph, start_node)
        print(shortest_paths)
        ```
	- 출력 예시
        - {'A': 0, 'B': 3, 'C': 8, 'D': 1, 'E': 2}
        - → A에서 다른 노드까지의 최단 거리
    - 적용 사례
        - 개요: 다익스트라 알고리즘은 현실에서 다양한 최단 경로 문제를 해결하는 데 사용
        - 실 적용 사례
	        - 네비게이션 시스템 (GPS, 지도 서비스)
                - 출발지에서 목적지까지의 최단 경로 탐색
                - 도로의 가중치(거리, 교통량, 시간 등)를 고려하여 최적의 경로 추천
	        - 네트워크 라우팅 프로토콜 (Computer Network Routing)
                - OSPF(Open Shortest Path First) 프로토콜에서 최적의 데이터 전송 경로 계산
                - 네트워크에서 패킷이 가장 빠르게 전달될 수 있는 경로 선택
	        - 로봇 경로 계획 (Pathfinding for Robots)
                - 장애물이 있는 환경에서 최단 경로를 찾기 위한 로봇 경로 계획
                - 예: 자율주행 차량의 주행 경로 탐색
	        - 게임 AI (Game AI Pathfinding)
                - 캐릭터가 맵에서 목표 지점까지 도달하는 최적의 경로를 계산
                - 예: A 알고리즘과 함께 사용됨
    - 다익스트라 알고리즘의 한계점
	    - 음수 가중치가 있는 그래프에서는 사용할 수 없다.
	        - 예: 벨만-포드(Bellman-Ford) 알고리즘이 음수 간선을 처리할 수 있음.
	    - 한 번 계산하면 가중치가 변하면 다시 계산해야 한다.
	        - 실시간 경로 탐색이 필요할 경우에는 A* 알고리즘과 같은 대안 고려.
	    - 최악의 경우 모든 노드를 방문해야 하므로 O(V²)의 성능 한계가 있다.
	        - 간선이 많은 경우에는 O(E log V)의 우선순위 큐(힙) 기반 다익스트라를 사용해야 함.
    - 정리
        - 다익스트라 알고리즘은 한 정점에서 다른 모든 정점까지의 최단 경로를 구하는 알고리즘
        - 탐욕적(Greedy) 방식으로 동작하며, 우선순위 큐(힙)을 사용하면 O(E log V)로 최적화 가능
        - GPS, 네트워크 라우팅, 게임 AI, 로봇 경로 탐색 등 다양한 분야에서 활용됨
        - 음수 가중치를 처리할 수 없으며, 가중치 변경 시 다시 계산해야 하는 한계가 있음

- 벨만-포드(Bellman-Ford) 알고리즘의 개념과 한계
    - 벨만-포드(Bellman-Ford) 알고리즘 개념
        - 벨만-포드 알고리즘은 음수 가중치를 포함하는 그래프에서 최단 경로를 찾을 수 있는 알고리즘
        - 다익스트라(Dijkstra) 알고리즘과는 다르게, 음수 간선이 존재하는 그래프에서도 올바른 최단 경로를 계산할 수 있다는 것이 큰 장점
    - 작동 원리
	    - 시작 정점(source)에서 모든 정점까지의 거리를 무한대(∞)로 초기화하고, 시작 정점의 거리는 0으로 설정한다.
	    - (V-1)번 반복하면서 모든 간선을 확인하며, 거리 배열을 갱신한다.
	    - 간선 (u, v, w)에서 distance[v] > distance[u] + w이면 distance[v]를 업데이트한다.
	    - 추가적으로 한 번 더 모든 간선을 확인하여 값이 갱신된다면, 음수 사이클이 존재함을 판별할 수 있다.
    - 벨만-포드 알고리즘의 한계
	    - 시간 복잡도가 O(VE)로 느림
	        - 다익스트라 알고리즘(힙 사용 시 O(E log V))보다 느리며, 특히 간선이 많은 밀집 그래프에서는 비효율적
	        - 벨만-포드는 모든 간선을 (V-1)번 확인해야 하므로 O(VE)의 시간 복잡도를 가진다.
	    - 음수 가중치 순환(음수 사이클)이 존재하면 최단 경로를 정의할 수 없음
	        - 음수 가중치가 있는 그래프에서는 다익스트라 알고리즘이 사용할 수 없지만, 벨만-포드는 이를 처리할 수 있다.
	        - 하지만, 음수 사이클이 존재하면 무한히 작은 값으로 최단 경로가 계속 갱신되므로 정상적인 최단 경로를 찾을 수 없다.
	        - 이를 방지하기 위해 V번째 반복에서 값이 갱신되면 음수 사이클이 존재함을 판별한다.
	    - 효율성이 다익스트라보다 낮음
	        - 다익스트라는 우선순위 큐(힙)를 사용하여 빠르게 최단 경로를 탐색하지만, 벨만-포드는 모든 간선을 확인하는 방식이라 성능이 떨어진다.
	        - 따라서 음수 간선이 없는 경우라면 다익스트라를 사용하는 것이 일반적으로 더 효율적이다.
    - 벨만-포드의 활용 사례
	    - 음수 가중치를 포함한 네트워크에서 최단 경로 계산
	    - 음수 사이클(예: 금융 거래에서 환율 차익)이 있는지 판별
	    - 거리 제한이 있는 그래프(예: 교통 시스템)에서 최단 경로 탐색
    - 결론
        - 벨만-포드는 음수 간선을 포함할 수 있는 상황에서 안전한 최단 경로 탐색이 필요할 때 적절한 알고리즘이며, 그렇지 않은 경우에는 다익스트라를 사용하는 것이 효율적

- 플로이드-워셜(Floyd-Warshall) 알고리즘의 개념과 활용 사례
    - 플로이드-워셜(Floyd-Warshall) 알고리즘 개념
        - 모든 정점 쌍 간의 최단 경로(All-Pairs Shortest Paths)를 구하는 알고리즘
        - 음수 가중치가 있는 그래프에서도 동작할 수 있지만, 음수 사이클이 존재하면 정상적인 최단 경로를 찾을 수 없다.
    - 작동 원리
        - 동적 프로그래밍(DP) 기반의 알고리즘으로, D[i][j]를 i에서 j로 가는 최단 거리로 정의하고, 점진적으로 더 나은 경로를 찾는 방식으로 수행
	    - k번 정점을 경유하는 방식으로 최단 거리를 갱신하는 점진적인 갱신 기법을 사용
	    - 초기화
	        - D[i][i] = 0 (자기 자신까지의 거리는 0)
	        - 간선 (u, v, w)가 주어지면 D[u][v] = w, 존재하지 않으면 INF(무한대)로 설정.
	    - 점진적인 갱신
	        - 중간 정점 k를 하나씩 추가하면서, D[i][j] 값을 업데이트
	        - D[i][j] = min(D[i][j], D[i][k] + D[k][j])
	        - 즉, i → j로 직접 가는 거리보다 i → k → j를 거쳐 가는 거리가 더 짧으면 갱신.
	    - 최종 결과
	        - 모든 정점 쌍 간의 최단 거리가 구해진다.
	        - 만약 D[i][i] < 0인 경우, 음수 사이클이 존재함을 의미
    - 플로이드-워셜 알고리즘의 시간 복잡도
	    - O(V³): 3중 반복문을 사용하여 정점 개수 V에 대해 V×V×V 연산을 수행
	    - V가 100 이하일 때 현실적으로 사용 가능
        - 큰 그래프에서는 다익스트라(우선순위 큐 사용, O(E log V))를 활용하는 것이 일반적
    - 플로이드-워셜 활용 사례
	    - 네트워크 라우팅 최적화
	        - 모든 장치(노드) 간의 최소 지연 시간(최단 거리)을 계산하는 데 사용
	        - 예: 인터넷 라우팅 프로토콜(예: OSPF의 일부 계산 방식)
	    - 도시 간 최단 거리 계산
	        - 도로 네트워크에서 각 도시 간 최단 이동 거리를 사전에 계산하여 빠르게 조회 가능
	    - 게임 개발
	        - 맵의 모든 위치 간의 최단 경로를 미리 계산하여 실시간 탐색을 빠르게 처리
	    - 교통 분석 및 물류 시스템
	        - 물류센터 간 최적 이동 경로를 찾고, 전체적인 물류 비용을 최소화하는 데 활용.
	    - 사회적 네트워크 분석
	        - 특정 사용자 간 최단 연결 경로 분석(예: 친구 추천 시스템).
    - 플로이드-워셜 알고리즘 정리
	    - 모든 정점 쌍 간 최단 경로를 찾는 알고리즘.
	    - 음수 가중치를 포함한 그래프에서도 동작하나, 음수 사이클이 있으면 문제 발생.
	    - 시간 복잡도 O(V³)로, 작은 그래프(100개 이하의 정점)에서 주로 사용.
	    - 네트워크, 교통, 게임, 물류 최적화 등에서 활용.
        - 즉, 정점 개수가 적은 경우(V ≲ 100) 모든 쌍의 최단 경로를 한 번에 구해야 할 때 가장 적합한 알고리즘

- 최소 신장 트리(Minimum Spanning Tree, MST)
    - 정의
        - 그래프의 모든 정점을 포함하면서, 간선의 가중치 합이 최소가 되는 트리
        - 주어진 연결 그래프(모든 정점이 연결됨)에서 모든 정점을 연결하는 부분 그래프 중, 간선 가중치의 합이 최소인 트리를 찾는 것이 목표
    - MST의 특징
	    - 트리(Tree) 구조
	        - MST는 사이클이 존재하지 않는 연결 그래프이며, N개의 정점을 포함하면 반드시 N-1개의 간선을 갖는다.
	    - 최소 비용
	        - 가능한 모든 신장 트리 중에서 간선의 가중치 합이 최소가 되는 트리
	    - 연결 그래프에서만 정의됨
	        - 그래프가 연결되지 않은 경우 MST를 만들 수 없다.
    - 최소 신장 트리의 활용 사례
        - MST는 여러 실생활 문제에서 효율적인 연결 구조를 설계하는 데 사용
        - 활용 사례
	        - 네트워크 구축(인터넷, 전기, 도로, 철도 등)
                - 최소 비용으로 모든 도시를 연결하는 전력망 또는 도로망을 설계할 때 사용.
                - 인터넷 통신망을 최소한의 비용으로 구축하는 경우에도 활용.
            - 배관 및 회로 설계
                - 배관 시스템에서 최소 비용으로 모든 지점을 연결하는 최적의 설계를 찾는 문제.
                - 전자 회로에서 최소한의 전선으로 연결을 최적화하는 문제.
            - 클러스터링(데이터 분석 및 그래프 이론)
                - MST를 사용하여 군집(Cluster)을 만들거나 데이터 간 유사성을 분석하는 데 활용.
            - 이미지 처리 및 패턴 인식
                - 최소 신장 트리를 사용하여 이미지에서 윤곽선을 감지하거나, 패턴을 그룹화하는 작업에 응용.
    - 최소 신장 트리를 찾는 알고리즘
        - 개요
            - MST를 찾는 대표적인 알고리즘으로 크루스칼(Kruskal) 알고리즘과 프림(Prim) 알고리즘 존재
        - 알고리즘
            - 크루스칼(Kruskal) 알고리즘
                - 간선을 정렬한 후, 최소 가중치의 간선부터 선택하는 방식(그리디 알고리즘)
                - 서로소 집합(Disjoint Set, Union-Find)을 사용하여 사이클 여부를 판별.
                - 시간 복잡도: O(E log E) (간선 개수 정렬 O(E log E) + 유니온-파인드 O(E α(V)))
                - 적용: 간선 개수가 적은 희소 그래프(Sparse Graph)에 적합.
            - 프림(Prim) 알고리즘
                - 임의의 정점에서 시작하여, 연결된 간선 중 최소 가중치를 선택하는 방식(그리디 알고리즘)
                - 우선순위 큐(Heap)를 사용하여 최소 가중치를 효율적으로 선택.
                - 시간 복잡도: O(E log V) (힙을 사용하는 경우)
    - 정리
	    - MST(최소 신장 트리)는 모든 정점을 연결하는 간선의 가중치 합이 최소인 트리
	    - 네트워크 구축, 회로 설계, 데이터 클러스터링 등 다양한 분야에서 활용
	    - 크루스칼 알고리즘과 프림 알고리즘이 대표적인 MST 알고리즘
	    - 크루스칼: 간선을 정렬 후 선택 → 희소 그래프에 적합
	    - 프림: 정점에서 출발해 최소 간선을 선택 → 밀집 그래프에 적합
        - 즉, 그래프에서 최소 비용으로 모든 정점을 연결하는 문제를 해결할 때 MST를 활용하면 된다

- 크루스칼(Kruskal) 알고리즘의 개념과 구현 방법
    - 크루스칼(Kruskal) 알고리즘 개념
        - 최소 신장 트리(MST, Minimum Spanning Tree)를 찾는 대표적인 알고리즘 중 하나로, 그리디(Greedy) 알고리즘을 기반으로 함
        - 주어진 그래프에서 모든 정점을 최소 비용으로 연결하는 트리를 구성하는 것이 목표
        - 간선의 가중치를 기준으로 정렬한 후, 최소 비용의 간선부터 추가하면서 사이클을 방지하는 방식으로 동작
    - 크루스칼 알고리즘의 동작 원리
	    - 모든 간선을 가중치 기준으로 오름차순 정렬
	    - 사이클이 발생하지 않는 경우에만 간선을 추가하면서 최소 신장 트리를 구성
	        - 이를 위해 유니온-파인드(Union-Find, Disjoint Set) 자료구조를 사용하여 사이클 여부를 판별
	    - N-1개의 간선이 선택되면 종료한다. (N은 정점의 개수)
    - 크루스칼 알고리즘의 구현 방법
	    - 간선 리스트를 가중치 기준으로 정렬
	    - 유니온-파인드(Union-Find) 자료구조 초기화
	        - 각 정점을 자기 자신을 부모로 설정 (대표 노드 초기화)
	    - 정렬된 간선을 하나씩 확인하면서 MST에 추가
	        - 두 정점이 같은 집합에 속해 있지 않으면(사이클이 발생하지 않으면) MST에 포함
	        - 포함 후 유니온 연산 수행(두 집합을 하나로 합침)
	    - MST의 간선 개수가 (N-1)개가 되면 종료
    - 크루스칼 알고리즘의 시간 복잡도
	    - O(E log E) (간선 정렬: O(E log E) + 유니온-파인드 연산 O(E α(V)), 여기서 α(V)는 아커만 함수의 역함수로 거의 O(1))
	    - 간선 개수 E가 많을수록 정렬 비용이 크지만, 일반적으로 매우 효율적
    - 크루스칼 알고리즘 구현 (Python)
        ```python
        # 크루스칼 알고리즘 구현 (Union-Find 사용)
        class UnionFind:
            def __init__(self, n):
                self.parent = list(range(n))  # 자기 자신을 부모로 초기화
                self.rank = [0] * n           # 랭크(트리 깊이) 초기화
            
            def find(self, x):
                if self.parent[x] != x:  # 루트 노드 찾기 (경로 압축)
                    self.parent[x] = self.find(self.parent[x])
                return self.parent[x]
            
            def union(self, x, y):
                root_x = self.find(x)
                root_y = self.find(y)
                
                if root_x != root_y:  # 사이클이 발생하지 않을 경우 병합
                    if self.rank[root_x] > self.rank[root_y]:
                        self.parent[root_y] = root_x
                    elif self.rank[root_x] < self.rank[root_y]:
                        self.parent[root_x] = root_y
                    else:
                        self.parent[root_y] = root_x
                        self.rank[root_x] += 1
                    return True  # Union 성공
                return False  # 이미 같은 집합 (사이클 발생)

        def kruskal(n, edges):
            edges.sort(key=lambda x: x[2])  # 간선을 가중치 기준으로 정렬
            uf = UnionFind(n)  # 유니온-파인드 초기화
            mst = []  # 최소 신장 트리 (MST) 저장 리스트
            mst_cost = 0  # MST의 총 비용
            
            for u, v, weight in edges:
                if uf.union(u, v):  # 사이클이 발생하지 않는 경우 MST에 추가
                    mst.append((u, v, weight))
                    mst_cost += weight
                    if len(mst) == n - 1:  # MST 완성 (N-1개의 간선)
                        break
            
            return mst, mst_cost  # MST의 간선 리스트와 총 비용 반환

        # 사용 예제 (정점 개수: 4, 간선 리스트)
        edges = [
            (0, 1, 1),
            (0, 2, 3),
            (1, 2, 1),
            (1, 3, 4),
            (2, 3, 2)
        ]

        n = 4  # 정점 개수
        mst, total_cost = kruskal(n, edges)

        print("최소 신장 트리의 간선:", mst)
        print("최소 신장 트리의 총 비용:", total_cost)
        ```
    - 크루스칼 알고리즘의 활용 사례
	    - 네트워크 구축(전기, 통신, 도로, 철도 등)
	        - 최소한의 비용으로 도시(노드)들을 연결하는 도로망 또는 전력망 설계
	        - 예: 인터넷 망을 구축할 때, 최소한의 케이블 비용으로 연결
	    - 클러스터링(데이터 분석 및 그래프 이론)
	        - 그래프 기반 클러스터링(예: 소셜 네트워크 분석)
	    - 이미지 처리 및 패턴 인식
	        - 이미지에서 객체의 경계를 찾거나 연결을 분석할 때 활용
	    - 배관 및 회로 설계
	        - 최소한의 전선으로 모든 회로를 연결하는 최적 설계
    - 크루스칼 vs 프림 비교
        - 크루스칼:	O(E log E), 간선을 정렬한 후 작은 것부터 선택, 간선 개수가 적은 희소 그래프(Sparse Graph)에 적합
        - 프림:	O(E log V), 임의의 정점에서 시작하여 최소 간선을 선택, 간선이 많은 밀집 그래프(Dense Graph)에 적합
    - 정리
	    - 크루스칼 알고리즘은 그리디(Greedy) 알고리즘을 기반으로 최소 신장 트리를 찾는 방법.
	    - 간선의 가중치를 기준으로 정렬 후, 유니온-파인드(Union-Find)를 이용하여 사이클을 방지하면서 MST를 구성.
	    - 시간 복잡도 O(E log E)로, 간선이 적은 희소 그래프에서 효율적.
	    - 네트워크 구축, 클러스터링, 배관 설계 등 다양한 분야에서 활용.
        - 즉, 크루스칼 알고리즘은 최소 비용으로 그래프를 연결하는 최적의 해법을 제공하며, 특히 간선 개수가 적은 경우 매우 효율적

- 프림(Prim) 알고리즘과 크루스칼 알고리즘의 차이점
    - 프림(Prim) 알고리즘 vs 크루스칼(Kruskal) 알고리즘 비교 개요
        - 프림(Prim)과 크루스칼(Kruskal) 알고리즘은 최소 신장 트리(MST, Minimum Spanning Tree)를 찾는 대표적인 알고리즘
        - 두 알고리즘은 작동 방식과 최적화 방식에서 차이
    - 개념적 차이
        - 프림(Prim)	
            - 임의의 시작 정점에서 출발하여, 가장 적은 비용의 간선을 추가하면서 MST를 확장
            - 정점 중심(Edge-Growing 방식) → MST에 포함된 정점과 연결된 최소 가중치 간선을 선택
        - 크루스칼(Kruskal)
            - 모든 간선을 정렬한 후, 최소 비용의 간선을 하나씩 추가하여 MST를 구성(사이클 방지)
            - 간선 중심(Sorting 방식) → 가중치가 가장 작은 간선부터 선택하면서 트리를 구축
    - 알고리즘 동작 방식
        - 프림(Prim) 알고리즘 동작 방식
	        - 임의의 정점에서 시작하여 MST 집합(트리)에 포함
	        - 현재 MST에 포함된 정점과 연결된 최소 가중치 간선을 선택
	        - 해당 간선의 도착 정점을 MST에 추가
	        - N-1개의 간선이 선택될 때까지 반복
	            - 우선순위 큐(힙, Priority Queue)를 사용하면 O(E log V)로 최적화 가능
        - 크루스칼(Kruskal) 알고리즘 동작 방식
            - 모든 간선을 가중치 기준으로 오름차순 정렬
            - 사이클이 발생하지 않는 경우에만 간선을 추가하면서 MST를 구성
                - 이를 위해 유니온-파인드(Union-Find) 자료구조를 활용하여 사이클 여부를 판별
            - N-1개의 간선이 선택되면 종료.
                - 간선 정렬이 핵심이므로 O(E log E).
    - 시간 복잡도 비교
        - 프림(Prim): O(E log V), 우선순위 큐(Heap) 사용 시 빠름, 정점 중심
        - 크루스칼(Kruskal): O(E log E), 간선 정렬이 주된 연산, 간선 중심
    - 프림, 크루스칼 비교
	    - 프림 알고리즘은 간선이 많을수록(밀집 그래프) 더 유리하다.
	    - 크루스칼 알고리즘은 간선이 적을수록(희소 그래프) 더 유리하다.
	    - 일반적으로 E log E ≈ E log V이므로, 차이는 크지 않지만 그래프 밀도에 따라 적합성이 달라짐.
    - 언제 어떤 알고리즘을 사용할까?
        - 프림(Prim), 밀집 그래프(Dense Graph, E ≈ V²)
        - 크루스칼(Kruskal), 희소 그래프(Sparse Graph, E ≈ V)
        - 프림은 정점 중심이라서 간선이 많은 그래프(완전 그래프 등)에 적합.
	    - 크루스칼은 간선 중심이라서 간선이 적은 그래프(희소 그래프)에 적합.
    - 결론
	    - 프림 알고리즘: 한 정점에서 출발하여 MST를 확장해 나가는 방식 → 간선이 많은 밀집 그래프에 적합
	    - 크루스칼 알고리즘: 간선을 정렬한 후, 최소 비용 간선을 하나씩 선택하는 방식 → 간선이 적은 희소 그래프에 적합
        - 즉, 프림 알고리즘은 네트워크, 크루스칼 알고리즘은 클러스터링이나 도로 설계 같은 문제에서 더 적합

- KMP(Knuth-Morris-Pratt) 문자열 검색 알고리즘의 개념과 구현 방법
    - KMP(Knuth-Morris-Pratt) 문자열 검색 알고리즘 개념
        - KMP 알고리즘은 문자열 내에서 특정 패턴을 효율적으로 찾는 문자열 검색 알고리즘
        - O(N + M)의 시간 복잡도로 동작하여, 단순 문자열 검색(O(NM))보다 빠름.
    - KMP 알고리즘의 핵심 개념
	    - 불필요한 비교 최소화
	        - 단순 비교 방식(Brute Force)에서는 패턴이 불일치하면 처음부터 다시 검사하지만,
	        - KMP는 이미 일치한 부분 정보를 활용하여 불필요한 비교를 건너뜀.
	    - 부분 일치 테이블(Partial Match Table, π 배열) 사용
	        - 패턴 내부에서 자기 자신과 일치하는 접두사(prefix)와 접미사(suffix)의 길이를 저장.
	        - 이를 활용하여 불일치 발생 시 최적의 위치로 점프.
    - KMP 알고리즘의 동작 과정
	    - 부분 일치 테이블(π 배열) 생성
	        - 패턴에서 자기 자신을 포함한 접두사와 접미사가 같은 최대 길이를 저장.
	        - 이를 활용해 패턴 내에서 점프할 위치를 계산.
	    - 문자열 검색
	        - 패턴과 본문을 비교하면서, 불일치가 발생하면 π 배열을 이용해 패턴을 이동.
	        - 중복 비교를 피하고, 이미 일치한 부분을 재활용.
    - KMP 알고리즘 세부 동작 과정
        - 부분 일치 테이블(π 배열) 생성
            - 구현 원리
	            - pi[i]는 패턴의 0~i까지 부분 문자열에서, 접두사와 접미사가 일치하는 최대 길이를 저장.
	            - π 테이블은 패턴 내부에서 점프할 위치를 결정하는 데 사용됨.
            - 예제: “ABABCABAB”
        - 문자열 검색 과정
	        - j는 패턴의 현재 인덱스
	        - i는 본문 문자열의 현재 인덱스
	            - text[i] == pattern[j]이면 둘 다 다음 문자로 이동.
	            - text[i] != pattern[j]이면 π 배열을 참고하여 점프.
	                - j = π[j - 1]로 이동하여 불필요한 비교를 최소화.
	            - 패턴이 끝까지 일치하면, i - M + 1 위치에서 패턴이 발견됨.
    - KMP 알고리즘 구현 (Python)
        ```python
        def compute_pi(pattern):
            """ 부분 일치 테이블 (π 배열) 생성 """
            m = len(pattern)
            pi = [0] * m
            j = 0  # 접두사 접미사 비교 위치
            
            for i in range(1, m):
                while j > 0 and pattern[i] != pattern[j]:  # 불일치 시, j를 이전 π 값으로 점프
                    j = pi[j - 1]
                
                if pattern[i] == pattern[j]:  # 일치하면 j 증가
                    j += 1
                    pi[i] = j
            
            return pi

        def kmp_search(text, pattern):
            """ KMP 문자열 검색 알고리즘 """
            n, m = len(text), len(pattern)
            pi = compute_pi(pattern)
            j = 0  # 패턴의 현재 위치
            result = []  # 패턴이 발견된 위치 저장
            
            for i in range(n):
                while j > 0 and text[i] != pattern[j]:  # 불일치 시 점프
                    j = pi[j - 1]
                
                if text[i] == pattern[j]:  # 일치하면 j 증가
                    if j == m - 1:  # 패턴이 끝까지 일치하면 결과 저장
                        result.append(i - m + 1)
                        j = pi[j]  # 다음 탐색을 위해 j 갱신
                    else:
                        j += 1
            
            return result

        # 사용 예제
        text = "ABABDABACDABABCABAB"
        pattern = "ABABCABAB"
        matches = kmp_search(text, pattern)
        print("패턴 발견 위치:", matches)
        ```
    - KMP 알고리즘의 활용 사례
	    - 텍스트 검색 (검색 엔진, 문자열 매칭)
	        - 예: 문서에서 특정 단어 검색
	    - 유전자 서열 분석
	        - DNA 서열에서 특정 패턴 검색
	    - 보안 및 악성 코드 탐지
	        - 특정 패턴이 포함된 악성 코드 탐색
	    - 데이터 압축
	        - 중복 문자열 탐색 및 최적화
    - 결론
	    - KMP는 O(N + M)으로 동작하는 효율적인 문자열 검색 알고리즘.
	    - π 배열을 사용하여 중복된 패턴을 활용, 불필요한 비교를 최소화.
	    - 텍스트 검색, 보안, DNA 분석 등 다양한 분야에서 활용 가능.
        - 빠르고 효율적인 문자열 검색이 필요할 때 KMP를 활용하면 좋음

- 보이어-무어(Boyer-Moore) 알고리즘의 특징과 시간 복잡도
    - 개요
        - 보이어-무어(Boyer-Moore) 알고리즘은 문자열 내에서 특정 패턴을 빠르게 찾는 문자열 검색 알고리즘 중 하나
        - 이 알고리즘은 오른쪽에서 왼쪽으로 비교하는 방식과 불일치 시 점프하는 규칙을 활용하여 검색 속도를 크게 향상시킨다.
    - 보이어-무어 알고리즘의 특징
	    - 오른쪽에서 왼쪽으로 비교
	        - 일반적인 문자열 검색 알고리즘(예: KMP)은 왼쪽에서 오른쪽으로 비교하지만, 보이어-무어 알고리즘은 패턴의 끝에서부터 비교한다.
	        - 따라서 불일치가 발생하면 더 멀리 점프할 수 있어 검색 속도가 빠름.
	    - 두 가지 점프(Shift) 규칙을 사용하여 최적화
	        - (1) 배드 캐릭터 규칙(Bad Character Rule)
	            - 불일치가 발생한 문자를 기준으로, 패턴 내에서 해당 문자가 마지막으로 등장한 위치까지 점프.
	            - 패턴 내에 없으면 패턴 길이만큼 점프.
	        - (2) 좋은 접미사 규칙(Good Suffix Rule)
	            - 이미 일치한 접미사를 기준으로, 패턴 내에서 동일한 접미사가 다시 나타나는 위치까지 점프.
	    - 실제 검색 시 평균적으로 O(N/M)의 속도를 가지며, 일반적인 텍스트 검색에서 매우 효율적
	        - KMP보다 빠르게 동작하는 경우가 많으며, 긴 텍스트에서 패턴을 찾을 때 성능이 뛰어남.
    - 보이어-무어 알고리즘의 시간 복잡도
        - 최악의 경우: O(NM)
            - 예를 들어, text = "AAAAAAAAA", pattern = "AAAAAB"일 때, 한 글자씩만 이동하면 O(NM) 발생 가능.
        - 평균적인 경우: O(N/M)
            - 패턴이 긴 경우, 불일치 발생 시 빠르게 점프할 수 있어 일반적으로 O(N/M)의 성능을 보인다.
        - 최적의 경우: O(N)
            - 특정한 경우에는 한 번의 패턴 비교만으로 빠르게 문자열을 찾을 수 있음.
    - 보이어-무어 알고리즘의 활용
	    - 텍스트 검색(검색 엔진, 데이터 검색)
	    - 유전자 서열 분석(DNA 서열 패턴 탐색)
	    - 보안(악성 코드 탐지)
	    - 컴파일러 및 정규식 매칭
    - 보이어-무어 vs KMP 알고리즘 비교
	    - 보이어-무어는 긴 텍스트에서 패턴이 길수록 더 효율적
	    - KMP는 최악의 경우에도 O(N + M) 보장 → 짧은 패턴이나 작은 텍스트에서는 KMP가 더 적합할 수도 있음.
    - 결론
	    - 보이어-무어 알고리즘은 뒤에서 앞으로 비교하며, 점프 규칙(배드 캐릭터 & 좋은 접미사)을 활용하여 빠른 검색이 가능.
	    - 일반적인 경우 O(N/M)로 매우 빠르지만, 최악의 경우 O(NM)이 발생할 수도 있음.
	    - 검색 엔진, 보안, 유전자 분석 등 패턴 검색이 중요한 다양한 분야에서 사용됨.
        - 즉, 긴 텍스트에서 빠르게 검색할 때 보이어-무어를 고려하는 것이 좋음

- 라빈-카프(Rabin-Karp) 알고리즘의 원리와 적용 사례
    - 라빈-카프(Rabin-Karp) 문자열 검색 알고리즘 개요
        - 라빈-카프(Rabin-Karp) 알고리즘은 해시(Hash) 함수를 이용하여 문자열 내에서 특정 패턴을 빠르게 찾는 알고리즘
        - 문자열의 해시 값을 비교하여 검색 속도를 향상시키며, O(N + M) 평균 시간 복잡도를 가짐
    - 라빈-카프 알고리즘의 동작 방식
	    - 해시 함수(Hash Function) 생성
	        - 문자열을 숫자로 변환하는 해시 함수를 정의한다.
	        - 예를 들어, 패턴의 해시 값을 미리 계산해두고, 본문 문자열의 부분 문자열을 같은 방식으로 해시 변환하여 비교한다.
	    - 해시 충돌 처리
	        - 서로 다른 문자열이 같은 해시 값을 가질 수 있으므로 충돌 발생 시, 직접 문자열을 비교하여 확인.
	    - 롤링 해시(Rolling Hash) 기법 사용
	        - 본문에서 M 길이의 부분 문자열을 계속 업데이트하면서 해시 값을 효율적으로 갱신한다.
	        - 새 문자가 추가될 때 기존 해시 값을 활용하여 빠르게 계산.
    - 라빈-카프 알고리즘 구현 (Python)
        ```python
        # 라빈-카프 알고리즘 구현
        def rabin_karp(text, pattern, prime=101):
            n, m = len(text), len(pattern)
            base = 256  # 알파벳 개수 (ASCII 문자 기준)
            pattern_hash = 0
            text_hash = 0
            h = 1
            result = []

            # h = base^(m-1) % prime (해시 값을 계산할 때 사용)
            for i in range(m - 1):
                h = (h * base) % prime

            # 패턴과 첫 번째 부분 문자열의 해시 값 계산
            for i in range(m):
                pattern_hash = (base * pattern_hash + ord(pattern[i])) % prime
                text_hash = (base * text_hash + ord(text[i])) % prime

            # 슬라이딩 윈도우 방식으로 문자열 비교
            for i in range(n - m + 1):
                # 해시 값이 일치하면 실제 문자열 비교
                if pattern_hash == text_hash:
                    if text[i:i + m] == pattern:
                        result.append(i)

                # 다음 윈도우의 해시 값 계산 (롤링 해시 기법 사용)
                if i < n - m:
                    text_hash = (base * (text_hash - ord(text[i]) * h) + ord(text[i + m])) % prime
                    if text_hash < 0:  # 음수 방지
                        text_hash += prime

            return result

        # 사용 예제
        text = "ABABDABACDABABCABAB"
        pattern = "ABABCABAB"
        matches = rabin_karp(text, pattern)
        print("패턴 발견 위치:", matches)
        ```
    - 라빈-카프 알고리즘의 시간 복잡도
        - 최악의 경우: O(NM) (해시 충돌이 많을 경우)
            - 모든 해시 값이 충돌하면, 문자열을 직접 비교해야 하므로 O(NM)
        - 평균적인 경우: O(N + M)
            - 해시 값이 고유하면 O(N + M)으로 동작하여 매우 빠름
    - 라빈-카프 알고리즘의 적용 사례
	    - 텍스트 검색
	        - 문서에서 특정 단어를 검색할 때 활용.
	    - 표절 탐지
	        - 문서 비교 시, 부분 문자열이 유사한지 확인하는데 사용.
	    - DNA 서열 분석
	        - 유전자 서열에서 특정 패턴이 존재하는지 찾는 데 활용.
	    - 보안 (악성 코드 탐지)
	        - 특정 악성 코드 패턴이 포함된 파일을 탐색.
    - 라빈-카프 vs KMP vs 보이어-무어 비교
	    - 라빈-카프: 해시를 활용하여 텍스트 검색이 빠르지만, 해시 충돌이 발생할 경우 느려질 수 있음.
	    - KMP: 패턴 내부의 중복을 활용하여 검색.
	    - 보이어-무어: 오른쪽에서 왼쪽으로 탐색하며 점프를 사용하여 효율적인 검색.
    - 결론
	    - 라빈-카프는 해시를 이용하여 문자열을 빠르게 검색하는 알고리즘.
	    - O(N + M) 평균 속도로 동작하며, 긴 텍스트에서 효율적.
	    - 해시 충돌이 많으면 최악의 경우 O(NM)까지 느려질 수 있음.
	    - 텍스트 검색, 표절 탐지, DNA 서열 분석 등 다양한 분야에서 활용 가능.
        - 즉, 빠른 검색이 필요한 경우 라빈-카프를 활용하면 좋지만, 충돌이 적도록 적절한 해시 함수 선택이 중요

- 접미사 배열(Suffix Array)의 개념과 활용 방안
    - 접미사 배열(Suffix Array)
        - 문자열의 모든 접미사(Suffix)를 사전순(lexicographical order)으로 정렬한 후, 해당 접미사가 시작하는 인덱스를 저장하는 배열
        - 즉, 문자열 내에서 부분 문자열을 빠르게 검색할 수 있도록 돕는 자료구조로, 문자열 검색, 유전자 분석, 데이터 압축 등에 활용
    - 접미사 배열의 예제
        - 문자열 “banana”의 접미사 배열
        - 모든 접미사를 추출하고 사전순으로 정렬
            - 접미사, 인덱스
            - a	        5
            - ana	    3
            - anana	    1
            - banana    0
            - na	    4
            - nana	    2
        - 접미사 배열 (Suffix Array): [5, 3, 1, 0, 4, 2]
    - 접미사 배열의 생성 방법
        - O(N log² N) - 단순 정렬 방식
	        - 문자열의 모든 접미사 생성
	        - 사전순으로 정렬
	        - 해당 접미사의 시작 인덱스를 저장
        - O(N log N) - 맨버-마이어스(Manber-Myers) 알고리즘
	        - 두 배씩 증가하는 길이 단위로 비교하여 빠르게 정렬
	        - O(N log N)에 접미사 배열을 생성할 수 있음
        - O(N) - SA-IS 알고리즘
	        - 가장 효율적인 알고리즘으로, O(N)에 접미사 배열을 구성할 수 있음
	        - 특히 긴 문자열(수백만 개 이상)에서 유용함
    - 접미사 배열의 활용
        - 문자열 검색 (Substring Search)
            - 특정 패턴이 문자열 내에 존재하는지 빠르게 찾을 수 있음.
            - 이진 탐색을 활용하여 O(M log N)으로 검색 가능 (M: 패턴 길이).
        - 사전순 정렬 (Lexicographic Sorting)
            - 문자열의 모든 부분 문자열을 사전순으로 정렬하는 문제 해결에 사용.
        - 가장 긴 공통 접두사(LCP, Longest Common Prefix)
            - 접미사 배열과 함께 LCP 배열을 사용하면, 문자열 내에서 가장 긴 반복되는 부분 문자열을 찾을 수 있음.
        - 데이터 압축
            - Burrows-Wheeler Transform (BWT)에서 중요한 역할을 함
            - BWT는 ZIP, bzip2 등의 압축 알고리즘에서 사용됨
        - 유전자 서열 분석
            - DNA 서열 분석에서 유사한 서열을 찾는 데 활용됨.
    - 접미사 트리, 접미사 배열
        - 접미사 트리는 더 빠르지만, 공간 복잡도가 크기 때문에 접미사 배열이 더 실용적인 경우가 많음
    - 결론
	    - 접미사 배열(Suffix Array)은 문자열 내 모든 접미사를 정렬한 후, 해당 접미사의 시작 위치를 저장하는 배열.
	    - 빠른 문자열 검색, LCP 계산, 데이터 압축, 유전자 분석 등에 활용.
	    - 생성 방법에는 단순 정렬(O(N log² N)), 맨버-마이어스(O(N log N)), SA-IS(O(N)) 등이 있음.
	    - 접미사 트리보다 메모리 사용량이 적고, 문자열 처리에 실용적으로 사용됨.
        - 즉, 대량의 텍스트 검색 및 유전자 분석, 압축 알고리즘에서 필수적인 자료구조

- 애너그램(Anagram) 검사 알고리즘을 구현하는 방법
    - 애너그램(Anagram) 검사 알고리즘 개요
        - 두 개의 문자열이 같은 문자들로 구성되었지만, 순서만 다를 경우를 의미
        - 즉, 문자열을 정렬하거나 문자 개수를 비교하면 애너그램인지 확인할 수 있음
    - 애너그램 검사 방법
        - 방법 1: 정렬(Sorting) 활용 (O(N log N))
            - 개요
	            - 두 문자열을 알파벳 순으로 정렬한 후 비교.
	            - 정렬한 결과가 같다면 애너그램.
            - 구현 (Python)
                ```python
                def is_anagram_sort(s1, s2):
                    return sorted(s1) == sorted(s2)
                print(is_anagram_sort("listen", "silent"))  # True
                print(is_anagram_sort("hello", "world"))    # False
                ```
            - 장단점
                - 장점: 간단하고 직관적
                - 단점: 정렬로 인해 O(N log N)의 시간 복잡도를 가짐
        - 방법 2: 해시맵(딕셔너리) 활용 (O(N))
            - 개요
                - 각 문자의 개수를 카운트하여 비교.
                - Python의 collections.Counter 사용 가능.
            - 구현 (Python)
                ```python
                from collections import Counter

                def is_anagram_counter(s1, s2):
                    return Counter(s1) == Counter(s2)
                ```
            - 장단점
                - 장점: 빠름 (O(N))
                - 단점: 공간을 추가로 사용해야 함
        - 방법 3: 배열 활용 (O(N))
	        - 개요
                - 알파벳 개수를 저장하는 리스트(길이 26)를 사용하여 각 문자의 등장 횟수를 기록.
                - 두 문자열의 개수를 비교.
            - 구현 (Python)
                ```python
                def is_anagram_array(s1, s2):
                    if len(s1) != len(s2):
                        return False
                    
                    count = [0] * 26  # 영어 소문자 기준
                    
                    for c1, c2 in zip(s1, s2):
                        count[ord(c1) - ord('a')] += 1
                        count[ord(c2) - ord('a')] -= 1
                    
                    return all(x == 0 for x in count)
                ```
            - 장단점
                - 장점: O(N)으로 매우 빠름, 추가 공간 사용이 적음
                - 단점: 영어 소문자(a~z)만 처리 가능 (유니코드 확장 시 비효율적)
    - 애너그램 검사 알고리즘 구현 방법에 대한 결론
        - 정렬 사용: 시간-O(N log N), 공간-O(1), 직관적이지만 느림
        - 딕셔너리 사용: 시간-O(N), 공간-O(N), 빠르지만 추가 공간 필요
        - 배열 사용: 시간-O(N), 공간-O(1), 가장 빠름, 영어 소문자에 최적화
        - 문자열이 길다면 Counter나 배열 활용 방법이 가장 효율적
        - 문자열에 특수 문자, 대소문자가 포함될 경우 Counter 방법이 유리

- A* (A-Star) 알고리즘의 개념과 활용 사례
    - A* 알고리즘 개념
        - A 알고리즘(A-Star Algorithm)은 최단 경로 탐색 알고리즘 중 하나
        - 다익스트라(Dijkstra) 알고리즘과 휴리스틱(Heuristic) 탐색을 결합하여 더 빠르고 효율적인 경로 탐색을 수행하는 알고리즘
    - A* 알고리즘은 두 가지 비용을 조합하여 최적의 경로를 탐색
	    - g(n): 시작 지점에서 현재 노드까지의 실제 비용 (경로 비용)
	    - h(n): 현재 노드에서 목표 노드까지의 예상 비용 (휴리스틱 함수)
	    - 총 비용 함수:
            - f(n) = g(n) + h(n)
                - g(n): 지금까지의 이동 거리
	            - h(n): 앞으로 예상되는 이동 거리
        - A* 알고리즘은 f(n)가 가장 작은 노드를 우선적으로 탐색하며, h(n) 값이 정확할수록 탐색 속도가 빨라짐
    - A* 알고리즘의 동작 방식
	    - 시작 노드를 우선순위 큐(Priority Queue)에 넣고 초기 비용을 설정한다.
	    - 우선순위 큐에서 f(n)이 가장 작은 노드를 선택하여 확장한다.
	    - 목표 지점에 도착하면 종료하고 최단 경로를 반환한다.
	    - 각 이동 경로에서 g(n)과 h(n)을 업데이트하여 최적의 노드를 선택한다.
    - A* 알고리즘의 시간 복잡도
	    - 최악의 경우 O(E log V) (다익스트라와 동일)
	    - 휴리스틱이 완벽할 경우 O(V)로 개선 가능
    - A* 알고리즘의 활용 사례
	    - 게임 AI (길찾기 알고리즘)
	        - 게임 캐릭터가 장애물을 피해서 목적지까지 최적 경로를 탐색할 때 사용
	        - 예: A* Pathfinding (Unity, 게임 엔진에서 널리 사용)
	    - 지도 및 네비게이션 시스템
	        - 구글 맵, 네비게이션 시스템에서 최단 경로 탐색에 활용
	        - h(n)으로 직선 거리(Euclidean Distance) 또는 교통량 데이터를 사용할 수 있음.
	    - 로봇 경로 계획
	        - 로봇이 장애물을 회피하며 목적지까지 최적 경로를 찾는 데 사용
	    - AI 및 머신러닝
	        - 상태 공간 탐색(State Space Search)에서 최적의 해결 경로를 찾는 데 활용

- 네트워크 플로우(Network Flow) 알고리즘의 개념과 적용 사례
    - 네트워크 플로우(Network Flow) 개념
        - 유량(Flow)이 있는 네트워크에서 최대 흐름(Maximum Flow) 또는 최적의 흐름을 찾는 알고리즘
        - 네트워크 플로우 문제는 노드(Node)와 간선(Edge)으로 이루어진 유량 네트워크에서 소스(Source)에서 싱크(Sink)로 보낼 수 있는 최대 유량을 찾는 문제를 해결하는 데 사용된다
    - 주요 네트워크 플로우 알고리즘
	    - 포드-풀커슨(Ford-Fulkerson) 알고리즘
	        - 증가 경로(Augmenting Path)를 찾으며 최대 유량을 증가시키는 방식
	        - DFS 기반 → O(E * max flow) 시간 복잡도
	    - 에드몬드-카프(Edmonds-Karp) 알고리즘
	        - 포드-풀커슨의 BFS 기반 구현 → O(VE²) 시간 복잡도
	    - 다이나믹 트리(Dynamic Tree) 기반 최대 유량 알고리즘
	        - O(VE log V)까지 최적화 가능
    - 네트워크 플로우 알고리즘의 적용 사례
	    - 인터넷 데이터 전송 및 네트워크 라우팅
	        - 데이터 패킷을 최적의 경로로 보내기 위한 네트워크 최적화
	    - 교통 흐름 최적화
	        - 도로 네트워크에서 특정 시간대의 교통량을 분배하여 혼잡을 줄이는 데 활용
	    - 공급망 최적화(Supply Chain Optimization)
	        - 물류 네트워크에서 최소 비용으로 제품을 분배하는 문제 해결
	    - 이분 매칭(Bipartite Matching)
	        - 학생과 프로젝트 매칭, 강의실 배정, 작업 스케줄링 등에서 활용
	        - 헝가리안 알고리즘(Hungarian Algorithm)과 함께 사용
	    - 전력망 최적화
	        - 발전소에서 각 도시로 최대한 효율적으로 전력을 분배하는 문제 해결
    - 네트워크 플로우 vs 다익스트라 vs A*
        - A*
            - 문제 유형: 최단 경로
            - 적용 사례: 길찾기, 게임 AI, 로봇 경로 탐색
        - 다익스트라
            - 문제 유형: 최단 경로
            - 적용 사례: 교통 경로 최적화, 통신망 경로 탐색
        - 네트워크 플로우
            - 문제 유형: 최대 유량
            - 적용 사례: 인터넷 라우팅, 교통 흐름 분석, 물류 최적화
    - 결론
        - A* 알고리즘
	        - 최적 경로 탐색 알고리즘으로 휴리스틱(h(n))을 활용하여 빠르게 최단 경로를 찾음.
	        - 게임 AI, 로봇 경로 탐색, 지도 네비게이션에 활용됨.
        - 네트워크 플로우 알고리즘
	        - 최대 유량(Max Flow)을 찾는 알고리즘으로 인터넷 라우팅, 교통 최적화, 물류 네트워크, 이분 매칭 등에 활용됨.
	        - 포드-풀커슨, 에드몬드-카프 등의 알고리즘이 대표적.
        - A* 알고리즘은 최단 경로 탐색, 네트워크 플로우 알고리즘은 최대 유량을 찾는 데 사용*
        - 현실 세계에서 최적화 문제를 해결할 때, 문제 유형에 따라 적절한 알고리즘을 선택해야 함.

- 트리(Tree) 자료구조에서 DFS와 BFS의 차이점
    - 개요
        - 트리(Tree)에서 DFS(Depth-First Search, 깊이 우선 탐색)과 BFS(Breadth-First Search, 너비 우선 탐색)은 노드를 탐색하는 두 가지 대표적인 방법
        - 두 알고리즘은 트리와 그래프에서 사용됨
        - 탐색 방식과 시간/공간 복잡도 측면에서 차이 존재
    
    - DFS(깊이 우선 탐색, Depth-First Search)
        - 개념
	        - DFS는 트리의 루트(root)에서 시작하여 한 방향으로 끝까지 탐색한 후, 다시 돌아와 다른 경로를 탐색하는 방식.
	        - 스택(Stack) 또는 재귀(Recursion)를 사용하여 구현 가능.

        - 동작 방식
	        - 현재 노드를 방문하고 출력(처리).
	        - 자식 노드가 있으면 먼저 깊이(depth) 방향으로 이동.
	        - 더 이상 방문할 노드가 없으면 백트래킹(Backtracking, 되돌아가기) 수행.
	        - 모든 노드를 방문할 때까지 반복.

        - DFS의 구현 방법
            - (1) 스택을 활용한 DFS (반복문)
                ```python
                def dfs_stack(root):
                stack = [root]  # 스택 사용
                while stack:
                    node = stack.pop()
                    print(node.value, end=" ")  # 방문 처리
                    # 자식 노드를 오른쪽부터 넣어 왼쪽부터 탐색
                    stack.extend(reversed(node.children))
                ```
            - (2) 재귀(Recursion) 방식
                ```python
                def dfs_recursive(node):
                    if node is None:
                        return
                    print(node.value, end=" ")  # 방문 처리
                    for child in node.children:
                        dfs_recursive(child)
                ```

            - DFS의 시간 및 공간 복잡도
                - 시간 복잡도: O(N) (모든 노드를 방문)
                - 공간 복잡도: O(H) (H = 트리의 높이)
                - 최악의 경우: 트리가 한쪽으로 편향되면 O(N) (스택 깊이가 최대 N)
	            - 균형 잡힌 트리: O(log N) (완전 이진 트리의 높이는 log N)

    - BFS(너비 우선 탐색, Breadth-First Search)
        - 개념
	        - BFS는 루트에서 가까운 노드부터 탐색하는 방식
	        - 큐(Queue, FIFO)를 사용하여 구현

        - 동작 방식
	        - 루트에서 시작하여 큐에 삽입
	        - 큐에서 노드를 하나씩 꺼내면서 방문
	        - 현재 노드의 모든 자식 노드를 큐에 추가
	        - 큐가 빌 때까지 반복

        - BFS의 구현 (Python)
            ```python
            from collections import deque

            def bfs(root):
                queue = deque([root])  # 큐 사용
                while queue:
                    node = queue.popleft()
                    print(node.value, end=" ")  # 방문 처리
                    queue.extend(node.children)  # 모든 자식 노드를 큐에 추가
            ```

        - BFS의 시간 및 공간 복잡도
            - 시간 복잡도: O(N) (모든 노드를 방문)
            - 공간 복잡도: O(W) (W = 최대 너비)
                - 트리의 폭(W)이 클 경우, 공간 복잡도가 커질 수 있음.
	        - 완전 이진 트리에서 W = 2^(logN) ≈ N이므로, 공간 복잡도는 O(N)

    - DFS, BFS 정리
        - DFS(깊이 우선 탐색)
	        - 재귀 또는 스택을 사용하며, 한 경로를 끝까지 탐색 후 돌아옴 (백트래킹)
	        - 공간 복잡도가 낮음 (O(H)) → 트리의 높이가 낮은 경우 적합
	        - 최적 경로 보장 X (DFS는 목표 노드를 먼저 찾는 것이 아니라 깊이 우선)
	        - 사용 예제: 백트래킹, 퍼즐, 순열/조합 생성, 그래프 탐색

        - BFS(너비 우선 탐색)
	        - 큐(Queue)를 사용하며, 한 레벨씩 탐색하면서 가장 가까운 노드부터 확장
	        - 최단 경로를 보장 (최소 이동 횟수로 도착할 수 있음)
	        - 공간 복잡도가 높음 (O(W)) → 트리의 너비가 클 경우 메모리 부담
	        - 사용 예제: 최단 거리 문제, 네트워크 검색, 웹 크롤링

    - 결론
        - 최단 경로가 필요하면 BFS
        - 모든 경로를 탐색하거나 백트래킹이 필요하면 DFS
        - 공간 절약이 필요하면 DFS, 너비가 좁으면 BFS
        - 최단 경로 문제라면 BFS를, 경로 탐색이나 조합 생성이라면 DFS를 사용하는 것이 일반적

- 스택(Stack)과 큐(Queue)
    - 스택(Stack)과 큐(Queue)의 개념
	    - 스택(Stack): LIFO(Last In, First Out, 후입선출) 구조
            - 나중에 들어온 데이터가 먼저 나감 (ex: 접시 쌓기)
	    - 큐(Queue): FIFO(First In, First Out, 선입선출) 구조
            - 먼저 들어온 데이터가 먼저 나감 (ex: 줄 서기)

    - 스택(Stack)
        - 특징
	        - 후입선출(LIFO): 마지막에 추가된 데이터가 가장 먼저 제거됨.
	        - 한쪽 끝에서만 데이터 삽입과 삭제가 이루어짐.
        - 연산
	        - push(x) → 요소 x를 스택에 추가
	        - pop() → 스택에서 요소 제거 후 반환
	        - peek() → 최상단 요소 확인(삭제 X)
	        - isEmpty() → 스택이 비었는지 확인

    - 큐(Queue)
        - 특징
	        - 선입선출(FIFO): 먼저 들어온 데이터가 먼저 나감
	        - 한쪽 끝에서 삽입(enqueue), 반대쪽 끝에서 삭제(dequeue)
        - 연산
	        - enqueue(x) → 요소 x를 큐에 추가
	        - dequeue() → 큐에서 요소 제거 후 반환
	        - front() → 큐의 첫 번째 요소 확인(삭제 X)
	        - isEmpty() → 큐가 비었는지 확인

    - 스택과 큐의 활용 사례
        - 스택(Stack) 활용
	        - 함수 호출 (재귀 호출)
	        - 웹 브라우저 뒤로 가기 (뒤로/앞으로 이동)
	        - 문자열 괄호 검사 (ex: ((())) 검증)
        - 큐(Queue) 활용
	        - CPU 프로세스 스케줄링
	        - 네트워크 패킷 처리 (데이터 스트리밍)
	        - 프린터 작업 대기열 (선착순 출력)
    - 결론
        - 마지막 데이터부터 처리해야 하면 스택 (LIFO)
        - 먼저 들어온 데이터를 먼저 처리해야 하면 큐 (FIFO)
        - 실제 구현에서는 deque를 사용하면 큐/스택 모두 효율적
        - 순서에 따라 데이터 처리 방식이 다르므로, 문제 유형에 맞게 스택과 큐를 선택 필요

- 덱/데크(Deque, Double-ended Queue) 개념과 활용 사례
    - 덱/데크(Deque)의 개념 (스택 + 큐 기능 둘다 가짐)
	    - 양쪽(앞과 뒤)에서 삽입과 삭제가 모두 가능한 큐
	    - 일반적인 큐(Queue)는 FIFO 구조이지만, 덱은 양방향 삽입/삭제 가능
	    - 스택(Stack)과 큐(Queue)의 기능을 모두 포함하는 자료구조
    - 덱의 연산
        - push_front(x): 앞쪽에 원소 추가
        - push_back(x): 뒤쪽에 원소 추가
        - pop_front(): 앞쪽 원소 삭제 및 반환
        - pop_back(): 뒤쪽 원소 삭제 및 반환
        - front(): 앞쪽 원소 확인 (삭제 X)
        - back(): 뒤쪽 원소 확인 (삭제 X)
        - isEmpty(): 덱이 비었는지 확인
    - 덱 구현
        ```python
        from collections import deque

        dq = deque()
        dq.append(1)  # push_back(1)
        dq.appendleft(2)  # push_front(2)
        print(dq.pop())  # pop_back() → 1
        print(dq.popleft())  # pop_front() → 2
        ```
    - 덱의 활용 사례
        - 양방향 탐색이 필요한 경우
	        - 웹 브라우저 앞으로/뒤로 가기 (앞으로 가기, 뒤로 가기)
	        - 슬라이딩 윈도우 최적화 (윈도우 내 최소/최대값 구하기)
	        -캐시(Cache) 구현 (LRU 캐시, 가장 오래된 데이터 삭제)

- 우선순위 큐(Priority Queue)의 개념과 구현 방식(힙 구조 포함)
    - 우선순위 큐 개념
	    - 일반적인 큐(Queue)는 FIFO 방식으로 동작하지만, 우선순위 큐(Priority Queue)는 높은 우선순위를 가진 원소가 먼저 나오는 큐
	    - 우선순위를 기준으로 자동 정렬됨 (최소/최대값을 빠르게 찾을 수 있음)

    - 우선순위 큐 구현 방식
	    - 리스트(List) 기반
	        - 삽입: O(1) (맨 뒤에 추가)
	        - 삭제: O(N) (최대/최소값을 찾기 위해 정렬 필요)
	    - 정렬된 리스트(Sorted List) 기반
	        - 삽입: O(N) (올바른 위치에 삽입)
	        - 삭제: O(1) (가장 앞의 원소 제거)
        - 힙(Heap) 기반 (가장 효율적)
	        - 삽입: O(log N)
	        - 삭제: O(log N)

    - 힙(Heap) 구조
	    - 완전 이진 트리(Complete Binary Tree) 기반의 자료구조.
	    - 최소 힙(Min Heap): 루트가 가장 작은 값
	    - 최대 힙(Max Heap): 루트가 가장 큰 값

    - 우선순위 큐 구현 (heapq 사용)
        ```python
        import heapq

        pq = []
        heapq.heappush(pq, 3)  # 삽입 (O(log N))
        heapq.heappush(pq, 1)  
        heapq.heappush(pq, 2)
        print(heapq.heappop(pq))  # 가장 작은 값(1) 출력 후 삭제
        ```

    - 최대 힙 구현 (Python에서는 기본적으로 최소 힙이므로 음수 값을 활용)
        ```python
        heapq.heappush(pq, -3)
        heapq.heappush(pq, -1)
        heapq.heappush(pq, -2)
        print(-heapq.heappop(pq))  # 가장 큰 값(3) 출력 후 삭제
        ```

    - 우선순위 큐 활용 사례
        - 네트워크 패킷 스케줄링 (우선순위가 높은 데이터 먼저 처리)
        - 다익스트라(Dijkstra) 알고리즘 (최단 경로 탐색)
        - 작업 스케줄링 (CPU 프로세스 우선순위 처리)

- 트리(Tree)와 그래프(Graph)
    - 트리(Tree) 개념
	    - 트리는 사이클이 없는 그래프의 특별한 형태
	    - N개의 정점이 있으면 항상 (N-1)개의 간선이 존재
	    - 루트(Root) 노드가 있으며, 부모-자식 관계가 존재
	    - 이진 트리(Binary Tree), AVL 트리, 이진 탐색 트리(BST) 등 다양한 유형 존재

    - 그래프(Graph) 개념
	    - 노드(Node)와 간선(Edge)으로 이루어진 자료구조
	    - 트리와 달리 사이클이 존재할 수도 있음
	    - 방향 그래프(Directed Graph)와 무방향 그래프(Undirected Graph) 존재
	    - DFS, BFS, 다익스트라, 크루스칼 알고리즘 등으로 탐색

    - 트리, 그래프 정리
        - 트리는 계층 구조(부모-자식 관계), 그래프는 자유로운 노드 간 연결
        - 트리는 DFS/BFS 탐색, 그래프는 다익스트라/크루스칼 등의 알고리즘 활용

    - 결론
        - 트리(Tree) vs 그래프(Graph) → 트리는 계층적이며 사이클 없음, 그래프는 복잡한 노드 연결 구조
        - (참고) 덱(Deque) → 양방향 삽입/삭제 가능, 웹 브라우저 뒤로/앞으로 가기, 슬라이딩 윈도우 최적화에 활용
        - (참고) 우선순위 큐(Priority Queue) → 힙(Heap) 기반이 가장 효율적, 다익스트라, 작업 스케줄링에 활용


- 이진 트리(Binary Tree)와 이진 탐색 트리(Binary Search Tree, BST)의 차이점
    - 이진 트리(Binary Tree) 개념
        - 각 노드가 최대 두 개의 자식 노드(왼쪽, 오른쪽)를 가질 수 있는 트리 구조

    - 이진 트리의 특징
	    - 각 노드의 자식 노드 수가 최대 2개.
	    - 이진 트리는 데이터의 크기와 관계없이 구성될 수 있음.
	    - 탐색, 삽입, 삭제에 대한 정렬된 규칙이 없음.

    - 이진 트리의 종류
	    - 포화 이진 트리(Full Binary Tree)
	        - 모든 노드가 0개 또는 2개의 자식 노드를 가짐.
	        - 모든 리프 노드가 같은 깊이에 존재.
	    - 완전 이진 트리(Complete Binary Tree)
	        - 왼쪽부터 순서대로 노드가 채워진 트리.
	        - 마지막 레벨을 제외한 모든 레벨이 가득 차 있음.
	    - 편향 이진 트리(Skewed Binary Tree)
	        - 한쪽 방향(왼쪽 또는 오른쪽)으로만 노드가 계속 연결됨.

    - 이진 탐색 트리(Binary Search Tree, BST) 개념
        - 이진 탐색 트리(Binary Search Tree, BST)는 이진 트리의 한 종류로, 정렬된 규칙을 갖는 트리 구조

    - BST의 특징
	    - 왼쪽 서브트리(left subtree)의 값 < 부모 노드(parent) 값
	    - 오른쪽 서브트리(right subtree)의 값 > 부모 노드(parent) 값
	    - 중복된 값이 없는 경우가 일반적 (일부 변형 BST에서는 허용)

    - BST의 장점
	    - 정렬된 데이터를 유지하면서 탐색, 삽입, 삭제 연산을 빠르게 수행 가능.
	    - 평균적으로 탐색, 삽입, 삭제 연산의 시간 복잡도: O(log N) (균형 잡힌 BST일 경우).

    - 결론
	    - 이진 트리: 단순한 트리 구조, 정렬되지 않음.
	    - 이진 탐색 트리(BST): 이진 트리의 한 종류로, 정렬된 구조를 유지하며 탐색을 효율적으로 수행
	    - BST는 빠른 탐색, 삽입, 삭제(O(log N))가 가능하여 데이터 검색, 정렬, 데이터베이스 인덱싱 등에 활용됨
        - 데이터 검색 및 정렬이 중요한 경우에는 BST를 사용하는 것이 효율적

- AVL 트리와 레드-블랙 트리(Red-Black Tree)의 차이점
    - AVL 트리와 레드-블랙 트리(Red-Black Tree) 개요
        - AVL 트리(Adelson-Velsky and Landis Tree)와 레드-블랙 트리(Red-Black Tree)는 균형 이진 탐색 트리(Self-balancing Binary Search Tree, BST)
        - 트리의 높이를 제한하여 최악의 경우에도 O(log N) 시간 복잡도를 유지하도록 설계된 자료구조
        - 둘 다 균형을 유지하나 유지 방식과 연산 속도에 차이 존재

    - AVL 트리(Adelson-Velsky and Landis Tree): 탐색 성능 중요할 시 선택 (읽기 연산)
        - 특징
	        - 높이 균형 트리(Height Balanced Tree)
            - 각 노드의 왼쪽 서브트리와 오른쪽 서브트리의 높이 차이가 1 이하가 되도록 유지
	        - 균형 인수(Balance Factor)
	            - 각 노드의 균형 인수(Balance Factor) = 왼쪽 서브트리 높이 - 오른쪽 서브트리 높이
	            - 균형 인수 값이 -1, 0, 1을 유지하도록 삽입/삭제 연산 후 회전(Rotation) 을 수행
	        - 회전 연산(Rotation)
	            - 불균형이 발생하면 단일 회전(Single Rotation, LL, RR) 또는 이중 회전(Double Rotation, LR, RL) 을 수행하여 균형을 맞춤

        - 시간 복잡도
	        - 탐색(Search): O(log N)
	        - 삽입(Insertion): O(log N) (불균형 발생 시 추가적인 회전 연산 필요)
	        - 삭제(Deletion): O(log N) (균형 유지 과정에서 추가 회전 필요)

        - 장점
	        - 트리의 높이가 더 낮아져 탐색 속도가 빠름
	        - 균형 상태가 엄격하게 유지되므로 탐색 성능이 항상 O(log N)로 일정

        - 단점
	        - 삽입/삭제 시 균형을 유지하기 위해 추가적인 회전 연산이 필요하여 오버헤드가 큼
	        - 레드-블랙 트리보다 구현이 복잡

    - 레드-블랙 트리(Red-Black Tree): 연산 성능 중요할 경우 선택 (삽입/삭제 연산)
        - 특징
	        - 느슨한 균형 트리(Loosely Balanced Tree)
                - AVL 트리처럼 완벽한 균형을 유지하지 않고, 적절한 범위 내에서 균형을 맞춰 연산 속도를 향상
	        - 색상 속성(Color Property)
                - 각 노드가 레드(Red) 또는 블랙(Black) 으로 색칠되며, 트리가 균형을 유지하도록 다음 규칙을 만족
                - 레드-블랙 트리의 규칙
	                - 모든 노드는 Red 또는 Black
	                - 루트(Root) 노드는 항상 Black
	                - Red 노드의 자식 노드는 모두 Black (Red 노드가 연속될 수 없음)
	                - 어떤 노드에서든 루트에서 리프까지 가는 모든 경로에서 Black 노드의 개수는 동일
	                - 삽입/삭제 후 위 규칙이 깨지면 색 변경(Color Flip) 또는 회전(Rotation)으로 수정

        - 시간 복잡도
            - 탐색(Search): O(log N)
	        - 삽입(Insertion): O(log N) (균형 유지 시 색상 변경 또는 회전 필요)
	        - 삭제(Deletion): O(log N) (균형 유지 시 색상 변경 또는 회전 필요)

        - 장점
	        - AVL 트리보다 회전 연산의 빈도가 적음, 즉 삽입 및 삭제가 상대적으로 빠름.
	        - 균형을 엄격하게 유지하지 않아 성능 최적화에 유리함.
	        - 운영체제의 레드-블랙 트리 기반 자료구조(예: Linux 커널의 RBTree) 등에 널리 사용됨.

        - 단점
	        - AVL 트리보다 탐색 시간이 다소 길어질 수 있음. (트리의 높이가 더 크기 때문)
	        - 트리의 균형이 엄격하지 않아 일부 경우 탐색 속도가 최적이 아닐 수 있음.

    - 어떤 경우에 어떤 트리를 선택해야 하는가?
	    - 탐색 성능이 중요한 경우 → AVL 트리
	        - 검색 속도가 일정해야 하는 경우(예: 데이터베이스 인덱싱, 캐시 시스템 등)
	        - 읽기 연산이 자주 발생하고, 삽입/삭제 연산이 적은 경우
	        - 예: 데이터베이스 인덱스, 캐싱 시스템
	    - 삽입/삭제 연산이 자주 발생하는 경우 → 레드-블랙 트리
	        - 삽입과 삭제가 빈번한 경우(예: 동적 집합, OS 스케줄러)
	        - 회전 연산이 적어 삽입/삭제가 빠른 트리가 필요한 경우
	        - 예: 운영체제(OS) 프로세스 스케줄링, Linux 커널, STL의 map/set 구현

    - 결론
	    - AVL 트리는 더 엄격한 균형 유지로 인해 탐색이 빠르지만, 삽입/삭제 시 회전 연산이 많아지는 단점
	    - 레드-블랙 트리는 균형을 다소 완화하면서 삽입/삭제 성능을 높인 트리로, 실제 응용에서 더 널리 사용
	    - 읽기 연산(Read-heavy)이 많은 경우 AVL 트리, 삽입/삭제(Write-heavy)가 많은 경우 레드-블랙 트리를 선택하는 것이 일반적인 기준
        - 탐색 최적화가 중요하다면 AVL 트리, 삽입/삭제 최적화가 중요하다면 레드-블랙 트리

- B-트리(B-Tree)와 B+트리(B+ Tree)의 구조와 활용 사례
    - 개요
        - B-트리(B-Tree)와 B+트리(B+ Tree)는 균형 M-원 검색 트리(Balanced M-Way Search Tree)
        - 주로 데이터베이스 및 파일 시스템에서 대량의 데이터를 효과적으로 관리하는 데 사용
        - 두 트리는 모두 균형 트리이며, 디스크 I/O 성능 최적화를 위해 설계
        - 데이터 저장 방식, 탐색 및 범위 검색 성능, 활용 사례에서 차이

    - B-트리(B-Tree)
        - 구조 (균형 M-원 검색 트리(Balanced M-Way Search Tree) 의 일종)
	        - 각 노드는 최대 M개의 자식과 최대 (M-1)개의 키를 가짐 (M은 트리의 차수)
	        - 키(Key)들은 정렬된 상태로 유지되며, 중위 순회(In-order Traversal) 시 오름차순 출력
	        - 내부 노드(Internal Node)와 리프 노드(Leaf Node) 모두 데이터(Key & Value)를 저장
	        - 모든 리프 노드는 같은 높이(Level)에 위치 → 균형 트리
	        - 삽입(Insert)과 삭제(Delete) 시 자동으로 균형을 유지 (Split & Merge)
	        - 높이가 낮고, 디스크 I/O를 최소화하여 검색 성능을 최적화

        - 탐색 과정
	        - 루트부터 리프까지 이진 탐색(Binary Search) 을 수행하여 키를 찾음.
	        - 트리의 높이(log_M N)가 낮으므로 탐색 속도가 빠름.

        - 삽입 & 삭제
	        - 노드가 가득 차면 분할(Split).
	        - 노드에 키가 부족하면 병합(Merge).

        - 활용 사례
	        - 데이터베이스 인덱스(DB Indexing) (MySQL, PostgreSQL, Oracle 등)
	        - 파일 시스템(File System) (NTFS, HFS+)
	        - 디스크 기반 검색(Secondary Storage Indexing)
	        - 운영체제의 페이징 및 메모리 관리

    - B+트리(B+ Tree)
        - 구조 (B-트리에서 파생된 트리)
	        - 내부 노드(Internal Node)와 리프 노드(Leaf Node)가 분리됨
	        - 내부 노드는 키(Key)만 저장 (데이터는 저장되지 않음)
	        - 실제 데이터(Record)는 리프 노드(Leaf Node)에서만 저장됨
	        - 리프 노드는 Linked List로 연결되어 있어 범위 검색이 용이
	        - 트리의 높이가 B-트리보다 낮아져 탐색 성능이 향상됨

        - 탐색 과정
	        - 내부 노드에서는 이진 탐색(Binary Search) 후 리프 노드로 이동
	        - 리프 노드에서 최종 데이터를 조회

        - 삽입 & 삭제
	        - B-트리와 마찬가지로 분할(Split)과 병합(Merge) 을 통해 균형을 유지

        - 활용 사례
	        - 데이터베이스 인덱스(DB Indexing) (MySQL InnoDB, SQLite 등)
	        - 파일 시스템(File System) (EXT4, HFS+, ReFS)
	        - 검색 엔진(Search Engine)
	        - Key-Value Store (LevelDB, RocksDB)

    - 결론
        - B-트리
	        - 삽입/삭제 연산이 빈번하고, 검색과 수정이 균형적으로 필요한 경우.
	        - 메모리 내 검색(예: 운영체제의 페이지 테이블) 및 일부 데이터베이스에서 사용됨.
        - B+트리
	        - 범위 검색(Range Query) 성능이 중요한 경우.
	        - 대용량 데이터 처리 및 디스크 기반 검색(Secondary Storage Indexing) 에 적합.
	        - MySQL InnoDB, NTFS, HFS+, EXT4 등 대부분의 데이터베이스 및 파일 시스템에서 사용됨.
            - 대부분의 실전 환경에서는 B+트리가 더 효율적이므로 데이터베이스 및 파일 시스템에서 널리 사용되고 있음

- 트라이(Trie) 자료구조 개념과 사용사례
    - 트라이(Trie) 자료구조 개요
        - 문자열을 저장하고 탐색하기 위한 트리(Tree) 기반 자료구조
        - 접두사 트리(Prefix Tree) 또는 디지털 검색 트리(Digital Search Tree) 일컬음
        - 빠른 검색, 자동 완성, 사전(Dictionary) 검색 등에 유용

    - 트라이(Trie) 구조
	    - 각 노드는 문자(Character)를 저장하며, 루트(Root) 노드는 비어 있음.
	    - 각 단어는 루트에서 리프까지 경로(Path)로 표현
	    - 자식 노드는 현재 문자 뒤에 올 수 있는 문자들을 저장
	    - 노드의 끝(endOfWord=True) 표시를 통해 단어의 끝을 식별

    - 예제: [“cat”, “car”, “cap”, “dog”, “dot”] 를 트라이에 저장하면?

                (root)
                /   |   \
                c     d     ...
            /|\     |\
            a  a  a  o o
            /   |   |   |
            t    r   p   g t  (endOfWord=True)

	    - cat, car, cap, dog, dot이 저장
	    - 각 노드는 단어의 일부를 나타내며, 노드 끝에서 endOfWord=True로 단어의 끝을 표시.

    - 트라이(Trie)의 주요 연산
        - 삽입(Insert)
	        - 문자열을 한 글자씩 트리를 따라 삽입.
	        - 새로운 문자가 등장하면 새로운 노드 생성.
	        - 마지막 노드에서 endOfWord=True로 단어의 끝을 표시.
            - 시간 복잡도: O(L) (L은 삽입하는 문자열 길이)

        - 검색(Search)
	        - 문자열을 한 글자씩 따라가며 존재 여부 확인.
	        - endOfWord=True인지 체크하여 단어의 끝인지 확인.
            - 시간 복잡도: O(L)

        - 삭제(Delete)
	        - 존재하는 단어의 끝을 endOfWord=False로 변경.
	        - 더 이상 필요 없는 노드를 삭제(재귀적으로 처리).
            - 시간 복잡도: O(L)

        - 자동 완성(Auto-complete)
	        - 특정 접두사(prefix)로 시작하는 단어를 검색.
	        - 해당 접두사 노드에서 하위 노드를 탐색하여 가능한 단어 반환.
            - 시간 복잡도: O(L) + O(K) (L: 접두사 길이, K: 가능한 단어 개수)

    - 트라이(Trie) 사용 사례
        - 자동 완성(Auto-complete)
	        - 검색 엔진(Google, Bing)의 검색어 자동 완성 기능.
	        - 스마트폰 키보드의 입력 보완(SwiftKey, Gboard).
	        - 검색창의 추천 단어 표시.

        - 사전(Dictionary) 및 문자열 검색
	        - 단어 목록이 저장된 사전에서 단어 존재 여부 검색.
	        - 특정 접두사로 시작하는 단어 찾기.
	        - Spell Checker(맞춤법 검사기) 구현.

        - 문자열 개수 세기(Word Frequency Counting)
	        - 단어 등장 횟수를 저장하여 가장 많이 사용된 단어 찾기.
	        - 검색 엔진에서 인기 검색어 랭킹을 유지하는 데 사용됨.

        - DNA 서열 검색
	        - 생물학에서 DNA 서열(A, C, G, T) 데이터를 저장하고 빠르게 검색.
	        - 유전자 패턴 매칭 및 유사 서열 탐색에 활용.

        - 네트워크 라우팅(Routing Table)
	        - 라우터에서 IP 주소를 접두사 기반으로 저장 및 검색.
	        - CIDR(Classless Inter-Domain Routing)와 같은 네트워크 주소 관리.

    - 트라이(Trie)를 선택해야 하는 경우
	    - 문자열 검색이 많거나, 접두사 기반 검색이 필요한 경우 (예: 자동 완성).
	    - 데이터가 정렬된 상태로 유지되어야 하는 경우.

    - 해시 테이블(Hash Table)을 선택해야 하는 경우
	    - 단순한 문자열 존재 여부만 확인할 때 (예: 단어 목록 저장).
	    - 공간 효율성이 더 중요한 경우.

    - 결론
	    - 트라이(Trie)는 문자열 검색 및 접두사 탐색에 최적화된 자료구조로, 자동 완성, 사전, 검색 엔진 등에 활용됨.
	    - 해시 테이블보다 더 많은 메모리를 사용하지만, 정렬된 검색과 접두사 검색이 가능.
	    - 검색 엔진, 맞춤법 검사기, DNA 분석, 네트워크 라우팅 등 다양한 실전 응용에서 사용됨.
        - 문자열을 빠르게 찾고, 접두사 검색을 지원하는 자료구조가 필요할 때 가장 적합한 선택

- 동적 연결 리스트(Linked List)와 배열(Array)의 차이점
    - 배열
        - 메모리 할당 방식: 고정 크기(정적 할당)
        - 메모리 위치: 연속된 메모리 블록에 저장
        - 인덱스 접근: O(1) 빠름
        - 원소 탐색(검색): 0(n) 순차 검색
        - 설명: 배열은 선언 시 크기 정해야 하며, 연속된 메모리 공간 사용해야 하므로 크기 변경 어려움
    - 연결 리스트
        - 메모리 할당 방식: 필요할 때마다 동적으로 할당
        - 메모리 위치: 임의의 메모리 위치에 저장(노드가 포인터로 연결)
        - 인덱스 접근: O(n) 느림
        - 원소 탐색(검색): 0(n) 순차 검색
        - 설명: 동적으로 노드 생성/해제 가능하여 크기 유연하게 조정 가능

- 이중 연결 리스트(Doubly Linked List)의 구조와 장점
    - 이중 연결 리스트(Doubly Linked List, DLL) 개념
	    - 각 노드(Node)가 앞(Previous)과 뒤(Next)를 가리키는 두 개의 포인터를 가지는 연결 리스트
	    - 단일 연결 리스트(Singly Linked List)는 한 방향(Next)으로만 이동 가능하지만, 이중 연결 리스트는 양방향 이동 가능(Previous, Next 포인터 존재)

    - 이중 연결 리스트의 구조
        - [Head] ↔ [Node1] ↔ [Node2] ↔ [Node3] ↔ [Tail]
        - 각 노드는 이전 노드(Prev)와 다음 노드(Next)를 가리키는 포인터를 포함

    - 이중 연결 리스트 노드 구조
        ```c
        struct Node {
            int data;       // 데이터
            Node* prev;     // 이전 노드를 가리키는 포인터
            Node* next;     // 다음 노드를 가리키는 포인터
        };
        ```

    - 이중 연결 리스트의 구조
        - 노드(Node) 구조
	        - 각 노드는 3가지 요소로 구성됨
	            - 데이터(Data): 저장할 값
	            - 이전 노드 포인터(Prev): 앞쪽 노드를 가리키는 포인터
	            - 다음 노드 포인터(Next): 뒤쪽 노드를 가리키는 포인터

    - 기본적인 연결 구조
        - NULL ← [Node1] ↔ [Node2] ↔ [Node3] → NULL
        - Node1.prev = NULL (첫 번째 노드의 Prev는 NULL)
	    - Node3.next = NULL (마지막 노드의 Next는 NULL)
	    - 중간 노드(Node2)는 양방향 링크(Prev, Next)를 가짐

    - 이중 연결 리스트의 장점
        - (1) 양방향 이동 가능
	        - 단일 연결 리스트(Singly Linked List)는 한 방향(Next)으로만 이동 가능하지만, 이중 연결 리스트는 양쪽(Prev, Next)으로 이동 가능
	        - 이전 노드로 쉽게 이동할 수 있어 탐색이 유연

        - (2) 노드 삭제 시 더 효율적
	        - 단일 연결 리스트에서는 노드를 삭제할 때 이전 노드를 탐색해야 하지만, 이중 연결 리스트는 Prev 포인터를 사용하여 O(1) 시간 복잡도로 삭제 가능
	        - 특히, 중간 노드 삭제 시 효율적

        - (3) 양방향 순회 가능
	        - 리스트를 앞에서 뒤로(Forward) 또는 뒤에서 앞으로(Backward) 순회 가능
	        - LRU(Least Recently Used) 캐시 구현에서 사용됨.

        - (4) 더블 링크 구조로 다양한 자료구조 구현 가능
	        - 이중 연결 리스트는 덱(Deque), LRU 캐시, 트리 순회 등에 활용됨
	        - 스택, 큐, 그래프 구현에도 사용 가능

    - 이중 연결 리스트의 단점
        - (1) 추가적인 메모리 사용
	        - Prev 포인터가 필요하기 때문에 단일 연결 리스트보다 메모리 사용량이 많음.

        - (2) 삽입 및 삭제 시 포인터 조작이 복잡
	        - 단일 연결 리스트보다 Prev와 Next 포인터를 모두 관리해야 하므로, 삽입/삭제 시 코드가 복잡.

        - (3) 구현이 상대적으로 어려움
	        - 포인터 관리가 복잡하여 실수로 메모리 누수 발생 가능.

    - 결론
	    - 이중 연결 리스트(Doubly Linked List)는 양방향 탐색과 빠른 삭제가 가능하므로 단일 연결 리스트보다 더 유연한 구조를 제공.
	    - 노드 삭제 및 탐색이 빠르고 양방향 순회가 가능하지만, 추가적인 메모리(Prev 포인터)가 필요하고 포인터 조작이 복잡.
	    - LRU 캐시, Undo 기능(뒤로 가기/앞으로 가기), 덱(Deque), 그래프 탐색 등에 적합.
	    - 메모리 효율이 중요한 경우 단일 연결 리스트, 성능과 유연성이 중요한 경우 이중 연결 리스트를 선택.

- 동적계획법 (Dynamic Programming, DP)
    - 정의 (What)
        - 복잡한 문제를 작은 하위 문제들로 나누어 해결하고, 그 결과를 저장하여 중복 계산을 방지하는 최적화 알고리즘 기법
        - 문제를 최적 부분 구조(Optimal Substructure)와 중복되는 하위 문제(Overlapping Subproblems)로 나눌 수 있을 때 효과적으로 사용됨
            - 최적 부분 구조, 중복 하위 문제로 나눌 수 있을 때 효과적

    - 원리 및 특징 (How)
	    - 분할과 정복 (Divide and Conquer)
	        - 문제를 더 작은 하위 문제로 나누고, 이를 해결한 결과를 조합하여 원래 문제를 해결한다.
	        - 모든 하위 문제를 독립적으로 해결하는 분할정복과는 달리, 하위 문제의 결과를 재사용한다.
	    - 최적 부분 구조 (Optimal Substructure)
	        - 문제의 최적해가 하위 문제들의 최적해로 구성될 수 있어야 한다.
	    - 메모이제이션 (Memoization)
	        - 재귀적으로 문제를 해결하며, 이미 계산된 하위 문제의 결과를 저장해 중복 계산을 방지한다. (Top-Down 방식)
	    - 테이블 작성 (Tabulation)
	        - 작은 문제부터 해결해 나가며, 테이블에 결과를 저장하여 상위 문제를 해결한다. (Bottom-Up 방식)

    - 장점 (Why)
	    - 성능 최적화
	        - 중복 계산을 줄이기 때문에 시간 복잡도가 획기적으로 개선된다.
                - 예: 피보나치 수열 계산에서 O(2^N) → O(N)으로 감소
	        - 다양한 문제 해결
	            - 경로 최적화, 문자열 문제, 그래프 문제 등 다양한 문제에 적용 가능
	        - 구현 용이성
	            - 메모이제이션과 테이블 작성 방식 모두 비교적 간단한 구현이 가능

    - 단점 및 한계
	    - 공간 복잡도 문제
	        - 테이블 또는 캐시를 저장하기 위한 메모리가 많이 필요할 수 있다.
	    - 문제 특성 제한
	        - 최적 부분 구조와 중복되는 하위 문제를 만족하지 않는 경우 사용할 수 없다.

    - 주요 적용 사례
	    - 피보나치 수열
	        - 재귀 호출 중복 문제를 해결하기 위한 대표적인 예제.
	    - 최장 공통 부분 문자열 (Longest Common Subsequence, LCS)
	        - 두 문자열의 공통 부분 문자열 길이를 구하는 문제.
	    - 최단 경로 문제 (Shortest Path Problem)
	        - 다익스트라 알고리즘, 플로이드-워셜 알고리즘에서 활용.
	    - 배낭 문제 (Knapsack Problem)
	        - 제한된 무게에서 최대 가치를 찾는 최적화 문제.

    - 사례 코드
        ```python
        # 피보나치 수열 동적계획법 (Bottom-Up)
        def fibonacci(n):
            dp = [0] * (n + 1)
            dp[1] = 1
            for i in range(2, n + 1):
                dp[i] = dp[i - 1] + dp[i - 2]
            return dp[n]

        print(fibonacci(10))  # 출력: 55
        ```


    - 결론 (핵심 요약)
        - 동적계획법은 하위 문제의 결과를 저장하여 계산 효율성을 높이는 강력한 알고리즘 기법
        - 최적화 문제를 해결하는 데 유용
        - 최적 부분 구조와 중복되는 하위 문제를 만족하는 문제에서 활용 가능하며, 성능 및 코드 효율성을 크게 향상 가능


- 원형 연결 리스트(Circular Linked List)의 개념과 활용 방안
    - 원형 연결 리스트(Circular Linked List) 개념
        - 원형 연결 리스트(Circular Linked List)는 마지막 노드가 첫 번째 노드를 가리키도록 구성된 연결 리스트
        - 즉, 리스트의 끝이 다시 리스트의 시작과 연결되어 있어 데이터를 순환 구조로 관리 가능
        - 단순 연결 리스트(Singly Linked List): 마지막 노드의 next가 NULL을 가리킴
        - 원형 연결 리스트(Circular Linked List): 마지막 노드의 next가 다시 첫 번째 노드(head) 를 가리킴

    - 원형 연결 리스트의 종류
        - 단일 원형 연결 리스트 (Singly Circular Linked List)
            - 노드가 하나의 방향(다음 노드)으로만 연결되며, 마지막 노드가 첫 번째 노드를 가리킴
        - 이중 원형 연결 리스트 (Doubly Circular Linked List)
            - 노드가 앞뒤로 연결되며, 마지막 노드의 next가 첫 번째 노드를, 첫 번째 노드의 prev가 마지막 노드를 가리킴

    - 원형 연결 리스트의 주요 특징
        - 마지막 노드가 첫 번째 노드와 연결되어 있음 → 리스트의 끝이 존재하지 않음
        - 임의의 위치에서 리스트를 순환하면서 처리 가능 → 선형 리스트보다 유연한 구조
        - 메모리 효율성이 높음 → NULL을 저장할 필요가 없음
        - 원형 구조 활용 가능 → 운영 체제의 프로세스 스케줄링 등에서 사용됨

    - 원형 연결 리스트의 주요 연산
        - 삽입 (Insertion): 리스트의 처음, 중간, 마지막 위치에 노드를 삽입
        - 삭제 (Deletion): 특정 노드 제거 후 리스트 유지
        - 순회 (Traversal): 처음부터 끝까지 순환하며 데이터 검색
        - 검색 (Search): 특정 값을 가진 노드 찾기

    - 원형 연결 리스트의 구현 (Python)
        - (1) 단일 원형 연결 리스트 구현
            ```python
            class Node:
                def __init__(self, data):
                    self.data = data
                    self.next = None  # 다음 노드를 가리키는 포인터

            class CircularLinkedList:
                def __init__(self):
                    self.head = None  # 첫 번째 노드 (head)

                def append(self, data):
                    new_node = Node(data)
                    if not self.head:  # 첫 번째 노드라면
                        self.head = new_node
                        self.head.next = self.head  # 원형 연결 리스트이므로 자신을 가리킴
                    else:
                        temp = self.head
                        while temp.next != self.head:
                            temp = temp.next  # 마지막 노드 찾기
                        temp.next = new_node
                        new_node.next = self.head  # 마지막 노드의 next를 head로 설정

                def display(self):
                    if not self.head:
                        print("리스트가 비어 있습니다.")
                        return
                    temp = self.head
                    while True:
                        print(temp.data, end=" -> ")
                        temp = temp.next
                        if temp == self.head:  # 한 바퀴 돌면 종료
                            break
                    print("(다시 head로)")

            cll = CircularLinkedList()
            cll.append(1)
            cll.append(2)
            cll.append(3)
            cll.display()

            # 출력 결과
            # 1 -> 2 -> 3 -> (다시 head로)
            # 원형 연결 리스트의 특성: 마지막 노드가 다시 첫 번째 노드(head) 를 가리킴
            ```

    - 원형 연결 리스트의 활용 방안
        - (1) 운영 체제의 프로세스 스케줄링 (CPU Scheduling)
            - 라운드 로빈(Round Robin) 스케줄링
                - 원형 연결 리스트를 이용하여 프로세스를 순환하며 실행
                - time quantum이 지나면 다음 프로세스로 전환
                - 마지막 프로세스가 실행된 후 다시 첫 번째 프로세스로 돌아옴
                    ```python
                    processes = CircularLinkedList()
                    processes.append("P1")
                    processes.append("P2")
                    processes.append("P3")
                    processes.display()
                    ```
                    - 프로세스가 순환하며 실행됨 → P1 → P2 → P3 → P1 ...

        - (2) 네트워크 토큰 링 프로토콜 (Token Ring)
            - 네트워크 통신 방식 중 토큰 링(Token Ring) 프로토콜은 원형 연결 리스트와 유사한 방식으로 동작
            - 데이터 전송을 위한 토큰(Token) 이 네트워크 노드 사이를 순환

        - (3) 멀티플레이어 게임에서 턴 기반 시스템
            - 보드 게임, 카드 게임과 같은 멀티플레이어 게임에서 턴(Turn)이 원형으로 돌아감
            - 플레이어가 순서대로 턴을 가지며, 마지막 플레이어의 다음 차례가 첫 번째 플레이어로 돌아감
    
        - (4) 데이터 버퍼 (Circular Buffer)
            - 원형 연결 리스트를 활용하여 메모리 버퍼 관리
            - 데이터가 가득 차면 가장 오래된 데이터를 덮어씀
            - 예: 오디오/비디오 스트리밍, 실시간 데이터 로깅

    - 결론
        - 원형 연결 리스트는 마지막 노드가 첫 번째 노드를 가리키는 구조
        - 운영 체제의 CPU 스케줄링, 네트워크 토큰 링, 게임 로직, 버퍼 관리 등에서 활용
        - 연결 리스트와 달리 끝이 없으므로 순환하며 데이터를 처리하는 경우 유용
        - 원형 연결 리스트는 특정 문제(순환 구조) 해결에 최적화된 자료구조

- 정렬 알고리즘에서 안정 정렬(Stable Sort)과 불안정 정렬(Unstable Sort)의 차이점
    - 정렬(Sorting) 알고리즘에서 안정성과 불안정성의 개념
        - 안정 정렬(Stable Sort)
            - 동일한 값(키)을 가진 요소들의 상대적인 순서가 유지되는 정렬 방식
            - 즉, 정렬 전 순서가 A → B 였다면, 정렬 후에도 A → B 순서가 유지됨
        - 불안정 정렬(Unstable Sort)
            - 동일한 값을 가진 요소들의 상대적인 순서가 보장되지 않는 정렬 방식
            - 정렬 후에 순서가 변경될 수 있음
            - 즉, 안정 정렬은 같은 값을 가진 요소들의 원래 순서를 유지하지만, 불안정 정렬은 그렇지 않다.

    - 안정 정렬 vs 불안정 정렬 알고리즘
        - 안정 정렬 알고리즘 (Stable Sort): 삽입, 버블, 병합, 계수, 파이썬 sort
            - 삽입 정렬 (Insertion Sort): O(n²), 작은 데이터에서 효율적
            - 버블 정렬 (Bubble Sort): O(n²), 구현이 쉬우나 비효율적
            - 병합 정렬 (Merge Sort): O(n log n), 안정 정렬이면서 효율적
            - 계수 정렬 (Counting Sort): O(n + k), 숫자가 범위 내에서 균등하게 분포할 때 유리
            - 기본적인 파이썬 sorted(): O(n log n), Timsort (안정 정렬)
            - 상황: 데이터의 상대적인 순서를 유지해야 하는 경우 (예: 데이터베이스 정렬, 객체 리스트 정렬)

        - 불안정 정렬 알고리즘 (Unstable Sort): 선택, 퀵, 힙
            - 선택 정렬 (Selection Sort): O(n²), 단순하지만 안정성이 없음
            - 퀵 정렬 (Quick Sort): O(n log n), 빠르지만, 기본 구현은 불안정
            - 힙 정렬 (Heap Sort): O(n log n), 힙 구조 사용, 안정성이 없음
            - 상황: 속도가 중요하고 데이터 순서가 중요하지 않은 경우 (예: 큰 데이터셋의 빠른 정렬)

    - 실전에서 안정 정렬이 필요한 경우
        - 은행 고객 정렬 (이름순 → 나이순 정렬)
            - 같은 이름을 가진 고객들의 원래 순서를 유지해야 함
        - 데이터베이스의 다중 열 정렬 (1차 정렬 후 2차 정렬 유지)
            - 예: ORDER BY age, name ASC
        - 웹 사이트 검색 결과 정렬 (우선순위 유지)
            - 동일 점수를 가진 문서들이 원래 순서를 유지해야 함

    - 안정 정렬 vs 불안정 정렬 선택 가이드
        - 데이터 순서 유지 필수: 병합 정렬(Merge Sort), Timsort (Python sorted())
        - 속도 최우선, 순서 중요 X: 퀵 정렬(Quick Sort), 힙 정렬(Heap Sort)
        - 메모리 사용 최적화: 퀵 정렬(Quick Sort), 힙 정렬(Heap Sort)
        - 작은 데이터 정렬: 삽입 정렬, 선택 정렬
    - 결론
        - 안정 정렬(Stable Sort): 동일한 값을 가진 요소들의 순서를 유지하는 정렬
        - 불안정 정렬(Unstable Sort): 동일한 값을 가진 요소들의 순서가 바뀔 수 있음
        - 실전에서 안정 정렬이 필요한 경우: 데이터베이스 정렬, 다중 키 정렬, 사용자 인터페이스(UI) 정렬
        - Python의 sorted()는 안정 정렬을 보장하는 Timsort 사용
        - 즉, 데이터의 원래 순서를 유지해야 한다면 "안정 정렬"을 선택하는 것이 중요


- 팀 정렬(Timsort)의 개념과 활용 사례
    - 팀 정렬(Timsort) 개념 (삽입 + 병합)
        - 삽입 정렬(Insertion Sort)과 병합 정렬(Merge Sort)을 결합하여 최적의 성능을 제공하는 정렬 알고리즘
        - Python, Java, Android, Swift 등 주요 프로그래밍 언어의 기본 정렬 알고리즘으로 사용되고 있음
        - 안정 정렬(Stable Sort) → 같은 값의 순서가 유지됨
        - 최악의 경우에도 O(n log n) 보장
        - 이미 부분적으로 정렬된 데이터에서 매우 빠름 (O(n) 성능 가능)

    - 팀 정렬(Timsort)의 동작 원리
        - (1) 분할 (Runs Detection)
            - 원본 배열에서 부분적으로 정렬된 "런(Run)"을 찾음
            - 이미 정렬된 데이터는 그대로 유지하여 불필요한 연산을 줄임
            - 최소 런 크기는 32~64 (보통 32)로 설정됨
        - (2) 삽입 정렬(Insertion Sort) 적용
            - 작은 런(Run)을 정렬할 때는 삽입 정렬 사용 (O(n²))
            - 이유: 작은 데이터에서는 삽입 정렬이 캐시 친화적이고 빠름
        - (3) 병합 정렬(Merge Sort) 적용
            - 정렬된 런(Run)을 병합할 때 병합 정렬 사용 (O(n log n))
            - 병합할 때 균형을 유지하여 성능 최적화
            - 즉, 팀 정렬은 데이터가 정렬된 정도에 따라 삽입 정렬과 병합 정렬을 조합하여 최적의 성능을 보장함

    - 팀 정렬(Timsort)의 시간 복잡도
        - 최선의 경우 (거의 정렬됨): O(n)
            - 이미 정렬된 데이터에 대해 매우 빠른 O(n) 성능을 낼 수 있음
        - 평균적인 경우: O(n log n)
        - 최악의 경우: O(n log n)
        - 참고: 일반적인 정렬과 다르게, 부분 정렬된 데이터를 활용하는 점이 핵심

    - 팀 정렬(Timsort)의 활용 사례
        - (1) Python 기본 정렬 (sorted(), list.sort())
            - Python의 sorted() 및 list.sort()는 내부적으로 팀 정렬을 사용
            - 팀 정렬은 파이썬의 공식 정렬 알고리즘으로 채택됨
            - 데이터가 부분적으로 정렬되어 있으면 더욱 빠르게 동작
                ```python
                numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5]
                sorted_numbers = sorted(numbers)  # Timsort 사용
                print(sorted_numbers)
                ```

        - (2) Java의 Arrays.sort() (Object 배열)
            - Java에서 Arrays.sort()
                - 기본형 배열(int[], double[])은 QuickSort 사용
                - 객체 배열(Object[])은 Timsort 사용
                - Java의 객체 정렬에서도 팀 정렬이 기본 적용됨
                - 참고: Android의 Collections.sort()는 팀 정렬을 사용 (모바일 환경에서 성능 최적화를 위해 팀 정렬 적용)
                    ```java
                    Arrays.sort(array);         // Timsort 사용 (Object 배열)
                    Collections.sort(list);     // 내부적으로 Timsort 적용
                    ```

        - (3) Swift의 기본 정렬
            - Swift에서도 기본 정렬 알고리즘으로 Timsort를 사용
                ```swift
                var numbers = [3, 1, 4, 1, 5, 9]
                numbers.sort()  // Timsort 사용
                ```

    - 팀 정렬(Timsort)의 장점과 단점
        - 장점
            - 안정 정렬(Stable Sort) → 같은 값을 가진 요소들의 순서를 유지
            - 부분적으로 정렬된 데이터에서 매우 빠름 (O(n))
            - 최악의 경우에도 O(n log n) 보장
            - 캐시 친화적 → CPU 성능 최적화

        - 단점
            - 추가 메모리 사용(O(n)) → 제자리 정렬(in-place)이 아님
            - 매우 작은 데이터에서는 QuickSort보다 약간 느릴 수 있음
            - 설계가 복잡하여 직접 구현하기 어려움

    - 결론
        - 팀 정렬(Timsort)은 삽입 정렬 + 병합 정렬을 결합한 강력한 정렬 알고리즘
            - 삽입 + 병합 정렬 결합
        - Python, Java, Android, Swift 등 다양한 환경에서 기본 정렬 알고리즘으로 사용
        - 부분적으로 정렬된 데이터를 빠르게 처리할 수 있어 현실 세계의 데이터 정렬에 적합
        - 안정 정렬(Stable)이며, 최악의 경우에도 O(n log n) 성능을 보장
        - 팀 정렬은 실제 환경에서 가장 효율적인 정렬 알고리즘으로 채택 되고 있음

- 런(Run)에 대한 설명
    - 개념
        - 정렬된 부분 배열(서브배열, Subarray) 을 의미
        - Timsort에서 사용되는 개념으로, 원본 배열 내에서 이미 정렬된 연속된 요소들의 그룹
    - 추가 설명
        - Timsort는 먼저 원본 배열에서 "런(Run)"을 찾은 후, 이를 병합하여 정렬 속도를 최적화
        - 런이 길수록 Timsort의 성능이 향상됨 (O(n) 가능)
        - 일반적으로 최소 런 크기는 32~64로 설정됨

    - 런(Run)의 예시
        - 예제 배열
            - [ 5, 1, 4, 7, 9, 2, 3, 6, 8, 10 ]
            - Timsort는 먼저 "런(Run)"을 찾음 (이미 정렬된 부분을 "런(Run)"으로 인식)
                - [ 1, 4, 7, 9 ] → Run 1 (정렬된 부분)
                - [ 2, 3, 6, 8, 10 ] → Run 2 (정렬된 부분)
            - 각 런(Run)을 정렬하고, 병합 정렬(Merge Sort)로 합침
            - [ 1, 2, 3, 4, 6, 7, 8, 9, 10 ]
            - 결과적으로 원본 배열이 정렬됨
                - 이미 정렬된 부분을 활용하여 불필요한 연산을 줄임

    - 런(Run)의 크기 (최소 런 크기)
        - Timsort는 최소 런 크기를 설정하여 최적화된 정렬을 수행
        - 보통 32 ~ 64 요소 크기를 기본 최소 런 크기로 사용
        - 작은 런(Run)은 삽입 정렬(Insertion Sort)로 정렬
        - 큰 런(Run)은 병합 정렬(Merge Sort)로 합병

    - 런(Run)의 특징
        - 정렬된 부분 배열(서브배열): 이미 정렬된 데이터를 찾아서 런(Run)으로 인식
        - 작은 런(Run)은 삽입 정렬 사용: 작은 배열은 Insertion Sort가 효율적
        - 큰 런(Run)은 병합 정렬 사용: 큰 배열은 Merge Sort로 합병
        - 실제 데이터에서 최적화 가능: 현실적인 데이터는 대부분 부분적으로 정렬됨

    - 런(Run) 사용 이유
        - 현실 세계의 데이터는 이미 부분적으로 정렬된 경우가 많음
        - 정렬된 부분(Run)을 활용하면 O(n) 시간 복잡도로 정렬 가능
        - 불필요한 비교 연산을 줄여 정렬 속도 향상

    - 결론
        - 런(Run)은 Timsort에서 "이미 정렬된 부분 배열"을 의미
        - 런을 활용하면 삽입 정렬 & 병합 정렬을 조합하여 최적의 성능을 낼 수 있음
        - Timsort가 Python, Java, Android에서 기본 정렬 알고리즘으로 사용되는 이유 중 하나임


- 셸 정렬(Shell Sort)의 개념과 시간 복잡도
    - 셸 정렬(Shell Sort) 개념
        - 셸 정렬(Shell Sort) 은 삽입 정렬(Insertion Sort)의 개선된 버전
        - 데이터 간 "간격(gap)"을 조절하면서 부분적으로 정렬한 후, 마지막에 삽입 정렬을 수행
        - 1959년 Donald Shell이 개발한 알고리즘으로, 부분 정렬을 활용하여 효율적으로 정렬
        - 셸 정렬의 핵심 아이디어
            - 초기 큰 간격(gap)으로 원소들을 비교 및 정렬
            - 점차 간격을 줄이며 정렬 수행
            - 마지막에는 삽입 정렬을 수행하여 최종 정렬 완료
        - 삽입 정렬보다 빠른 이유
            - 삽입 정렬은 한 칸씩만 이동해야 하지만, 셸 정렬은 큰 간격으로 이동하여 빠르게 정렬 가능
            - 작은 간격에서 정렬할 때, 대부분의 데이터가 이미 정렬된 상태이므로 삽입 정렬이 최적의 성능(O(n))을 발휘

    - 셸 정렬(Shell Sort) 알고리즘 동작 과정
        - 예제: [9, 8, 3, 7, 5, 6, 4, 1] 정렬
        - Step 1: 초기 간격(gap) 설정
            - 보통 배열 길이의 절반 (N/2) 을 첫 번째 간격으로 설정
            ```
            초기 배열: [9, 8, 3, 7, 5, 6, 4, 1]
            gap = 4 (길이 8 / 2)
            ```

        - Step 2: gap 간격으로 삽입 정렬 수행
            ```
            [9, 8, 3, 7, 5, 6, 4, 1]
              ↔          ↔          ↔   (4칸 간격 비교)
            변환 후: [5, 1, 3, 4, 9, 6, 8, 7]
            ```

        - Step 3: gap을 줄이고(gap=2) 다시 삽입 정렬 수행
            ```
            [5, 1, 3, 4, 9, 6, 8, 7]
              ↔   ↔   ↔   ↔   ↔   ↔   (2칸 간격 비교)
            변환 후: [3, 1, 5, 4, 7, 6, 9, 8]
            ```

        - Step 4: gap=1, 삽입 정렬 수행
            ```
            [3, 1, 5, 4, 7, 6, 9, 8]
            삽입 정렬 → [1, 3, 4, 5, 6, 7, 8, 9] (완전 정렬)
            ```
            - 즉, 부분적으로 정렬하면서 최종 정렬 시 삽입 정렬이 빠르게 수행됨

    - 셸 정렬(Shell Sort) 시간 복잡도 분석
        - 최악 시간 복잡도 (Worst Case)
            - 일반적인 간격(gap) 선택 방식에서는 O(n²)
            - 하지만 Hibbard 또는 Knuth 간격을 사용하면 O(n^(3/2) 또는 O(n log² n))으로 개선 가능
        - 최선 시간 복잡도 (Best Case)
            - 데이터가 거의 정렬된 상태라면 O(n)
        - 평균 시간 복잡도 (Average Case)
            - 간격 선택에 따라 다르지만, 보통 O(n log n) ~ O(n^(3/2))
        - GAP(간격)에 따른 시간복잡도
            - 단순한 N/2 방식: O(n²)
            - Hibbard 수열 (1, 3, 7, 15, 31...): O(n^(3/2)) -> O(n^1.5)
            - Knuth 수열 (1, 4, 13, 40...): O(n log² n)
        - 결론
            - 삽입 정렬보다 빠르지만, 퀵 정렬(Quick Sort)보다는 느릴 수 있음
            - 최적의 간격(gap) 선택에 따라 성능이 달라짐
            - 실제 응용에서는 O(n log n)에 가까운 성능을 낼 수 있어 중형 크기 데이터에 유용

    - 셸 정렬(Shell Sort) Python 코드 구현
        ```python
        def shell_sort(arr):
            n = len(arr)
            gap = n // 2  # 초기 간격 설정

            while gap > 0:
                for i in range(gap, n):
                    temp = arr[i]
                    j = i
                    # 삽입 정렬 수행 (gap만큼 떨어진 요소 비교)
                    while j >= gap and arr[j - gap] > temp:
                        arr[j] = arr[j - gap]
                        j -= gap
                    arr[j] = temp
                gap //= 2  # 간격을 줄임

        arr = [9, 8, 3, 7, 5, 6, 4, 1]
        shell_sort(arr)
        print(arr)  # 출력: [1, 3, 4, 5, 6, 7, 8, 9]
        ```
        - 간격을 줄여가며 삽입 정렬을 수행하여 정렬 속도를 최적화
        - 삽입 정렬보다 빠르게 정렬 가능하지만, 퀵 정렬보다는 다소 느릴 수 있음

    - 셸 정렬(Shell Sort)의 장점과 단점
        - 장점
            - 삽입 정렬보다 훨씬 빠름 → 큰 간격 정렬 후 작은 간격 정렬이 효율적
            - 정렬된 데이터가 많을수록 빠르게 정렬 가능
            - 추가 메모리 사용이 적음 → O(1) 공간 복잡도 (In-place 정렬)
            - 힙 정렬(Heap Sort)보다 비교 연산이 적어 실제 성능이 더 나을 수 있음

        - 단점
            - 퀵 정렬(Quick Sort)보다 일반적으로 성능이 떨어짐
            - 간격(gap) 선택에 따라 성능이 크게 달라짐
            - 최악의 경우 O(n²)으로 비효율적일 수 있음

    - 결론
        - 셸 정렬(Shell Sort)은 삽입 정렬의 단점을 개선하여 더 빠르게 동작하는 정렬 알고리즘
        - 간격(Gap)을 조절하면서 정렬하는 방식으로, 중형 크기 데이터에 적합
        - 평균 시간 복잡도는 O(n log² n)으로 삽입 정렬(O(n²))보다 빠름
        - 퀵 정렬(Quick Sort)보다는 느릴 수 있으나, 추가 메모리를 거의 사용하지 않음
        - 즉, 셸 정렬은 삽입 정렬을 개선한 강력한 알고리즘으로, 정렬 속도를 최적화할 때 매우 유용

- Pigeonhole 정렬 알고리즘의 개념과 사용 사례
    - Pigeonhole 정렬 알고리즘 개념
        - 특정 범위 내의 정수를 정렬하는데 최적화된 정렬 알고리즘
        - 계수 정렬(Counting Sort)과 유사한 방식으로 동작
        - 데이터의 최소값과 최대값의 범위가 작을 때 매우 빠르게 정렬할 수 있음

    - 기본 개념
        - 입력 데이터의 범위(R)와 크기(N)가 비슷할 때 매우 빠르게 동작
        - 빈 배열(Pigeonholes, 구멍)을 생성하여 데이터의 개수를 저장하는 방식으로 작동
        - 시간 복잡도: O(N + R) (데이터 개수 N과 값의 범위 R에 의해 결정됨)

    - Pigeonhole 정렬의 동작 과정
        - 입력 배열에서 최소값과 최대값을 찾음 → 범위 R을 계산
        - 길이가 R인 배열(Pigeonholes)을 생성
        - 각 숫자를 해당 위치에 저장
        - 배열을 순차적으로 탐색하며 정렬된 데이터를 출력

    - Pigeonhole 정렬의 예제
        - 예제: [8, 3, 2, 7, 4, 6, 8] 정렬
            - 최소값: 2, 최대값: 8 → 범위 R = 8 - 2 + 1 = 7
            - 크기 7(0~6)의 Pigeonholes 배열 생성
            - 각 숫자를 해당 위치(인덱스)에 저장 (예: 8 - 최소값(2) = 6)
            - 배열을 순차적으로 읽어 정렬된 리스트 생성

    - 코드 구현
        ```python
        def pigeonhole_sort(arr):
            min_val = min(arr)
            max_val = max(arr)
            size = max_val - min_val + 1

            # Pigeonholes 배열 생성
            holes = [0] * size

            # 입력 배열의 값을 해당 pigeonhole에 저장
            for num in arr:
                holes[num - min_val] += 1

            # 정렬된 결과 생성
            index = 0
            for i in range(size):
                while holes[i] > 0:
                    arr[index] = i + min_val
                    index += 1
                    holes[i] -= 1

        arr = [8, 3, 2, 7, 4, 6, 8]
        pigeonhole_sort(arr)
        print(arr)  # 출력: [2, 3, 4, 6, 7, 8, 8]
        ```
        - O(N + R)의 시간 복잡도로 정렬을 수행함
        - 입력 범위가 작을 경우 매우 빠른 성능 제공

    - Pigeonhole 정렬의 사용 사례
        - 정수 범위가 작은 경우
            - 정렬할 데이터의 개수(N)가 적고, 값의 범위(R)가 작은 경우
        - 유일한 정수 키가 있는 데이터 정렬
            - 항공편 좌석 번호, 병원 대기 번호 등 연속된 번호를 정렬할 때 유용
        - 데이터가 특정 범위 내에서 균등하게 분포된 경우
            - 수치 데이터의 값이 한정된 범위에서 고르게 분포될 때 높은 성능 발휘

    - 결론: Pigeonhole 정렬은 정렬할 데이터가 특정한 범위 내에 있을 때 매우 강력하지만, 범위가 너무 클 경우 비효율적

- 내부 정렬(Internal Sorting)과 외부 정렬(External Sorting)의 차이점을 설명하시오.
- O(n log n) 정렬 알고리즘을 비교하고 어떤 상황에서 어떤 정렬이 유리한지 설명하시오.
- 특정 데이터 크기(예: 1억 개)의 숫자를 정렬해야 할 때 가장 적합한 알고리즘을 선택하고 이유를 설명하시오.
- 페르마의 소정리(Fermat’s Little Theorem)란 무엇인가?
- RSA 암호화 알고리즘에서 소수를 이용하는 이유를 설명하시오.
- 해시 함수(Hash Function)의 개념과 이상적인 조건을 설명하시오.
- 체이닝(Chaining)과 개방 주소법(Open Addressing)의 차이점을 설명하시오.
- 퍼펙트 해시(Perfect Hashing)란 무엇인가?
- 블룸 필터(Bloom Filter)의 개념과 사용 사례를 설명하시오.
- 선형 탐색(Linear Search)와 이진 탐색(Binary Search)의 차이를 설명하시오.
- 점프 탐색(Jump Search)와 보간 탐색(Interpolation Search)의 개념과 차이점을 설명하시오.
- 그래프 탐색에서 DFS가 BFS보다 유리한 경우와 반대의 경우를 설명하시오.
- 위상 정렬(Topological Sorting)의 개념과 구현 방법을 설명하시오.
- 유니온-파인드(Union-Find) 알고리즘의 개념과 활용 사례를 설명하시오.
- 최소 스패닝 트리(MST)에서 크루스칼 알고리즘과 프림 알고리즘의 차이점을 설명하시오.
- 네트워크 플로우(Network Flow) 문제를 해결하는 방법을 설명하시오.
- 이중 연결 그래프(Biconnected Graph)란 무엇인가?
- 그래프에서 강한 연결 요소(Strongly Connected Components, SCC)의 개념을 설명하시오.
- Z-알고리즘(Z-Algorithm)의 개념과 활용 사례를 설명하시오.
- 라빈-카프(Rabin-Karp) 알고리즘의 시간 복잡도를 분석하시오.
- 보이어-무어(Boyer-Moore) 알고리즘이 KMP 알고리즘보다 빠를 수 있는 이유를 설명하시오.
- 접미사 트리(Suffix Tree)의 개념과 활용 방안을 설명하시오.
- 편집 거리(Edit Distance) 알고리즘(Levenshtein Distance)의 개념을 설명하시오.
- 롤링 해시(Rolling Hash)의 개념과 활용 방안을 설명하시오.
- RSA 암호화 알고리즘의 원리와 수학적 기반을 설명하시오.
- 대칭키 암호화(Symmetric Encryption)와 비대칭키 암호화(Asymmetric Encryption)의 차이점을 설명하시오.
- AES(Advanced Encryption Standard) 알고리즘의 원리를 설명하시오.
- 해시 함수(Hash Function)의 주요 특징과 안전한 해시 알고리즘(SHA, MD5 등)의 차이점을 설명하시오.
- 디피-헬만 키 교환(Diffie-Hellman Key Exchange) 방식의 개념을 설명하시오.
- 블록 암호(Block Cipher)와 스트림 암호(Stream Cipher)의 차이를 설명하시오.
- 의사 결정 트리(Decision Tree)의 개념과 활용 사례를 설명하시오.
- 랜덤 포레스트(Random Forest) 알고리즘의 원리와 장점을 설명하시오.
- 신경망(Neural Network)의 개념과 활성화 함수(Activation Function)의 역할을 설명하시오.
- 서포트 벡터 머신(SVM)의 개념과 커널 트릭(Kernel Trick)의 활용을 설명하시오.
- K-최근접 이웃(K-Nearest Neighbor, KNN) 알고리즘의 개념과 단점을 설명하시오.
- 유전자 알고리즘(Genetic Algorithm)의 개념과 적용 사례를 설명하시오.
- 분할 정복(Divide and Conquer)과 동적 계획법(Dynamic Programming)의 차이점을 설명하시오.
- 탐욕 알고리즘(Greedy Algorithm)의 특징과 한계를 설명하시오.
- 메타휴리스틱 알고리즘(Metaheuristic Algorithm)의 개념과 예제를 설명하시오.
- 근사 알고리즘(Approximation Algorithm)의 개념과 활용 사례를 설명하시오.
- NP-완전 문제(NP-Complete)의 개념과 대표적인 문제를 설명하시오.
- 시뮬레이티드 어닐링(Simulated Annealing)의 개념과 활용 사례를 설명하시오.
- 트리(Tree)란 무엇이며, 트리의 주요 특징을 설명하시오.
- 이진 탐색 트리(BST)의 개념과 탐색, 삽입, 삭제 연산의 시간 복잡도를 설명하시오.
- AVL 트리와 레드-블랙 트리(Red-Black Tree)의 차이점을 설명하시오.
- B-트리(B-Tree)의 구조와 활용 사례를 설명하시오.
- B+ 트리(B+ Tree)와 B-트리의 차이점을 설명하시오.
- 트리 순회(Tree Traversal) 방식인 전위 순회(Preorder), 중위 순회(Inorder), 후위 순회(Postorder)의 차이점을 설명하시오.
- 레드-블랙 트리(Red-Black Tree)의 삽입과 삭제 시 재구성 과정에 대해 설명하시오.
- K-ary 트리란 무엇이며, 이진 트리와의 차이점을 설명하시오.
- Huffman 트리의 개념과 텍스트 압축에 활용되는 원리를 설명하시오.
- 볼록 껍질(Convex Hull) 알고리즘의 개념과 구현 방법을 설명하시오.
- 최근접 점 쌍(Closest Pair of Points) 문제를 해결하는 방법을 설명하시오.
- 회전하는 캘리퍼스(Rotating Calipers) 기법의 개념과 활용 사례를 설명하시오.
- 점 내 포함 문제(Point-in-Polygon, PIP)의 개념과 해결 방법을 설명하시오.
- 브루트 포스(Brute Force)를 이용한 볼록 껍질 알고리즘의 시간 복잡도를 분석하시오.
- 피보나치 수열을 재귀(Recursive)와 동적 계획법(DP)으로 구현하는 방법을 비교하시오.
- 최장 공통 부분 수열(Longest Common Subsequence, LCS) 알고리즘의 개념과 구현 방법을 설명하시오.
- 배낭 문제(Knapsack Problem)에서 0-1 Knapsack과 Fractional Knapsack의 차이점을 설명하시오.
- 행렬 체인 곱셈(Matrix Chain Multiplication) 알고리즘을 설명하시오.
- 최장 증가 부분 수열(Longest Increasing Subsequence, LIS)의 개념과 구현 방법을 설명하시오.
- 그래프 색칠 문제(Graph Coloring Problem)란 무엇인가?
- 최소 색칠 문제(Minimum Graph Coloring)를 해결하는 방법을 설명하시오.
- 4색 정리(Four Color Theorem)의 개념과 의미를 설명하시오.
- NP-완전 문제로서의 그래프 색칠 문제의 특징을 설명하시오.
- 백트래킹을 이용한 그래프 색칠 알고리즘을 설명하시오.
- 근사 알고리즘(Approximation Algorithm)이란 무엇이며, 활용 사례를 설명하시오.
- 여행하는 외판원 문제(Travelling Salesman Problem, TSP)에서 근사 알고리즘을 적용하는 방법을 설명하시오.
- 시뮬레이티드 어닐링(Simulated Annealing)의 개념과 최적화 문제 해결에의 적용을 설명하시오.
- 유전 알고리즘(Genetic Algorithm)의 개념과 동작 원리를 설명하시오.
- 개미 군집 최적화(Ant Colony Optimization, ACO) 알고리즘의 개념과 활용 사례를 설명하시오.
- 입자 군집 최적화(Particle Swarm Optimization, PSO) 알고리즘의 개념과 활용 사례를 설명하시오.
- 에라토스테네스의 체(Sieve of Eratosthenes) 알고리즘의 개념과 활용 사례를 설명하시오.
- 유클리드 호제법(Euclidean Algorithm)을 이용한 최대공약수(GCD) 구하는 방법을 설명하시오.
- 확장 유클리드 알고리즘(Extended Euclidean Algorithm)의 개념과 활용 방안을 설명하시오.
- 모듈러 연산(Modular Arithmetic)이란 무엇이며, RSA 암호화와의 관계를 설명하시오.
- 중국인의 나머지 정리(Chinese Remainder Theorem, CRT)의 개념과 활용 사례를 설명하시오.
- 페르마의 소정리(Fermat’s Little Theorem)의 개념과 활용 방안을 설명하시오.
- 로지스틱 회귀(Logistic Regression)의 개념과 활용 사례를 설명하시오.
- K-평균 군집화(K-Means Clustering) 알고리즘의 개념과 활용 사례를 설명하시오.
- 랜덤 포레스트(Random Forest) 알고리즘의 개념과 장점을 설명하시오.
- 신경망(Neural Network)의 활성화 함수(Activation Function) 개념을 설명하시오.
- 그래디언트 부스팅(Gradient Boosting)과 XGBoost의 차이점을 설명하시오.
- 딥러닝에서 CNN(Convolutional Neural Network)과 RNN(Recurrent Neural Network)의 차이점을 설명하시오.
- 분할 정복과 동적 계획법의 차이점을 실제 사례와 함께 설명하시오.
- BFS와 DFS를 이용한 문제 해결 사례를 설명하시오.
- 메모이제이션(Memoization) 기법이 필요한 경우와 그 장점을 설명하시오.
- 문제 해결을 위해 알고리즘을 선택할 때 고려해야 할 요소는 무엇인가?
- 문제 해결 전략에서 탐욕 알고리즘을 적용할 수 있는 조건은 무엇인가?
- 최소 공통 조상(Lowest Common Ancestor, LCA) 문제의 개념과 해결 방법을 설명하시오.
- 세그먼트 트리(Segment Tree)의 개념과 활용 사례를 설명하시오.
- 펜윅 트리(Fenwick Tree, Binary Indexed Tree)의 개념과 구현 방법을 설명하시오.
- 트라이(Trie) 자료구조의 개념과 활용 사례를 설명하시오.
- 그래프에서 타잔(Tarjan)의 알고리즘을 이용한 강한 연결 요소(SCC) 찾기 방법을 설명하시오.
- 벨만-포드 알고리즘과 다익스트라 알고리즘의 차이점을 비교하시오.
- 플로이드-워셜 알고리즘과 벨만-포드 알고리즘의 차이점을 설명하시오.
- 그래프에서 최단 경로 문제를 해결하기 위해 다익스트라, 벨만-포드, 플로이드-워셜 중 어떤 것을 선택해야 하는지 설명하시오.
- 최대 독립 집합(Maximum Independent Set)의 개념과 해결 방법을 설명하시오.
- 최소 컷-최대 유량(Min-Cut Max-Flow) 정리를 설명하시오.
- 조합(Combination)과 순열(Permutation)의 개념과 차이점을 설명하시오.
- 파스칼의 삼각형(Pascal’s Triangle)과 이항 계수(Binomial Coefficient)의 관계를 설명하시오.
- 카탈란 수(Catalan Number)의 개념과 활용 사례를 설명하시오.
- 스타링 수(Stirling Number)의 개념과 활용 사례를 설명하시오.
- 모듈러 역원(Modular Inverse)과 확장 유클리드 알고리즘(Extended Euclidean Algorithm)을 이용한 계산 방법을 설명하시오.
- 중국인의 나머지 정리(Chinese Remainder Theorem, CRT)의 개념과 활용 사례를 설명하시오.
- 폴라드 로(Pollard Rho) 알고리즘을 이용한 소인수 분해(Factorization) 방법을 설명하시오.
- 아호-코라식(Aho-Corasick) 알고리즘의 개념과 활용 사례를 설명하시오.
- Z-알고리즘(Z-Algorithm)을 이용한 문자열 검색 방법을 설명하시오.
- 접미사 트리(Suffix Tree)와 접미사 배열(Suffix Array)의 차이점을 설명하시오.
- 문자열 정렬을 위한 버켓 정렬(Bucket Sort)의 개념과 구현 방법을 설명하시오.
- 롤링 해시(Rolling Hash) 기법을 이용한 서브스트링 검색 방법을 설명하시오.
- 퍼셉트론(Perceptron) 알고리즘의 개념과 활용 사례를 설명하시오.
- 확률적 경사 하강법(Stochastic Gradient Descent, SGD)의 개념과 적용 사례를 설명하시오.
- 신경망(Neural Network)에서 과적합(Overfitting)을 방지하는 방법을 설명하시오.
- 앙상블 학습(Ensemble Learning)의 개념과 랜덤 포레스트(Random Forest)와 부스팅(Boosting)의 차이를 설명하시오.
- 서포트 벡터 머신(SVM)의 핵심 개념과 커널 기법(Kernel Trick)을 설명하시오.
- 허프만 코딩(Huffman Coding) 알고리즘의 개념과 압축 원리를 설명하시오.
- 런-길이 부호화(Run-Length Encoding, RLE) 알고리즘의 개념과 활용 사례를 설명하시오.
- BWT(Burrows-Wheeler Transform)의 개념과 데이터 압축에서의 활용 방법을 설명하시오.
- Lempel-Ziv-Welch(LZW) 압축 알고리즘의 개념과 구현 방법을 설명하시오.
- 데이터베이스 인덱싱에서 B-트리(B-Tree)와 해시 인덱싱의 차이점을 설명하시오.
- 멀티스레딩(Multi-threading)과 병렬 처리(Parallel Processing)의 차이점을 설명하시오.
- 뮤텍스(Mutex)와 세마포어(Semaphore)의 차이점을 설명하시오.
- 데드락(Deadlock)의 개념과 예방 방법을 설명하시오.
- 생산자-소비자 문제(Producer-Consumer Problem)를 해결하는 방법을 설명하시오.
- 스핀락(Spinlock)의 개념과 사용 사례를 설명하시오.
- 분산 알고리즘(Distributed Algorithm)의 개념과 주요 활용 사례를 설명하시오.
- 양자 컴퓨팅(Quantum Computing)에서의 알고리즘(예: Shor’s Algorithm, Grover’s Algorithm)의 개념을 설명하시오.
- 딥러닝에서 사용하는 최적화 기법(Adam, RMSProp 등)의 개념을 설명하시오.
- 강화 학습(Reinforcement Learning)의 개념과 Q-learning, SARSA 알고리즘의 차이점을 설명하시오.
- 최신 AI 및 알고리즘 연구에서 가장 주목받는 기술 동향을 설명하시오.
- 고급 암호화 기법(예: 동형 암호, 격자 기반 암호)의 개념을 설명하시오.
- 블록체인(Blockchain)에서 사용되는 해시 알고리즘과 블록 검증 방법을 설명하시오.
- 생체 인식(Biometrics)에서 딥러닝을 활용한 얼굴 인식 및 지문 인식 알고리즘을 설명하시오.
- 자율 주행 자동차에서 활용되는 알고리즘(예: SLAM, Dijkstra, A*)을 설명하시오.
- 분산 시스템에서의 합의 알고리즘(Consensus Algorithm)의 개념과 종류를 설명하시오.
- 리더 선출 알고리즘(Leader Election Algorithm)의 개념과 활용 사례를 설명하시오.
- Paxos 알고리즘의 개념과 분산 환경에서의 적용 사례를 설명하시오.
- Raft 알고리즘의 원리와 Paxos와의 차이점을 설명하시오.
- 블록체인(Blockchain)에서 사용되는 합의 알고리즘(PoW, PoS 등)의 개념과 차이점을 설명하시오.
- 분산 해시 테이블(Distributed Hash Table, DHT)의 개념과 활용 사례를 설명하시오.
- 클라우드 컴퓨팅에서 사용되는 오토스케일링(Auto Scaling) 알고리즘의 개념을 설명하시오.
- 컨테이너 오케스트레이션(Container Orchestration)에서 사용되는 스케줄링 알고리즘을 설명하시오.
- 강화 학습에서 사용하는 DQN(Deep Q-Network)의 개념과 활용 사례를 설명하시오.
- AlphaGo가 사용한 MCTS(Monte Carlo Tree Search) 알고리즘의 개념과 적용 방법을 설명하시오.
- GAN(Generative Adversarial Network)의 개념과 활용 사례를 설명하시오.
- 트랜스포머(Transformer) 모델의 구조와 기존 RNN, CNN과의 차이점을 설명하시오.
- 딥러닝에서 BERT(Bidirectional Encoder Representations from Transformers)의 개념과 활용 사례를 설명하시오.
- YOLO(You Only Look Once) 알고리즘의 개념과 실시간 객체 탐지에서의 활용 방법을 설명하시오.
- 자율주행 자동차에서 사용되는 SLAM(Simultaneous Localization and Mapping) 알고리즘을 설명하시오.
- 양자 컴퓨팅에서 Shor’s Algorithm이 기존 암호화 체계를 위협하는 이유를 설명하시오.
- 딥러닝 기반의 알고리즘 압축 기술(Pruning, Quantization 등)의 개념과 활용 사례를 설명하시오.
- 연합 학습(Federated Learning)의 개념과 데이터 보호를 위한 활용 사례를 설명하시오.
- 프로세스 스케줄링(Process Scheduling) 알고리즘의 종류와 차이점을 설명하시오.
- CPU 스케줄링에서 SJF(Shortest Job First)와 Round Robin(RR)의 차이점을 설명하시오.
- 다중 스레딩(Multi-threading)과 멀티프로세싱(Multi-processing)의 차이점을 설명하시오.
- 병렬 컴퓨팅에서 OpenMP와 MPI의 차이점을 설명하시오.
- 동기(Synchronous)와 비동기(Asynchronous) 프로그래밍의 개념과 활용 사례를 설명하시오.
- GPU 연산에서 CUDA와 OpenCL의 차이점을 설명하시오.
- Task Scheduling에서 DAG(Directed Acyclic Graph)를 활용하는 방법을 설명하시오.
- PageRank 알고리즘의 개념과 웹 검색 엔진에서의 활용 사례를 설명하시오.
- TF-IDF(Term Frequency-Inverse Document Frequency)의 개념과 검색 엔진에서의 활용을 설명하시오.
- 협업 필터링(Collaborative Filtering)의 개념과 추천 시스템에서의 활용 방법을 설명하시오.
- 콘텐츠 기반 필터링(Content-Based Filtering)의 개념과 활용 사례를 설명하시오.
- 하이브리드 추천 시스템(Hybrid Recommendation System)의 개념과 적용 사례를 설명하시오.
- 자연어 처리(NLP)에서 Word2Vec과 FastText의 차이점을 설명하시오.
- 검색 엔진 최적화(SEO)에서 사용되는 주요 알고리즘을 설명하시오.
- PID(비례-적분-미분) 제어 알고리즘의 개념과 활용 사례를 설명하시오.
- 칼만 필터(Kalman Filter)의 개념과 센서 데이터 융합에서의 활용 방법을 설명하시오.
- A* (A-Star) 알고리즘의 개념과 로봇 경로 탐색에서의 활용 방법을 설명하시오.
- 최대 유량 문제(Maximum Flow Problem)란 무엇인가?
- 포드-풀커슨(Ford-Fulkerson) 알고리즘의 개념과 구현 방법을 설명하시오.
- 최소 컷(Minimum Cut) 문제란 무엇인가?
- P vs NP 문제란 무엇이며, 현재 연구 동향을 설명하시오.
- 밀러-라빈(Miller-Rabin) 소수 판별법이란 무엇인가?
- 서포트 벡터 머신(Support Vector Machine)의 개념과 활용 방안을 설명하시오.
- 다중 집합(Multiset) 문제를 해결하는 방법을 설명하시오.
- 분수 Knapsack 문제와 0-1 Knapsack 문제의 차이를 설명하시오.
- D* 알고리즘과 A* 알고리즘의 차이점을 설명하시오.
- RRT(Rapidly-exploring Random Tree) 알고리즘의 개념과 로봇 경로 계획에서의 활용을 설명하시오.
- ACID(원자성, 일관성, 고립성, 지속성)의 개념과 데이터베이스에서의 적용 방법을 설명하시오.
- 2단계 잠금(2-Phase Locking, 2PL) 기법의 개념과 활용 사례를 설명하시오.
- 트랜잭션 스케줄링에서 직렬 가능성(Serializability)의 개념을 설명하시오.
- 데이터베이스 샤딩(Sharding)과 파티셔닝(Partitioning)의 차이점을 설명하시오.
- NoSQL과 RDBMS의 차이점을 설명하고, 각 기술이 적합한 사례를 설명하시오.
- 동형 암호(Homomorphic Encryption)의 개념과 활용 사례를 설명하시오.
- 격자 기반 암호(Lattice-Based Cryptography)의 개념과 기존 암호화 방식과의 차이점을 설명하시오.
- 블록체인의 해시 함수(Hash Function)가 보안성을 보장하는 원리를 설명하시오.
- TLS(Transport Layer Security) 핸드셰이크 과정과 보안 프로토콜을 설명하시오.
- OAuth와 OpenID Connect의 차이점과 활용 사례를 설명하시오.
- 엣지 컴퓨팅(Edge Computing)에서의 데이터 처리 알고리즘을 설명하시오.
- 5G 네트워크에서 MEC(Multi-access Edge Computing)의 개념과 활용 사례를 설명하시오.
- 양자 내성 암호(Post-Quantum Cryptography)란 무엇이며, 현재 연구 동향을 설명하시오.
- 디지털 트윈(Digital Twin) 기술과 활용 사례를 설명하시오.
- IoT(Internet of Things)에서 사용되는 효율적인 데이터 압축 및 전송 알고리즘을 설명하시오.
- 시스템 부하 분산(Load Balancing) 알고리즘의 개념과 주요 기법을 설명하시오.
- 라운드 로빈(Round Robin)과 가중치 기반 라운드 로빈(Weighted Round Robin)의 차이점을 설명하시오.
- 서버 부하 분산에서 해시 기반 로드 밸런싱(Hash-Based Load Balancing)의 개념을 설명하시오.
- 캐시 일관성(Cache Consistency) 문제를 해결하는 방법을 설명하시오.
- CDN(Content Delivery Network)에서 사용되는 캐싱 및 최적화 기법을 설명하시오.
- LRU(Least Recently Used)와 LFU(Least Frequently Used) 캐싱 알고리즘의 차이점을 설명하시오.
- 대규모 데이터베이스 시스템에서 샤딩(Sharding)과 리플리케이션(Replication)의 차이를 설명하시오.
- RAID(Redundant Array of Independent Disks) 레벨별 성능 차이와 활용 사례를 설명하시오.
- 장애 감지(Fault Detection) 알고리즘의 개념과 활용 사례를 설명하시오.
- 분산 시스템에서 장애 감지와 복구를 위한 하트비트(Heartbeat) 알고리즘을 설명하시오.
- 백업 및 복구(Backup & Recovery) 전략에서 Snapshotting 기법을 설명하시오.
- 데이터 무결성(Data Integrity)을 보장하는 방법과 관련 알고리즘을 설명하시오.
- 장애 조치(Failover)와 장애 복구(Failback)의 차이를 설명하시오.
- 침입 탐지 시스템(IDS: Intrusion Detection System)에서 사용되는 이상 탐지(Anomaly Detection) 알고리즘을 설명하시오.
- 머신러닝을 이용한 보안 위협 탐지 기법을 설명하시오.
- 방화벽(Firewall)과 침입 방지 시스템(IPS: Intrusion Prevention System)의 차이를 설명하시오.
- DDoS(Distributed Denial of Service) 공격 탐지 및 대응 알고리즘을 설명하시오.
- 해시 기반 메시지 인증 코드(HMAC: Hash-based Message Authentication Code)의 원리를 설명하시오.
- 웹 애플리케이션 보안에서 SQL 인젝션 탐지 및 방어 방법을 설명하시오.
- 사이버 보안에서 공인 키 인프라(PKI: Public Key Infrastructure)의 역할과 알고리즘을 설명하시오.
- 유전 알고리즘(Genetic Algorithm)을 활용한 최적화 기법을 설명하시오.
- 시뮬레이티드 어닐링(Simulated Annealing) 알고리즘의 개념과 활용 사례를 설명하시오.
- 강화 학습에서 Q-learning과 SARSA의 차이를 설명하시오.
- 차량 경로 최적화(Vehicle Routing Problem, VRP)에서 사용되는 알고리즘을 설명하시오.
- 선형 계획법(Linear Programming)과 정수 계획법(Integer Programming)의 차이를 설명하시오.
- 라그랑주 승수법(Lagrange Multiplier)과 최적화 문제 해결에서의 활용을 설명하시오.
- 동적 계획법(DP)을 활용한 최소 비용 경로 문제 해결 방법을 설명하시오.
- 컨테이너 오케스트레이션(Container Orchestration)에서 사용되는 스케줄링 알고리즘을 설명하시오.
- 마이크로서비스 아키텍처(Microservices Architecture)에서 서비스 디스커버리(Service Discovery) 알고리즘을 설명하시오.
- 클라우드 환경에서 사용되는 무상태(State-less)와 상태 저장(State-full) 아키텍처의 차이를 설명하시오.
- 서버리스(Serverless) 컴퓨팅에서 동적 리소스 할당 알고리즘을 설명하시오.
- API Gateway에서 사용되는 부하 분산 및 캐싱 전략을 설명하시오.
- 엣지 컴퓨팅(Edge Computing)에서 사용되는 데이터 필터링 및 처리 알고리즘을 설명하시오.
- IoT(Internet of Things)에서 저전력 네트워크 프로토콜(LPWA, LoRa, NB-IoT)의 차이를 설명하시오.
- IoT 디바이스에서 데이터 동기화를 위한 분산 알고리즘을 설명하시오.
- 실시간 스트리밍 데이터 처리에서 CEP(Complex Event Processing)의 개념과 알고리즘을 설명하시오.
- MQTT(Message Queuing Telemetry Transport) 프로토콜과 데이터 송수신 최적화 기법을 설명하시오.
- 양자 컴퓨팅(Quantum Computing)에서 슈어(Shor's Algorithm)의 개념과 기존 암호 체계에 미치는 영향을 설명하시오.
- 양자 컴퓨터에서 그로버(Grover’s Algorithm)를 활용한 데이터 검색 최적화 방법을 설명하시오.
- AI 기반 코드 자동 생성 모델(예: GitHub Copilot)의 알고리즘 원리와 활용 사례를 설명하시오.
- 컴퓨터 아키텍처(Computer Architecture)의 주요 구성 요소와 역할을 설명하시오.
- CISC와 RISC 프로세서의 차이점을 설명하시오.
- 파이프라이닝(Pipelining)의 개념과 성능 향상 효과를 설명하시오.
- 스레드(Thread)와 프로세스(Process)의 차이점을 설명하시오.
- 가상 메모리(Virtual Memory)의 개념과 페이지 교체(Page Replacement) 알고리즘을 설명하시오.
- 페이지 교체 알고리즘(FIFO, LRU, Optimal)의 개념과 성능 비교를 설명하시오.
- 세그먼테이션(Segmentation)과 페이징(Paging)의 차이점을 설명하시오.
- 동기식 I/O와 비동기식 I/O의 차이점을 설명하시오.
- 커널 모드(Kernel Mode)와 사용자 모드(User Mode)의 차이를 설명하시오.
- 데이터베이스 정규화(Normalization) 과정과 이상현상(Anomaly) 해결 방법을 설명하시오.
- 정규형(NF: Normal Form)의 개념과 1NF, 2NF, 3NF, BCNF의 차이를 설명하시오.
- 인덱스(Index)의 개념과 B-Tree 인덱스와 Hash 인덱스의 차이점을 설명하시오.
- 데이터베이스 트랜잭션(Transaction)에서 고립성(Isolation)의 중요성과 격리 수준(Isolation Level)을 설명하시오.
- 알고리즘 설계에서 "컴퓨팅 사고(Computational Thinking)"의 개념과 중요성을 설명하시오.
- 알고리즘의 정확성(Correctness)을 증명하는 방법을 설명하시오.
- 순환식(Recurrence Relation)의 개념과 마스터 정리(Master Theorem)의 활용 방법을 설명하시오.
- 다항 시간 알고리즘(Polynomial-Time Algorithm)과 비다항 시간 알고리즘(Non-Polynomial-Time Algorithm)의 차이를 설명하시오.
- 탐욕 알고리즘(Greedy Algorithm)이 최적해를 보장할 수 있는 조건을 설명하시오.
- 셸 정렬(Shell Sort)의 개념과 시간 복잡도를 설명하시오.
- 팀 정렬(Timsort)의 원리와 기존 정렬 알고리즘과의 차이를 설명하시오.
- 비교 기반 정렬(Comparison-Based Sort)과 비비교 기반 정렬(Non-Comparison-Based Sort)의 차이를 설명하시오.
- 특정한 상황(거의 정렬된 데이터, 랜덤 데이터)에서 최적의 정렬 알고리즘을 선택하는 기준을 설명하시오.
- 메모리 제약이 있는 환경에서 적합한 정렬 알고리즘을 선택하는 방법을 설명하시오.
- 점프 탐색(Jump Search)의 개념과 활용 방안을 설명하시오.
- 보간 탐색(Interpolation Search)의 개념과 이진 탐색(Binary Search)과의 차이점을 설명하시오.
- 피보나치 탐색(Fibonacci Search)의 개념과 이진 탐색과의 차이점을 설명하시오.
- 기수 트리(Radix Tree, Patricia Tree)의 개념과 활용 사례를 설명하시오.
- B-트리(B-Tree)와 B+트리(B+ Tree)의 차이점을 설명하시오.
- 트리(Tree)와 그래프(Graph)의 차이를 설명하시오.
- 오일러 경로(Eulerian Path)와 해밀턴 경로(Hamiltonian Path)의 차이점을 설명하시오.
- 그래프에서 두 노드 간 최단 경로 문제를 해결하는 다양한 알고리즘(Dijkstra, Bellman-Ford, Floyd-Warshall 등)의 비교를 설명하시오.
- A* (A-Star) 알고리즘에서 휴리스틱(Heuristic) 함수가 중요한 이유를 설명하시오.
- 네트워크 플로우(Network Flow) 문제에서 이분 매칭(Bipartite Matching)을 해결하는 방법을 설명하시오.
- 보이어-무어(Boyer-Moore) 알고리즘이 KMP(Knuth-Morris-Pratt) 알고리즘보다 빠른 경우를 설명하시오.
- 라빈-카프(Rabin-Karp) 알고리즘이 사용하는 해싱(Hashing) 기법을 설명하시오.
- 롤링 해시(Rolling Hash)와 해밍 거리(Hamming Distance)의 개념을 설명하시오.
- 접미사 배열(Suffix Array)과 접미사 트리(Suffix Tree)의 차이점을 설명하시오.
- LCS(Longest Common Subsequence)와 LCS(Longest Common Substring)의 차이점을 설명하시오.
- 동적 계획법에서 "중복 계산 방지"를 위한 메모이제이션(Memoization)과 탑다운(Top-Down) 방식의 차이를 설명하시오.
- 동적 계획법과 그리디 알고리즘의 차이를 설명하고, 각각을 적용할 수 있는 대표적인 문제를 설명하시오.
- 행렬 체인 곱셈(Matrix Chain Multiplication) 알고리즘을 설명하시오.
- 배낭 문제(Knapsack Problem)에서 0-1 Knapsack과 Fractional Knapsack의 차이를 설명하시오.
- Floyd-Warshall 알고리즘을 동적 계획법으로 해결하는 과정과 시간 복잡도를 설명하시오.
- 탐욕 알고리즘(Greedy Algorithm)이 NP-완전 문제에서 최적해를 보장할 수 있는 경우를 설명하시오.
- 근사 알고리즘(Approximation Algorithm)의 개념과 대표적인 예제(TSP, Vertex Cover 문제)를 설명하시오.
- 유전 알고리즘(Genetic Algorithm)의 개념과 최적화 문제에서의 활용 사례를 설명하시오.
- 시뮬레이티드 어닐링(Simulated Annealing) 알고리즘이 지역 최적해(Local Optimum) 문제를 해결하는 방법을 설명하시오.
- 파티클 스웜 최적화(Particle Swarm Optimization, PSO)의 개념과 활용 사례를 설명하시오.
- 볼록 껍질(Convex Hull) 알고리즘의 개념과 활용 사례를 설명하시오.
- 최근접 점 쌍(Closest Pair of Points) 문제를 해결하는 방법을 설명하시오.
- 회전하는 캘리퍼스(Rotating Calipers) 기법을 설명하고 활용 가능한 문제를 설명하시오.
- 포인트 인 폴리곤(Point in Polygon) 문제의 해결 방법을 설명하시오.
- 라인 세그먼트 교차(Line Segment Intersection) 문제를 해결하는 알고리즘을 설명하시오.
- 트랜스포머(Transformer) 모델의 구조와 기존 RNN, CNN과의 차이를 설명하시오.
- 딥러닝에서 배치 정규화(Batch Normalization)의 원리와 효과를 설명하시오.
- 강화 학습에서 DQN(Deep Q-Network)과 PPO(Proximal Policy Optimization)의 차이점을 설명하시오.
- GAN(Generative Adversarial Network)에서 생성자(Generator)와 판별자(Discriminator)의 역할을 설명하시오.
- 그래프 신경망(GNN, Graph Neural Network)의 개념과 활용 사례를 설명하시오.
- 양자 알고리즘에서 Shor’s Algorithm이 기존 RSA 암호화를 위협하는 이유를 설명하시오.
- 양자 내성 암호(Post-Quantum Cryptography)의 개념과 필요성을 설명하시오.
- 블록체인의 해시 알고리즘(Hash Algorithm)이 보안성을 보장하는 원리를 설명하시오.
- 동형 암호(Homomorphic Encryption)의 개념과 활용 사례를 설명하시오.
- Zero-Knowledge Proof(영지식 증명)의 개념과 보안 응용 사례를 설명하시오.
- 확률과 통계를 활용한 알고리즘 최적화 기법을 설명하시오.
- 랜덤화 알고리즘(Randomized Algorithm)의 개념과 활용 사례를 설명하시오.
- 모듈러 연산(Modular Arithmetic)의 개념과 RSA 암호화에서의 활용을 설명하시오.
- 밀러-라빈(Miller-Rabin) 소수 판별 알고리즘을 설명하시오.
- 페르마 소정리(Fermat’s Little Theorem)와 소수 판별에서의 활용을 설명하시오.
- 스패닝 트리(Spanning Tree)와 최소 신장 트리(MST)의 차이를 설명하시오.
- 다이나믹 그래프(Dynamic Graph)의 개념과 업데이트 시 최단 경로를 유지하는 방법을 설명하시오.
- 전방 탐색(Forward Search)과 역방향 탐색(Backward Search)의 차이를 설명하시오.
- 이진 인덱스 트리(Binary Indexed Tree, Fenwick Tree)의 개념과 활용 사례를 설명하시오.
- Heavy-Light Decomposition을 활용한 트리 쿼리 최적화 기법을 설명하시오.
- 그래프 색칠 문제(Graph Coloring Problem)와 그 활용 사례를 설명하시오.
- 평면 그래프에서 4색 정리(Four Color Theorem)의 개념과 증명을 설명하시오.
- Vertex Cover 문제의 정의와 근사 알고리즘을 설명하시오.
- 클러스터링(Coarsening) 기법을 이용한 그래프 압축 방법을 설명하시오.
- 그래프 컷(Graph Cut) 알고리즘과 이미지 분할에서의 활용을 설명하시오.
- DP에서 상태 압축(State Compression) 기법을 설명하시오.
- 분할 정복(Divide and Conquer)과 동적 계획법(Dynamic Programming)의 차이를 설명하시오.
- DP 최적화 기법 중 Convex Hull Trick을 설명하시오.
- DP 최적화 기법 중 Knuth Optimization을 설명하시오.
- 가변 길이 배열을 다루는 DP 문제 해결 방법을 설명하시오.
- 네트워크 플로우(Network Flow)에서 최소 컷-최대 유량 정리를 설명하시오.
- 헝가리안 알고리즘(Hungarian Algorithm)의 개념과 이분 매칭(Bipartite Matching)에서의 활용을 설명하시오.
- Push-Relabel Algorithm을 활용한 최대 유량 문제 해결 방법을 설명하시오.
- Edmonds-Karp Algorithm을 활용한 네트워크 유량 최적화 방법을 설명하시오.
- 네트워크 플로우를 활용한 다중 경로 라우팅 최적화 기법을 설명하시오.
- 강화 학습에서 MCTS(Monte Carlo Tree Search)의 개념과 활용을 설명하시오.
- 그래프 신경망(GNN, Graph Neural Network)의 개념과 활용 사례를 설명하시오.
- 최신 딥러닝 모델에서 Transformer의 성능을 최적화하는 알고리즘을 설명하시오.
- 하이퍼 파라미터 튜닝(Hyperparameter Tuning)에서 Bayesian Optimization의 개념과 활용을 설명하시오.
- 연합 학습(Federated Learning)의 개념과 보안 강화 기법을 설명하시오.
- 확률 알고리즘(Probabilistic Algorithm)의 개념과 주요 활용 사례를 설명하시오.
- 수학적 최적화 문제에서 라그랑주 승수법(Lagrange Multiplier)의 개념을 설명하시오.
- 베이즈 정리(Bayes' Theorem)를 활용한 알고리즘 최적화 방법을 설명하시오.
- 몬테카를로 방법(Monte Carlo Method)과 의사 난수 발생(Pseudo Random Number Generation)의 차이를 설명하시오.
- KMP 알고리즘에서 실패 함수(Failure Function)의 개념과 역할을 설명하시오.
- 라빈-카프 알고리즘에서 해싱 충돌(Hash Collision) 문제를 해결하는 방법을 설명하시오.
- 문자열 집합에서 유사 문자열 검색을 최적화하는 알고리즘을 설명하시오.
- Levenshtein Distance(편집 거리)의 개념과 문자열 비교에서의 활용을 설명하시오.
- 네트워크 플로우 문제에서 다중 소스-다중 싱크(Multi-Source, Multi-Sink) 문제를 해결하는 방법을 설명하시오.
- 최소 비용 유량(Minimum Cost Flow) 문제를 해결하는 알고리즘을 설명하시오.
- 강화 학습에서 정책 그래디언트(Policy Gradient) 알고리즘의 개념과 활용을 설명하시오.
- 최신 딥러닝 모델에서 Transformer를 기반으로 한 최적화 알고리즘을 설명하시오.
- 생성형 AI(Generative AI)에서 Diffusion Model이 사용되는 원리를 설명하시오.
- 자율 주행에서 강화 학습을 활용한 의사결정 알고리즘을 설명하시오.
- 최근 연구에서 가장 주목받는 알고리즘 최적화 기법을 설명하시오.
- 고급 암호화 알고리즘(예: 동형 암호, 격자 기반 암호)의 개념과 활용을 설명하시오.
- 알고리즘 공학(Algorithm Engineering)이란 무엇이며, 기존 알고리즘 연구와의 차이점을 설명하시오.
- 최근 빅데이터 알고리즘 연구에서 가장 중요한 이슈는 무엇인가?
- 양자 알고리즘(Quantum Algorithm) 중 Grover's Algorithm이 데이터 검색에서 제공하는 속도 향상 효과를 설명하시오.