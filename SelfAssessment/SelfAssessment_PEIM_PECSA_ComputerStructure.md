# Concepts, Features, Types and Pros and Cons

Organize concepts, features, types and Pros and Cons

## Computer Structure

- 폰 노이만(John von Neumann) 구조와 특징 설명
    - 정의
        - 폰 노이만 구조(Von Neumann Architecture)는 컴퓨터의 기본적인 구조를 정의한 개념
        - 1945년 존 폰 노이만(John von Neumann)이 제안한 컴퓨터 설계 방식
        - 프로그램 내장 방식(stored-program concept)을 기반으로 하며, 현재 대부분의 컴퓨터가 이 구조 유지 중
    - 폰 노이만 구조의 주요 특징
        - 프로그램 내장 방식 (Stored-Program Concept)
            - 프로그램(명령어)과 데이터를 동일한 메모리(RAM)에 저장하고, 필요할 때 CPU가 이를 가져와 실행하는 방식
            - 과거의 컴퓨터(예: 초기 기계식 컴퓨터)는 하드웨어 배선을 변경해야 명령어를 바꿀 수 있었지만, 폰 노이만 구조에서는 소프트웨어적으로 프로그램을 변경 가능
        - 하나의 메모리를 명령어와 데이터가 공유
            - 프로그램의 명령어와 데이터가 같은 메모리 공간에 저장
            - CPU는 이를 순차적으로 가져와(fetch) 실행
        - 중앙처리장치(CPU)가 연산 수행
            - CPU는 명령어를 실행하는 중심 역할을 하며, 다음과 같은 구성 요소로 이루어짐
                - 연산장치(ALU, Arithmetic Logic Unit): 덧셈, 뺄셈, 논리 연산 등 수행
                - 제어장치(CU, Control Unit): 명령어를 해석하고 실행 순서를 제어
                - 레지스터(Register): 연산에 필요한 임시 데이터를 저장
        - 순차적 실행 방식
            - 프로그램 명령어는 순차적으로(fetch-decode-execute cycle) 실행
            - 현대 컴퓨터는 성능 향상을 위해 파이프라이닝, 캐시 메모리, 분기 예측 등을 활용하여 속도를 높이고 있음
        - 메모리 병목 현상(Von Neumann Bottleneck)
            - CPU와 메모리가 단일 버스를 공유하기 때문에, CPU가 처리할 데이터보다 메모리에서 데이터를 가져오는 속도가 느려지는 현상 발생 가능성 있음
            - 해결하기 위해 캐시 메모리(Cache), 다중 코어 CPU, 병렬 처리 등의 기술 등장
    - 폰 노이만 구조 / 하버드 구조 비교
        - 메모리 분리 여부
          - 명령어와 데이터가 같은 메모리 사용
          - 명령어와 데이터를 별도 메모리에 저장
        - 버스(Bus) 구조
          - 명령어와 데이터 전송을 같은 버스로 처리
          - 명령어와 데이터 전송을 개별 버스로 처리
        - 장점
          - 설계가 단순하고 범용성이 높음
          - 메모리 병목 현상이 적고 성능이 높음
        - 단점
          - 메모리 병목 문제 발생 가능
          - 하드웨어 설계가 복잡하고 비용 증가
        - 적용 사례
          - 대부분의 범용 컴퓨터(PC, 서버)
          - 임베디드 시스템, DSP(Digital Signal Processor)
    - 결론
      - 현대 컴퓨터의 기초를 제공
      - 대부분의 범용 컴퓨터가 폰 노이만 구조를 기반으로 설계됨
        - 프로그램 내장 방식으로 소프트웨어 변경 가능
      - 하드웨어 배선 변경 없이 소프트웨어적으로 기능을 수정할 수 있음
        - 컴퓨터 아키텍처 발전의 기틀 제공
      - 이후 하버드 구조, 캐시 메모리, 다중 코어 CPU 등 성능 개선 기술이 등장하는 기반이 됨
      - 재 정리
        - 폰 노이만 구조는 프로그램 내장 방식을 기반으로 하며, CPU, 메모리, 단일 버스를 활용하여 순차적으로 명령을 실행하는 컴퓨터 구조
        - 하지만 메모리 병목 문제가 발생할 수 있어, 현대 컴퓨터에서는 캐시 메모리, 병렬 처리 등의 기술로 이를 보완하고 있음

- 하버드 아키텍처(Harvard Architecture)와 폰 노이만 구조의 차이점
    - 폰 노이만 구조 / 하버드 구조 비교
        - 메모리 분리 여부
          - 폰 노이만: 명령어와 데이터가 같은 메모리 사용
          - 하버드 구조: 명령어와 데이터를 별도 메모리에 저장
        - 버스(Bus) 구조
          - 폰 노이만: 명령어와 데이터 전송을 같은 버스로 처리
          - 하버드 구조: 명령어와 데이터 전송을 개별 버스로 처리
        - 장점
          - 폰 노이만: 설계가 단순하고 범용성이 높음
          - 하버드 구조: 메모리 병목 현상이 적고 성능이 높음
        - 단점
          - 폰 노이만: 메모리 병목 문제 발생 가능
          - 하버드 구조: 하드웨어 설계가 복잡하고 비용 증가
        - 적용 사례
          - 폰 노이만: 대부분의 범용 컴퓨터(PC, 서버)
          - 하버드 구조: 임베디드 시스템, DSP(Digital Signal Processor)

- CISC와 RISC의 개념과 차이점
    - CISC
        - Complex Instruction Set Computer
        - CISC(복잡한 명령어 집합 컴퓨터, Complex Instruction Set Computer)는 하나의 명령어로 복잡한 연산을 수행할 수 있도록 설계된 프로세서 아키텍처
        - 명령어 개수가 많고, 하나의 명령어가 여러 개의 연산을 수행할 수 있음.
        - 어셈블리 코드가 간결해지고, 프로그래밍이 쉬워짐.
        - 특징
	        - 명령어 개수 多, 명령어 길이 가변적
	        - 하나의 명령어로 여러 연산 수행 가능 → 고급 언어와 유사한 기능 지원
	        - 메모리 접근 방식이 다양함 → 다양한 주소 지정 방식 지원
	        - 마이크로코드(Microcode) 사용 → 하드웨어가 복잡해짐
	        - CPU 설계가 복잡하지만, 코드 크기를 줄일 수 있음
        - 대표적인 CISC 프로세서
	        - Intel x86 (Pentium, Core i7, i9 등)
	        - AMD Ryzen
    - RISC
        - Reduced Instruction Set Computer
        - RISC(축소 명령어 집합 컴퓨터, Reduced Instruction Set Computer)는 명령어를 단순화하여 빠르게 실행할 수 있도록 설계된 프로세서 아키텍처
        - 단순한 명령어를 빠르게 실행하는 방식으로, 하드웨어가 단순하고 최적화가 쉬움.
        - 명령어가 동일한 크기로 고정되어 있어 파이프라이닝(Pipelining) 성능이 우수함.
        - 특징
	        - 명령어 개수 少, 명령어 길이 고정적
	        - 하나의 명령어는 한 가지 연산만 수행 (LOAD/STORE 방식)
	        - 메모리 접근 제한 (메모리 연산은 LOAD/STORE 명령어로만 가능)
	        - 하드웨어 설계가 단순하고, 실행 속도가 빠름
	        - CPU 내부에서 명령어 실행이 균일하여 병렬 처리(Pipelining) 최적화 가능
        - 대표적인 RISC 프로세서
	        - ARM (스마트폰, 태블릿, 임베디드 시스템 등 대부분의 모바일 CPU)
	        - Apple M1/M2, Qualcomm Snapdragon, Samsung Exynos
	        - IBM PowerPC, RISC-V, MIPS
    - 핵심 차이점 정리
        - CISC는 명령어가 강력하여 코드가 짧지만, 실행 속도가 상대적으로 느림.
        - RISC는 단순한 명령어를 빠르게 실행하여 성능이 뛰어나며, 저전력 설계에 적합함
    - CISC vs RISC 실제 적용 예시
        - CISC (Intel x86)
            - 데스크톱, 서버, 고성능 컴퓨팅에 사용
            - 명령어 하나로 여러 연산을 수행
            - Windows PC, 고성능 워크스테이션, 서버에서 주로 사용
            - 예제 (Intel x86 어셈블리)
            - 한 번의 명령어로 메모리에서 값을 가져오고 연산 가능
            - CISC 구조에서는 복잡한 연산을 적은 명령어로 표현 가능
        - RISC (ARM, Apple M1)
            - 모바일, 임베디드, 저전력 환경에 사용
            - 단순한 명령어를 빠르게 실행하여 전력 효율성이 높음
            - 스마트폰, 태블릿, IoT, 클라우드 서버에서 사용
            - 예제 (ARM 어셈블리)
            - LOAD/STORE 방식을 사용하여 메모리 접근 제한
            - 파이프라이닝이 용이하여 실행 속도가 빠름
    - 최신 트렌드: RISC 기반의 시장 확대
        - 최근 RISC 아키텍처(특히 ARM)가 점점 시장을 확장하고 있음
            - Apple M1, M2 칩 → MacBook에서 ARM 기반으로 전환
            - Qualcomm Snapdragon → 모바일 & 노트북 시장 확대
            - AWS Graviton → 클라우드 서버에서도 ARM 기반 RISC CPU 사용 증가
        - 이유
            - RISC는 전력 효율이 높아 모바일, 클라우드, 서버 환경에 적합
            - CISC(Intel, AMD)는 고성능 컴퓨팅에 강하지만, 전력 소모가 많음
            - ARM 기반 CPU가 계속 발전하면서 서버 및 데스크톱 시장에서도 사용 증가
    - 결론
        - CISC는 복잡한 명령어 집합을 제공하여 코드 크기를 줄일 수 있지만, 실행 속도가 상대적으로 느림.
        - RISC는 단순한 명령어를 빠르게 실행하여 성능이 뛰어나고, 전력 효율이 높음.
        - Intel, AMD CPU는 CISC 기반, ARM 및 Apple M1/M2, Qualcomm은 RISC 기반.
        - 최근에는 RISC 기반 프로세서가 성능을 향상시키면서 시장 점유율이 증가하는 추세.
        - 전통적인 고성능 컴퓨팅(CISC) vs 저전력, 고효율(RISC)의 차이가 있으며, 최근에는 RISC 기반 CPU가 데스크톱/서버 시장에서도 성장 중

- 컴퓨터의 기본 구성 요소(입력, 출력, 기억장치, 연산장치, 제어장치)에 대해 설명
    - 개요
        - 컴퓨터의 기본 구성 요소
            - 입력 장치(Input Device), 출력 장치(Output Device), 기억 장치(Memory), 연산 장치(ALU, Arithmetic Logic Unit), 제어 장치(Control Unit)
    - 기본 구성 요소들
        - 입력 장치(Input Device)
            - 사용자가 데이터를 컴퓨터에 입력할 수 있도록 하는 장치
            - 키보드, 마우스, 터치스크린, 마이크, 스캐너, 카메라 등이 포함
            - 입력된 데이터는 기억 장치에 저장되거나 연산 장치에서 처리됨
        - 출력 장치(Output Device)
            - 컴퓨터가 처리한 데이터를 사용자에게 전달하는 장치
            - 모니터, 프린터, 스피커, 프로젝터 등이 포함
        - 기억 장치(Memory)
            - 데이터를 저장하고 유지하는 역할을 하는 장치
            - 주기억 장치(메인 메모리, RAM) 와 보조 기억 장치(스토리지) 로 나뉨
                - 주기억 장치 (Primary Memory, RAM)
                    - RAM(Random Access Memory): 실행 중인 프로그램과 데이터를 저장하는 임시 저장 공간
                    - ROM(Read-Only Memory): 부팅 시 기본적인 시스템 정보를 저장하는 읽기 전용 메모리
                - 보조 기억 장치 (Secondary Storage)
                    - 데이터를 영구적으로 저장하는 장치
                    - 하드 디스크(HDD), 솔리드 스테이트 드라이브(SSD), USB, CD/DVD 등이 포함
            - 예시
                - RAM: 프로그램 실행 속도와 멀티태스킹을 담당
                - HDD(하드 디스크 드라이브): 대용량 데이터를 저장하지만 속도가 느림
                - SSD(솔리드 스테이트 드라이브): HDD보다 빠른 저장 장치
                - USB 메모리: 휴대용 저장 장치
                - 클라우드 스토리지: 인터넷을 통해 데이터를 저장하는 공간 (예: Google Drive, Dropbox)
        - 연산 장치(ALU, Arithmetic Logic Unit)
            - 컴퓨터에서 모든 연산(산술 및 논리 연산)을 담당하는 핵심 부품
            - CPU(중앙처리장치) 내에 존재하며, 덧셈, 뺄셈, 곱셈, 나눗셈 같은 산술 연산과 AND, OR, NOT 같은 논리 연산을 수행
        - 제어 장치(Control Unit)
            - 컴퓨터의 모든 구성 요소를 조정하고 제어하는 역할을 하는 장치
            - 입력 → 연산 → 기억 → 출력 순서대로 명령을 처리하도록 지시
            - CPU의 일부이며, 프로그램 명령을 해석하고 실행
    - 컴퓨터의 동작 과정
        - 입력장치 통해 데이터 입력 받음
        - 제어 장치가 명령을 해석하고 실행 순서를 결정
        - 연산 장치가 계산 / 논리 연산 수행
        - 결과가 기억 장치에 저장
        - 최종 결과가 출력 장치를 통해 사용자에게 제공

- CPU의 기본 동작 사이클(Fetch-Decode-Execute)
    - 개요
        - CPU는 프로그램을 실행할 때 Fetch-Decode-Execute라는 기본적인 동작 사이클 수행
        - CPU가 명령어를 가져오고, 해석하고, 실행하는 과정
    - Fetch (명령어 가져오기)
        - CPU는 프로그램 카운터(PC)에 저장된 주소에서 다음에 실행할 명령어를 가져옴
        - 명령어는 메모리(RAM)에서 명령어 레지스터(IR, Instruction Register)로 로드
        - 프로그램 카운터의 값을 증가시켜 다음 명령어를 가리키도록 설정
        - 주요 역할
            - 명렁어를 메모리에서 읽어옴
            - PC(Program Counter)를 증가
    - Decode (명령어 해석)
        - CPU의 명령어 해독기(Instruction Decoder)가 명령어를 분석하여 어떤 연산을 수행할 지 해석
        - 명령어가 필요한 데이터(레지스터 또는 메모리)를 확인하고 연산을 수행할 장치를 결정
        - ALU(산술 논리 연산 장치) 또는 제어 장치로 필요한 정보를 전달
        - 주요 역할
            - 명령어를 분석, 필요한 연산 결정
            - 사용될 레지스터 또는 메모리를 확인
    - Execute (명령어 실행)
        - 명령어에 따라 실제 연산을 수행
            - 산술 연산(+,-,*,/) -> ALU 사용
            - 데이터 이동(메모리 <-> 레지스터) -> 레지스터 연산
            - 조건 분기(점프) -> 프로그램 카운터 변경
        - 연산 결과는 메모리 또는 레지스터에 저장
        - 필요한 경우, CPU는 인터럽트를 처리하거나 결과를 출력 장치로 보낼 수 있음
        - 주요 역할
            - 명령어를 실행하고 결과를 저장
            - 메모리, 레지스터, ALU를 이용하여 연산 수행
    - 반복되는 사이클
        - CPU가 프로그램을 실행하는 동안 지속적으로 반복
            - Fetch -> Decode -> Execute
            - 프로그램이 끝날 때까지 반복(PC가 종료 명령어를 가리킬 때 종료)
    - 예제 (CPU가 실행하는 과정)
        - 예를 들어 A = B + C 연산 수행될 때
            - Fetch: 메모리에서 A = B + C 명령어를 가져옴
            - Decode: A에 B + C를 저장하는 명령어임을 해석
            - Execute: B와 C 값을 ALU에서 더하고, 결과를 A에 저장
    - 추가 개념
        - 파이프라이닝 (Pipelining): Fetch, Decode, Execute 단계를 동시에 수행하여 CPU 성능을 향상
        - 캐시 메모리: Fetch 단계에서 캐시 메모리를 활용하여 명령어 접근 속도를 증가
        - 인터럽트(Interrupt): 실행 도중 이벤트가 발생하면 중단하고 특정 처리를 수행
    - 결론, 요약
        - Fetch: 메모리에서 명령어 읽기 (구성 요소: 프로그램 카운터(PC), 명령어 레지스터(IR))
        - Decode: 명령어 분석/해석 및 연산 준비 (구성 요소: 명령어 해독기(Decoder))
        - Execute: 연산 수행 및 결과 저장 (구성 요소: ALU, 레지스터, 메모리)

- 명령어 파이프라이닝(Instruction Pipelining)의 개념과 장단점
    - 개념
        - CPU에서 하나의 명령어를 처리하는 동안 다음 명령어를 동시에 실행하는 기술
        - 여러 개의 명령어를 중첩하여 실행함으로써 CPU의 성능을 향상시키는 기법
        - 비유
            - 전통적인 방식 > 빵 한개씩 완성한 후 다음 빵 만들기
            - 파이프라이닝 > 반죽, 굽기, 포장 단계 구분 후 여러 개의 빵을 동시에 제작
    - 동작 과정 (파이프라인의 단계)
        - 1단계: IF (Instruction Fetch), 명령어를 메모리에서 가져옴
        - 2단계: ID (Instruction Decode), 명령어를 해석하여 필요한 연산과 레지스터를 결정
        - 3단계: EX (Execute), 연산 수행(ALU 연산, 주소 계산 등)
        - 4단계: MEM (Memory Access), 메모리에서 데이터 읽기/쓰기 수행
        - 5단계: WB (Write Back), 연산 결과를 레지스터에 저장
        - 참고 사항
            - 1번째 명령어가 IF, 두번째는 ID, 세번째는 EX 단계에 있을 수 있음, 즉 여러개의 명렁어가 동시에 실행되면서 CPU 활용도 높임
    - 명령어 파이프라이닝의 장점
        - 성능 향상 (Throughput 증가)
            - 여러 개의 명령어를 동시에 처리하므로 전체적인 처리 속도가 증가함.
            - CPU가 한 번에 하나의 명령어만 처리하는 순차 실행 방식보다 빠름.
            - 예를 들어, 5단계 파이프라인이면 이론적으로 최대 5배의 성능 향상 가능.
        - CPU 활용도 증가
            - 파이프라인이 없으면 CPU의 일부 유닛(ALU, 메모리 등)이 한 번에 하나의 명령어만 실행하여 자원이 비효율적으로 사용됨.
            - 하지만 파이프라이닝을 사용하면 각 유닛이 연속적으로 동작하므로 CPU의 자원 활용도가 높아짐.
        - 프로그램 실행 시간 단축
            - 개별 명령어의 실행 시간은 변하지 않지만, 전체 프로그램의 실행 시간이 단축됨.
            - 여러 명령어가 동시에 실행되므로 CPU의 명령어 처리 속도(Instruction Throughput)가 증가
    - 명령어 파이프라이닝의 단점
        - 파이프라인 해저드 (Pipeline Hazards) 발생
            - 파이프라이닝 과정에서 여러 개의 명령어가 동시에 실행되므로, 서로 간섭할 수 있는 문제가 발생할 수 있음
            - 데이터 해저드 (Data Hazard)
                - 이전 명령어의 실행 결과가 다음 명령어에 필요할 때 발생
                - 해결법: 데이터 포워딩 (Data Forwarding), 파이프라인 버블 삽입 (Stall)
            - 제어 해저드 (Control Hazard)
                - 분기문(조건문, if, jmp)이 실행될 때 발생
                - 해결법: 분기 예측 (Branch Prediction), 지연 슬롯 (Delayed Branch)
            - 구조적 해저드 (Structural Hazard)
                - 하드웨어 자원이 충분하지 않아 발생 (예: 한 번에 여러 명령어가 메모리에 접근)
                - 해결법: 리소스 추가 (멀티포트 메모리 사용)
        - 복잡한 설계 (Implementation Complexity)
            - CPU 내부에서 각 단계가 동시에 실행되도록 설계해야 하므로 하드웨어 구현이 복잡함
            - 명령어 간의 동기화 문제가 발생할 수 있음
            - 따라서 단순한 마이크로컨트롤러에서는 사용하기 어려울 수 있음
        - 예외(Interrupt) 처리 어려움
            - 파이프라이닝 중간에 인터럽트(Interrupt)가 발생하면 모든 파이프라인을 중단해야 할 수도 있음
            - 특정 명령어가 실패하거나 예외가 발생하면 롤백(Rollback) 처리가 필요함
    - 결론
        - 파이프라이닝은 CPU의 성능을 크게 향상시키는 기술이지만, 해저드 문제를 해결해야 효율적으로 동작할 수 있음
        - 분기 예측(Branch Prediction), 데이터 포워딩(Data Forwarding) 등 다양한 기법과 함께 사용

- 슈퍼스칼라(Superscalar) 구조란 무엇이며, 장점과 단점
    - 개념
        - CPU에서 여러개의 명령어를 동시에 실행할 수 있도록 다중 실행 파이프라인을 제공하는 아키텍쳐
        - 단일 클럭 사이클(Clock Cycle) 내에서 여러 개의 명령어를 병렬로 실행할 수 있도록 설계된 CPU 구조
        - 비유
            - 기본적인 파이프라인 구조: 한 줄로 서서 음식 받는 줄
            - 슈퍼스칼라 구조: 여러 줄(멀티 카운터) 동시에 가능
    - 슈퍼스칼라의 동작 방식
        - 명령어 디코더(Instruction Decoder)를 통해 명령어를 분석하고, 여러 개의 실행 유닛(Execution Units)에 나눠서 실행
            - CPU가 여러 개의 실행 유닛(ALU, FPU 등)을 가지며, 가능한 경우 여러 개의 명령어를 동시에 실행
            - 명령어의 의존성(Dependencies)이 없는 경우에만 병렬 실행 가능
            - 현대적인 CPU는 명령어 발행(Instruction Issue), 명령어 재배열(Out-of-Order Execution), 분기 예측(Branch Prediction) 등을 활용하여 효율적으로 실행
    - 슈퍼스칼라와 파이프라이닝 차이
        - 파이프라이닝은 명령어 실행을 연속적으로 수행하지만, 슈퍼스칼라는 여러 개의 명령어를 병렬로 실행하여 더 빠르게 처리
    - 슈퍼스칼라의 장점
        - 성능 향상 (Throughput 증가)
            - 한 번에 여러 개의 명령어를 실행할 수 있기 때문에 처리 속도가 빨라짐
            - 단일 클럭 사이클에서 여러 개의 명령어를 동시에 완료할 수 있음
            - n개의 실행 유닛이 있다면, 최적의 경우 n배의 성능 향상이 가능
        - 효율적인 CPU 활용
            - CPU 내부의 ALU, FPU(부동소수점 연산 유닛), 로드/스토어 유닛 등의 자원을 더 효율적으로 사용할 수 있음
            - Out-of-Order Execution(명령어 재배열)과 Register Renaming(레지스터 리네이밍) 같은 기술을 활용하면 실행 효율을 높일 수 있음
        - 멀티스레드(Multithreading) 성능 향상
            - CPU의 여러 실행 유닛을 활용하여 멀티스레드 환경에서 성능이 향상됨
            - 고성능 서버, 게임 엔진, 데이터베이스 처리 등에 적합
        - 현대 CPU의 필수 기술
            - x86, ARM, RISC-V 등의 모든 최신 프로세서가 슈퍼스칼라 구조를 채택하고 있음
            - 예: Intel, AMD, Apple M 시리즈, ARM Cortex 등의 CPU
    - 슈퍼스칼라의 단점
        - 명령어 병렬성이 낮으면 효과 감소
            - 슈퍼스칼라는 독립적인 명령어를 동시에 실행할 때만 성능이 향상됨
            - 명령어 간 데이터 의존성이 높으면 병렬 실행이 어렵고 성능 향상이 제한됨
            - 예: 연속적인 a = b + c; d = a + e; 같은 코드에서는 병렬 실행이 어려움
                - a가 종속적 
        - 하드웨어 설계 복잡성 증가
            - 여러 개의 실행 유닛을 운영하기 위해 CPU 내부가 복잡해짐
            - 명령어를 분석하여 어떤 명령어를 동시에 실행할 수 있는지 결정하는 로직(Instruction Dispatch Logic)이 필요
            - 분기 예측(Branch Prediction) 실패 시 성능 저하
        - 전력 소모 증가
            - 여러 개의 실행 유닛이 동작하기 때문에 단순한 프로세서보다 전력 소비가 많음
            - 배터리 효율이 중요한 모바일 기기에서는 전력 최적화가 필요
        - 소프트웨어 최적화 필요
            - 일부 경우 컴파일러나 프로그래머가 명령어 병렬성을 높이는 코드 최적화가 필요
            - 예: 명령어 재배열(Instruction Reordering), 루프 펼치기(Loop Unrolling) 등을 통해 병렬성을 증가시켜야 함
    - 슈퍼스칼라 vs 슈퍼파이프라인(Superpipelining)
        - 슈퍼스칼라 (Superscalar) → 한 번에 여러 개의 명령어를 병렬 실행
        - 슈퍼파이프라인 (Superpipelining) → 파이프라인 단계를 더 세분화하여 클럭당 실행 속도를 증가
💡          - 최신 CPU는 둘 다 사용
                - Intel, AMD, ARM 프로세서는 슈퍼스칼라 + 슈퍼파이프라인을 결합하여 고성능을 달성
    - 결론
        - 슈퍼스칼라는 현대 CPU에서 필수적인 성능 향상 기법이지만, 명령어 병렬성을 확보해야 효과적
        - 최신 CPU는 슈퍼스칼라 + 파이프라이닝 + Out-of-Order Execution + 분기 예측을 조합하여 성능을 극대화

- 명령어 집합 구조(ISA, Instruction Set Architecture)란 무엇이며, 설계 시 고려해야 할 요소
    - 개념
        - 컴퓨터의 하드웨어와 소프트웨어 간의 인터페이스를 정의하는 명령어들의 집합
        - CPU가 실행할 수 있는 명령어들의 집합과 이들 명령어가 수행하는 동작을 규정하는 추상적인 명세
        - ISA는 주어진 프로세서가 지원하는 명령어 형식, 데이터 유형, 주소 지정 방식, 레지스터 구조 등을 포함
        - 동일한 ISA를 따르는 프로세서들은 같은 바이너리 명령어를 실행할 수 있으며, 이로 인해 소프트웨어의 이식성이 보장됨
    - ISA 설계 시 고려해야 할 요소
        - 개요
            - ISA를 설계할 때는 성능, 전력 소비, 구현 비용 등을 균형 있게 고려해여 함
        - 주요 설계 고려 요소
            - 명령어 형식(Instruction Format)
	            - 명령어의 길이(고정 길이 vs 가변 길이)
	            - 명령어 필드(Opcode, 피연산자 등)의 크기 및 배치
	            - 인코딩 방식 (CISC vs RISC)
                    - 예시
	                    - RISC(Reduced Instruction Set Computing): 고정된 길이의 명령어, 단순한 인코딩 방식
	                    - CISC(Complex Instruction Set Computing): 가변 길이 명령어, 복잡한 인코딩 방식
            - 주소 지정 방식(Addressing Modes)
                - CPU가 데이터를 메모리에서 읽거나 저장하는 방법을 정의하는 방식
	            - 즉시 주소 지정(Immediate Addressing): 명령어 내에 직접 값이 포함됨
	            - 레지스터 주소 지정(Register Addressing): 연산 대상이 레지스터에 저장됨
	            - 직접 주소 지정(Direct Addressing): 메모리 주소가 명령어에 직접 포함됨
	            - 간접 주소 지정(Indirect Addressing): 명령어가 가리키는 메모리 주소에 실제 주소가 저장됨
	            - 기타(Indexed, Base + Offset, PC-relative 등)
            - 데이터 유형(Data Types)
                - ISA가 지원하는 데이터 유형과 연산 방식을 정의
	                - 정수(Integer)
	                - 부동소수점(Floating Point)
	                - 벡터(Vector, SIMD)
	                - 기타 복합 데이터 유형(예: MMX, AVX 등)
            - 연산 유형(Operations)
                - CPU가 수행할 수 있는 기본 연산의 종류
                    - 산술 연산(ADD, SUB, MUL, DIV)
                    - 논리 연산(AND, OR, XOR, NOT)
                    - 비교 연산(CMP)
                    - 데이터 이동(Load/Store, Move)
                    - 흐름 제어(Jump, Branch, Call, Return)
                    - 특수 명령어(SIMD, 멀티스레딩 관련 명령어)
            - 레지스터 구조(Register Architecture)
                    - 일반 목적 레지스터(GPR, General Purpose Register) 개수
                        - 예: x86(8개), ARM(16개 이상), RISC-V(32개)
                    - 전용 레지스터(Special Purpose Register)
                        - 예: PC(Program Counter), SP(Stack Pointer), FP(Frame Pointer)
                    - 명령어 인코딩에서 레지스터의 위치
                    - RISC는 대부분 레지스터 간 연산을 기반으로 설계됨
            - 메모리 모델 및 관리(Memory Model & Management)
                - 메모리 정렬(Alignment)
                - 엔디언(Endianness, Little-endian vs Big-endian)
                - 메모리 보호 및 가상 메모리 지원 여부
                - 캐시 및 페이지 테이블 관리 방식
            - 입출력 방식(I/O Model)
                - 메모리 매핑 I/O (Memory-mapped I/O)
                - 포트 매핑 I/O (Port-mapped I/O)
                - 인터럽트 기반 I/O (Interrupt-driven I/O)
                - DMA(Direct Memory Access) 지원 여부
            - 명령어 실행 방식 및 성능 최적화
	            - 파이프라이닝(Pipelining): 명령어를 단계적으로 실행하여 성능 향상
	            - 슈퍼스칼라(Superscalar): 여러 명령어를 동시에 실행하는 구조
	            - VLIW(Very Long Instruction Word): 병렬 실행을 위해 긴 명령어를 사용하는 방식
	            - Out-of-Order Execution: 명령어를 순서에 관계없이 실행하여 성능 향상
	            - Branch Prediction: 분기 예측 기술로 성능 향상
            - 에너지 효율성 및 저전력 설계
	            - 모바일 및 IoT 기기에서는 저전력 소비가 필수
	            - 단순한 명령어와 연산 구조(RISC 계열 ISA) 활용
	            - 전력 소모를 줄이기 위한 명령어 집합 제공(예: ARM의 Thumb 모드)
            - 소프트웨어 및 호환성 고려
	            - 이전 세대 ISA와의 하위 호환성 (Backward Compatibility)
	            - 가상화 및 보안 기능 지원 (예: Intel VT-x, ARM TrustZone)
	            - 운영체제(OS) 및 컴파일러 지원 고려
    - 결론
        - ISA 설계는 성능, 전력 소비, 하드웨어 구현 비용, 소프트웨어 호환성 등의 요소를 균형 있게 고려해야 함
        - RISC와 CISC와 같은 철학적 차이뿐만 아니라, 데이터 유형, 메모리 모델, 주소 지정 방식, 파이프라이닝 등의 다양한 요소를 최적화하는 것이 중요
        - 최근의 ISA 설계는 병렬 처리, AI 가속, 저전력 소비 등의 특성을 반영하여 발전하고 있으며, 대표적인 ISA로는 x86, ARM, RISC-V 등이 존재


- 명령어 형식(Format)과 종류(Type)
    - 명령어 형식(Instruction Format)
        - 컴퓨터의 CPU가 해석하고 실행할 수 있도록 구성된 명령어의 구조를 의미
        - 명령어는 일반적으로 연산 코드(Opcode)와 피연산자(Operand)로 구성되며, 하드웨어 및 명령어 집합 아키텍처(ISA, Instruction Set Architecture)에 따라 다양한 형식이 존재

    - 명령어 형식의 기본 요소
	    - 연산 코드(Opcode):
	        - 수행할 연산을 지정하는 코드 (예: ADD, SUB, MOV 등)
	    - 피연산자(Operand):
	        - 연산에 사용될 데이터나 레지스터, 메모리 주소 등을 지정
	    - 주소(Addressing Mode, 선택적 요소):
	        - 데이터가 저장된 위치를 결정하는 방식
	    - 기타 제어 비트(Control Bits, 선택적 요소):
	        - 명령 실행을 위한 추가적인 정보 (예: 조건부 실행, 인터럽트 제어 등)

    - 명령어 형식(Format)의 종류
        - 고정 길이 명령어(Fixed-length Instruction)
	        - 명령어의 길이가 일정하게 고정된 구조
	        - 장점: 해석 속도가 빠르고, 하드웨어 설계가 단순함
	        - 단점: 메모리 사용 비효율 발생 가능
	        - 예: RISC 프로세서(ARM, MIPS), x86 32비트 명령어

        - 가변 길이 명령어(Variable-length Instruction)
	        - 명령어의 길이가 연산 종류와 피연산자의 수에 따라 달라지는 구조
	        - 장점: 메모리 공간을 효율적으로 사용 가능
	        - 단점: 명령어 해석이 복잡하고, 실행 속도가 느려질 수 있음
	        - 예: CISC 프로세서(x86, x86-64)

    - 명령어 종류(Instruction Type)
        - 데이터 전송 명령어(Data Transfer Instructions)
	        - 데이터를 이동하는 명령어 (레지스터 간, 메모리 간, I/O 장치 간 이동)
	        - 예시
	            - MOV R1, R2 → R2의 값을 R1으로 복사
	            - LOAD R1, [1000] → 메모리 주소 1000번지의 값을 R1으로 로드
	            - STORE R1, [1000] → R1의 값을 1000번지에 저장

        - 연산 명령어(Arithmetic and Logical Instructions)
            - 산술 및 논리 연산을 수행하는 명령어
            - 예시
                - ADD R1, R2, R3 → R2 + R3 결과를 R1에 저장
                - SUB R1, R2, R3 → R2 - R3 결과를 R1에 저장
                - AND R1, R2, R3 → R2와 R3의 AND 연산 결과를 R1에 저장
                - OR R1, R2, R3 → R2와 R3의 OR 연산 결과를 R1에 저장

        - 제어 명령어(Control Instructions)
            - 프로그램 실행 흐름을 변경하는 명령어 (분기, 점프, 조건문)
            - 예시
                - JMP 200 → 200번지로 무조건 점프
                - BEQ R1, R2, 100 → R1과 R2가 같으면 100번지로 점프
                - CALL func → 함수 호출
                - RET → 함수 종료 후 복귀

        - 입출력 명령어(Input/Output Instructions)
	        - 외부 장치와 데이터를 주고받는 명령어
	        - 예시
	            - IN R1, 300 → 300번 포트에서 데이터를 입력받아 R1에 저장
	            - OUT 300, R1 → R1의 값을 300번 포트로 출력

        - 시스템 명령어(System Instructions)
	        - 운영 체제와 관련된 명령어 (인터럽트, 특수 레지스터 제어)
	        - 예시
	            - INT 0x80 → 소프트웨어 인터럽트 호출 (리눅스 시스템 호출)
	            - HLT → CPU 동작 중지(프로그램 종료)

    - 결론
	    - 명령어 형식(Format): CPU가 처리하는 명령어의 구조로, 고정 길이 vs. 가변 길이가 있음.
	    - 명령어 종류(Type): 기능에 따라 데이터 전송, 연산, 제어, 입출력, 시스템 명령어로 나뉨.
	    - RISC(고정 길이) vs. CISC(가변 길이): CPU 구조에 따라 명령어 형식과 종류가 다름.

- 레지스터(Register)의 종류와 역할
    - 레지스터(Register) 정의
	    - CPU 내부에서 데이터 및 명령어를 임시로 저장하는 초고속 메모리
	    - 일반적인 RAM보다 훨씬 빠른 속도를 가지며, CPU가 직접 접근하여 연산을 수행하는 저장 공간
	    - 연산 과정에서 데이터 임시 저장, 명령어 실행, 주소 지정 등의 역할을 수행

    - 레지스터의 주요 종류와 역할
        - 범용 레지스터 (General-Purpose Register): GPR, 연산 및 데이터 저장을 위한 다목적 레지스터
        - 누산기 (Accumulator Register): AC, 연산 결과 저장 및 연산 수행
        - 메모리 주소 레지스터 (Memory Address Register): MAR, 메모리에서 읽거나 쓸 주소를 저장
        - 메모리 버퍼 레지스터 (Memory Buffer Register): MBR, 메모리에서 읽거나 쓴 데이터를 임시 저장
        - 명령어 레지스터 (Instruction Register): IR, 현재 실행 중인 명령어 저장
        - 프로그램 카운터 (Program Counter): PC, 다음 실행할 명령어의 주소 저장
        - 스택 포인터 (Stack Pointer): SP, 스택의 최상위 주소를 가리킴
        - 상태 레지스터 / 플래그 레지스터 (Status Register / Flag Register), SR / FR, 연산 결과에 대한 상태 정보 저장

    - 레지스터 상세 설명
        - (1) 범용 레지스터 (General-Purpose Register, GPR)
            - 개념
	            - CPU에서 임시 데이터 저장 및 연산 수행을 위한 다목적 레지스터
	            - 특정한 용도로 제한되지 않으며, 프로그래머가 자유롭게 사용 가능

            - 역할
	            - 덧셈, 뺄셈, 논리 연산 등의 중간 결과 저장
	            - CPU 내부에서 데이터 이동 및 연산 수행

            - 예시
	            - x86 아키텍처의 경우, EAX, EBX, ECX, EDX 등이 범용 레지스터로 사용됨
	            - ARM 아키텍처에서는 R0 ~ R15 레지스터가 범용 레지스터로 사용됨

        - (2) 누산기 (Accumulator Register, AC)
            - 개념
	            - 산술 연산(ALU 연산)의 결과를 저장하는 레지스터
	            - 과거에는 범용 레지스터가 부족하여 누산기를 별도로 사용
                - 현대 CPU에서는 범용 레지스터가 이 역할을 대신 수행하기도 함

            - 역할
	            - 연산(Arithmetic & Logical Unit, ALU) 수행 시 중간 결과 저장
	            - 빠른 연산을 위해 주로 사용

            - 예시
	            - 명령어 실행 예시:
                    - ADD A, B : A와 B를 더한 결과를 누산기에 저장

        - (3) 메모리 주소 레지스터 (Memory Address Register, MAR)
            - 개념 (주소 저장)
	            - 메모리에서 데이터를 읽거나 쓸 때 참조할 주소를 저장하는 레지스터
	            - CPU가 메모리와 통신할 때 사용됨

            - 역할
	            - 메모리 접근 주소 지정
	            - CPU가 특정 주소의 데이터를 가져오거나 저장할 때 주소값을 저장

            - 예시
	            - LOAD R1, (MAR)
	                - MAR에 저장된 주소에서 데이터를 읽어와 R1에 저장.

        - (4) 메모리 버퍼 레지스터 (Memory Buffer Register, MBR)
            - 개념
	            - 메모리에서 읽거나 저장할 데이터를 임시로 저장하는 레지스터
	            - MAR과 함께 동작하여 메모리와 CPU 간의 데이터 전송을 담당

            - 역할
	            - 메모리에서 읽어온 데이터 임시 저장
	            - 메모리에 쓸 데이터를 저장

            - 예시 (데이터 저장)
	            - MBR ← [MAR]
	                - MAR에 저장된 주소에서 데이터를 가져와 MBR에 저장.

        - (5) 명령어 레지스터 (Instruction Register, IR)
            - 개념
	            - 현재 실행 중인 명령어를 저장하는 레지스터
	            - CPU가 명령어를 해석(Decoding)하고 실행하는 과정에서 사용됨.

            - 역할
	            - CPU가 현재 실행할 명령어를 저장하고 해석
	            - 명령어 페치(Fetch) 후 실행

            - 예시
	            - 명령어 패치(Fetch): IR ← [PC]
	            - 명령어 해석(Decode): 명령어 분석 후 실행

        - (6) 프로그램 카운터 (Program Counter, PC)
            - 개념
	            - 다음 실행할 명령어의 주소를 저장하는 레지스터
	            - CPU가 실행할 명령어의 흐름을 제어하는 역할

            - 역할
	            - 명령어 실행 후, 자동으로 다음 명령어 주소 증가
	            - 분기문(Jump, Call) 실행 시, 특정 주소로 이동

            - 예시
	            - 일반적인 실행 흐름: PC ← PC + 1
	            - 분기 명령어 실행: PC ← 0x2000 (특정 주소로 점프)

        - (7) 스택 포인터 (Stack Pointer, SP)
            - 개념
	            - 스택(Stack) 구조의 최상위 주소를 가리키는 레지스터
	            - 함수 호출, 지역 변수 저장 등에 사용됨.

            - 역할
	            - 함수 호출 시 반환 주소 저장 (PUSH)
	            - 함수 종료 시 저장된 주소 복원 (POP)

            - 예시
                - PUSH R1 : 스택에 R1 값 저장 (SP 감소)
                - POP R1 : 스택에서 R1 값 복원 (SP 증가)

        - (8) 상태 레지스터 / 플래그 레지스터 (Status Register / Flag Register)
            - 개념
	            - 연산 수행 후의 상태를 나타내는 비트 플래그(Flags)를 저장하는 레지스터
	            - 연산 결과에 따라 Z(Zero), C(Carry), S(Sign), O(Overflow) 등의 상태 비트를 설정

            - 역할
	            - 연산 결과의 조건을 저장하여 프로그램 흐름 결정
	            - 분기(조건문) 실행 시 사용됨 (예: if문 역할)

            - 플래그 비트 예시
                - Z (Zero Flag): 연산 결과가 0이면 설정
                - C (Carry Flag): 덧셈 시 자리올림 발생 시 설정
                - S (Sign Flag): 연산 결과가 음수이면 설정
                - O (Overflow Flag): 오버플로우 발생 시 설정

            - 예시
                - CMP R1, R2 : R1과 R2 비교
                - JZ LABEL : Zero 플래그(ZF)가 설정되면 LABEL로 점프

    - 레지스터 종류별 역할 요약
        - 범용 레지스터: GPR, 데이터 저장 및 연산 수행
        - 누산기: AC, 연산 결과 저장
        - 메모리 주소 레지스터:	MAR, 메모리 접근 주소 저장
        - 메모리 버퍼 레지스터:	MBR, 메모리에서 읽거나 쓸 데이터 저장
        - 명령어 레지스터: IR, 현재 실행 중인 명령어 저장
        - 프로그램 카운터: PC, 다음 실행할 명령어 주소 저장
        - 스택 포인터: SP, 스택 최상위 주소 관리
        - 상태/플래그 레지스터: SR / FR, 연산 결과 상태 저장 (Zero, Carry, Overflow 등)

    - 결론
	    - 레지스터는 CPU 내부에서 가장 빠른 메모리이며, 연산 및 데이터 저장을 담당
	    - 각 레지스터는 특정 역할을 가지며, CPU가 효율적으로 작동할 수 있도록 지원
	    - 범용(GPR)과 특수 목적(SP, PC, IR 등) 레지스터가 함께 동작하여 명령어 실행과 데이터 처리를 수행함

- 산술논리연산장치(ALU)의 역할과 기능은?
- CPU 내부의 주요 레지스터(PC, IR, MAR, MDR, ACC 등)의 역할을 설명하라.
- 정수 연산과 부동소수점 연산의 차이점과 처리 방식은?
- IEEE 754 부동소수점 표준의 구조와 특징은?
- Overflow와 Underflow의 개념과 발생 원인은?
- Carry, Borrow, Overflow, Zero 플래그의 역할은?


- CPU의 클럭 속도(Clock Speed)와 CPI(Cycles Per Instruction)의 관계를 설명하라.
- 연산 속도를 높이기 위한 주요 기술(파이프라이닝, 슈퍼스칼라, VLIW 등)을 설명하라.
- RISC에서 Load/Store 아키텍처의 의미와 장점은?
- VLIW(Very Long Instruction Word) 구조의 개념과 장점은?
- 캐시 메모리(Cache Memory)의 역할과 동작 원리는?
- 캐시 메모리의 매핑 방식(직접 사상, 연관 사상, 집합 연관 사상)의 차이점은?


- 캐시 히트(Cache Hit)와 캐시 미스(Cache Miss)의 개념과 영향을 설명하라.
- 캐시 메모리에서 미스 패널티(Miss Penalty)란 무엇인가?
- 캐시 교체 알고리즘(FIFO, LRU, LFU 등)의 종류와 특징은?
- 캐시 일관성(Coherency) 문제와 이를 해결하는 방법은?
- 멀티레벨 캐시(L1, L2, L3)의 개념과 필요성은?
- 가상 메모리(Virtual Memory)의 개념과 동작 방식은?


- 페이지 테이블(Page Table)과 TLB(Translation Lookaside Buffer)의 역할은?
- MMU(Memory Management Unit)의 역할과 동작 방식은?
- DRAM과 SRAM의 차이점은?
- DRAM에서 Refresh가 필요한 이유는?
- EEPROM과 Flash Memory의 차이점은?
- HDD와 SSD의 구조적 차이와 장점, 단점은?


- RAID의 개념과 다양한 RAID 수준(RAID 0, 1, 5, 10 등)의 차이점은?
- 하드디스크의 주요 성능 지표(Access Time, Seek Time, Latency, RPM 등)를 설명하라.
- 페이징(Paging)과 세그먼테이션(Segmentation)의 차이점은?
- 페이지 폴트(Page Fault)란 무엇이며, 발생 시 처리 방식은?
- Thrashing(스래싱)의 개념과 발생 원인은?
- 메모리 단편화(내부 단편화, 외부 단편화)의 개념과 해결 방법은?


- 멀티코어 프로세서(Multi-Core Processor)의 개념과 장점은?
- SMT(Simultaneous Multithreading)와 하이퍼스레딩(Hyper-Threading)의 차이점은?
- 다중 프로세서 시스템(Symmetric vs Asymmetric Multiprocessing)의 차이점은?
- NUMA(Non-Uniform Memory Access) 구조란 무엇인가?
- GPGPU(General-Purpose GPU)의 개념과 활용 사례는?
- Flynn의 분류(SISD, SIMD, MISD, MIMD)에 대해 설명하라.


- 멀티스레딩(Multithreading)의 개념과 장점은?
- 병렬 프로세싱(Parallel Processing)에서 Amdahl’s Law란 무엇인가?
- 데이터 흐름 컴퓨팅(Data Flow Computing)이란 무엇인가?
- 분산 시스템(Distributed System)과 병렬 시스템(Parallel System)의 차이점은?
- 시스템 버스(System Bus)의 개념과 역할은?
- 버스 중재 방식(Arbitration)의 종류와 특징은?


- PCI Express(PCIe)와 기존 PCI의 차이점은?
- USB(Universal Serial Bus)의 특징과 동작 방식은?
- 직렬 전송(Serial Transmission)과 병렬 전송(Parallel Transmission)의 차이점은?
- DMA(Direct Memory Access)의 개념과 장점은?
- 인터럽트(Interrupt)의 개념과 종류(하드웨어, 소프트웨어 인터럽트)는?
- 인터럽트 벡터 테이블(Interrupt Vector Table)이란 무엇인가?


- 메모리 매핑 방식(Memory-mapped I/O vs. Port-mapped I/O)의 차이점은?
- I/O 방식(폴링, 인터럽트, DMA)의 차이점과 장점은?
- 양자 컴퓨팅(Quantum Computing)의 개념과 기존 컴퓨터와의 차이점은?
- 뉴로모픽 컴퓨팅(Neuromorphic Computing)이란 무엇인가?
- 엣지 컴퓨팅(Edge Computing)과 클라우드 컴퓨팅(Cloud Computing)의 차이점은?
- NVMe(Non-Volatile Memory Express)의 개념과 기존 SATA SSD와의 차이점은?


- HBM(High Bandwidth Memory)과 GDDR의 차이점은?
- 칩렛(Chiplet) 구조란 무엇이며, 기존 단일 다이(Die) 설계와의 차이점은?
- TPU(Tensor Processing Unit)와 GPU의 차이점은?
- AI 가속기(Neural Processing Unit, NPU)의 개념과 역할은?
- 사이드 채널 공격(Side Channel Attack)의 개념과 방어 기법은?
- 최신 CPU에서 보안 강화 기술(Meltdown, Spectre 방어 기법 등)은?


- Out-of-Order Execution(명령어 비순차 실행)의 개념과 장점은?
- 데이터 흐름 아키텍처(Data Flow Architecture)의 개념과 특징은?
- 브랜치 프레딕션(Branch Prediction)의 개념과 중요성은?
- 정적(Static) vs. 동적(Dynamic) 브랜치 예측 기법의 차이점은?
- 투기 실행(Speculative Execution)이란 무엇인가?
- VLIW(Very Long Instruction Word)와 EPIC(Explicitly Parallel Instruction Computing)의 차이점은?


- CPU의 성능을 측정하는 주요 벤치마크(Benchmark) 지표는?
- GPU와 CPU의 연산 처리 방식의 차이점은?
- SIMD(Single Instruction Multiple Data)와 MIMD(Multiple Instruction Multiple Data)의 차이점은?
- ILP(Instruction Level Parallelism)과 TLP(Thread Level Parallelism)의 차이점은?
- 캐시 프리페칭(Cache Prefetching)의 개념과 장점은?
- 캐시 코히어런시(Cache Coherency) 유지 기법에는 어떤 것이 있는가?
- MESI 프로토콜의 개념과 역할은?


- NUMA(Non-Uniform Memory Access)와 UMA(Uniform Memory Access)의 차이점은?
- 가상 메모리의 주소 변환(Address Translation) 과정에서 발생하는 TLB 미스(TLB Miss) 처리 방식은?
- 세그먼트(segment)와 페이지(page)의 차이점은?
- TLB(Translation Lookaside Buffer)의 동작 방식과 성능 향상 기법은?
- 캐시 메모리에서 쓰기 정책(Write Through, Write Back)의 차이점은?
- 페이지 교체 알고리즘(LRU, FIFO, LFU, Optimal)의 비교와 장단점은?


- Working Set Model(작업 집합 모델)의 개념과 역할은?
- Amdahl’s Law와 Gustafson’s Law의 차이점은?
- SIMD(벡터 프로세싱)와 MIMD(멀티프로세서)의 차이점은?
- 스레드 기반 병렬 처리(Thread-level Parallelism)와 데이터 기반 병렬 처리(Data-level Parallelism)의 차이점은?
- 다중 프로세서 시스템에서 캐시 일관성을 유지하는 방법은?
- 멀티코어 프로세서에서 공유 메모리 접근 시 동기화 문제를 해결하는 방법은?


- Spin Lock과 Mutex의 차이점은?
- OpenMP와 MPI(Message Passing Interface)의 차이점과 사용 사례는?
- Hyper-Threading의 개념과 장단점은?
- 멀티코어 프로세서에서 Thread Affinity(스레드 결속)의 개념과 장점은?
- NUMA 시스템에서 성능을 최적화하는 방법은?
- MIPS와 ARM 아키텍처의 주요 차이점은?


- x86과 ARM의 설계 철학과 차이점은?
- CISC와 RISC의 차이점 및 대표적인 프로세서 아키텍처는?
- ARM 프로세서의 Thumb 모드란 무엇인가?
- PowerPC, SPARC, MIPS 아키텍처의 주요 특징과 차이점은?
- Superscalar와 VLIW 아키텍처의 차이점은?
- 프로세서에서 미시적 명령어(Microinstruction)와 매크로 명령어(Macroinstruction)의 차이점은?


- Out-of-Order Execution과 In-Order Execution의 차이점은?
- SIMD 아키텍처에서 벡터 레지스터(Vector Register)의 역할은?
- 펜티엄(Pentium) 프로세서에서 MMX, SSE, AVX 명령어 세트의 차이점은?
- I/O 서브시스템의 개념과 역할은?
- DMA(Direct Memory Access)와 Programmed I/O의 차이점은?
- 폴링(Polling)과 인터럽트(Interrupt)의 차이점과 장단점은?


- 인터럽트 우선순위(Interrupt Priority)와 다중 인터럽트 처리 방식은?
- PCI와 PCIe(PCI Express)의 차이점은?
- USB의 데이터 전송 방식(Control, Bulk, Interrupt, Isochronous Transfer)의 차이점은?
- SCSI, SATA, NVMe의 차이점과 특징은?
- 하드디스크의 RAID 구성 방식과 장단점은?
- SSD에서 TRIM 명령어의 역할은?


- 캐시 메모리와 디스크 캐시의 차이점은?
- 멜트다운(Meltdown)과 스펙터(Spectre) 취약점의 개념과 대응 방식은?
- RISC-V의 특징과 기존 RISC 아키텍처와의 차이점은?
- AI/딥러닝을 위한 TPU(Tensor Processing Unit)와 GPU의 차이점은?
- 뉴로모픽 컴퓨팅(Neuromorphic Computing)의 개념과 특징은?
- 퀀텀 컴퓨팅(Quantum Computing)에서 큐비트(Qubit)의 역할은?


- FPGA(Field Programmable Gate Array)의 개념과 활용 사례는?
- 칩렛(Chiplet) 설계 방식과 기존 단일 다이(Die) 설계와의 차이점은?
- 비휘발성 메모리(NVM, Non-Volatile Memory)의 개념과 활용 사례는?
- HBM(High Bandwidth Memory)과 GDDR 메모리의 차이점은?
- 엣지 컴퓨팅(Edge Computing)과 클라우드 컴퓨팅(Cloud Computing)의 차이점은?
- MIPS(Million Instructions Per Second)와 FLOPS(Floating Point Operations Per Second)의 차이점은?


- SPEC Benchmark란 무엇이며, 어떤 목적으로 사용되는가?
- CPU 성능을 높이기 위한 캐시 최적화 방법은?
- 메모리 접근 속도를 향상시키기 위한 기법에는 어떤 것이 있는가?
- 벤치마킹(Benchmarking) 시 고려해야 할 주요 요소는?
- GPU 가속(Acceleration)을 활용한 컴퓨팅 성능 향상 기법은?
- Branch Prediction Miss를 줄이기 위한 최적화 방법은?



- 쓰레드 컨텍스트 스위칭(Context Switching)이 성능에 미치는 영향은?
- 파이프라인 구조에서 구조적 해저드(Structural Hazard), 데이터 해저드(Data Hazard), 제어 해저드(Control Hazard)를 해결하는 방법은?
- 멀티코어 시스템에서 성능 병목(Bottleneck)을 최소화하는 기법은?
- 명령어 파이프라인(Instruction Pipeline)에서 발생할 수 있는 주요 해저드는 무엇이며, 해결 방법은?
- 지연 슬롯(Delay Slot)의 개념과 효과는?
- 점프 예측(Jump Prediction)과 분기 예측(Branch Prediction)의 차이점은?


- 동적 브랜치 예측에서 Two-Level Adaptive Branch Prediction 기법이란?
- 명령어 재배치(Instruction Reordering)의 개념과 성능 향상 방법은?
- 레지스터 윈도우(Register Window)의 개념과 적용 사례는?
- 다중 발행(Multiple Issue) 프로세서에서 명령어 스케줄링(Instruction Scheduling) 방법은?
- 제어 흐름 그래프(Control Flow Graph, CFG)의 개념과 분석 방법은?
- 명령어 인코딩(Instruction Encoding) 방식과 성능 영향은?
- JIT(Just-In-Time) 컴파일러의 동작 원리와 컴퓨터 구조와의 관계는?


- 페이지 크기(Page Size)를 결정할 때 고려해야 할 요소는?
- 다중 프로세스 환경에서 TLB(Translation Lookaside Buffer) 관리 방법은?
- 다중 페이지 테이블 구조(Inverted Page Table, Multi-Level Page Table)의 장단점은?
- 가상 메모리에서 Demand Paging과 Prepaging의 차이점은?
- Page Coloring 기법의 개념과 성능 향상 효과는?
- 페이지 공유(Page Sharing)와 복사-온-라이트(Copy-On-Write)의 차이점은?
- 메모리 압축(Memory Compression) 기법의 원리와 효과는?


- Swap Space의 개념과 성능에 미치는 영향은?
- 메모리 인터리빙(Memory Interleaving)의 개념과 효과는?
- 하드웨어 지원 가상화(VT-x, AMD-V)의 원리와 필요성은?
- SSD의 Wear Leveling 기법이란 무엇이며, 왜 필요한가?
- SSD에서 Garbage Collection의 개념과 성능 최적화 방법은?
- HDD의 ZBR(Zone Bit Recording) 기법이란?


- Caching Disk와 Write Buffering의 개념과 차이점은?
- NVMe over Fabrics(NVMe-oF)의 개념과 기존 NVMe와의 차이점은?
- DAS(Direct Attached Storage), NAS(Network Attached Storage), SAN(Storage Area Network)의 차이점은?
- 데이터 무결성(Data Integrity)을 보장하는 ECC(Error Correcting Code) 방식은?
- CRC(Cyclic Redundancy Check)와 ECC(Error Correction Code)의 차이점은?
- RAID 6에서 이중 패리티(Double Parity) 기법의 원리는?


- 파일 시스템에서 저널링(Journaling) 기법의 원리와 장점은?
- 분산 컴퓨팅(Distributed Computing)과 병렬 컴퓨팅(Parallel Computing)의 차이점은?
- MapReduce의 개념과 동작 방식은?
- CPU-GPU 공동 처리(Heterogeneous Computing)의 개념과 활용 사례는?
- OpenCL과 CUDA의 차이점과 활용 사례는?
- 클러스터(Cluster)와 그리드 컴퓨팅(Grid Computing)의 차이점은?


- MPI(Message Passing Interface)에서 동기식(Synchronous) vs 비동기식(Asynchronous) 통신의 차이점은?
- 병렬 프로그래밍에서 레이스 컨디션(Race Condition)과 이를 해결하는 방법은?
- 다중 스레드 환경에서 데드락(Deadlock)의 원인과 해결 방법은?
- NUMA 시스템에서 메모리 접근 최적화를 위한 방법은?
- GPGPU(General-Purpose computing on Graphics Processing Units) 기술이란?
- 하드웨어 기반 보안 기술인 TPM(Trusted Platform Module)이란?


- Meltdown과 Spectre 보안 취약점이 발생하는 원리와 대응 방안은?
- 사이드 채널 공격(Side-Channel Attack)의 종류와 대응 방법은?
- CPU에서 보안 강화를 위한 ASLR(Address Space Layout Randomization) 기법이란?
- 보안 강화 프로세서(Secure Enclave, TrustZone)의 개념과 역할은?
- 데이터 보호를 위한 메모리 암호화(Memory Encryption) 기술이란?
- 해시 기반 무결성 검증(Hash-Based Integrity Checking)의 원리와 활용 사례는?


- 서버 CPU에서 Secure Boot의 원리와 필요성은?
- 하드웨어 백도어(Hardware Backdoor)의 개념과 탐지 방법은?
- 클라우드 컴퓨팅 환경에서 가상 머신(VM) 보안 강화를 위한 하드웨어 기술은?
- 양자 컴퓨터(Quantum Computer)에서 큐비트(Qubit) 개념과 전통적인 비트(Bit)와의 차이점은?
- 뉴로모픽 컴퓨팅(Neuromorphic Computing)의 원리와 활용 가능성은?
- 광컴퓨터(Photonic Computing)의 개념과 기존 반도체 기반 컴퓨터와의 차이점은?


- PIM(Processing-In-Memory) 기술의 개념과 장점은?
- RISC-V가 기존 RISC/CISC 아키텍처와 비교하여 갖는 강점은?
- AI 전용 칩(ASIC, FPGA, TPU)의 차이점과 활용 사례는?
- 스마트NIC(Smart Network Interface Card)의 개념과 활용 사례는?
- Edge AI 컴퓨팅의 개념과 기존 AI 모델과의 차이점은?
- 소프트웨어 정의 하드웨어(Software-Defined Hardware)란 무엇이며, 활용 가능성은?


- 차세대 메모리 기술(Optane, MRAM, ReRAM, PCM 등)의 개념과 기존 DRAM, NAND 플래시와의 차이점은?
- 명령어 스케줄링(Instruction Scheduling)의 역할과 종류는?
- Register Renaming(레지스터 리네이밍)의 개념과 Out-of-Order Execution에서의 역할은?
- 동적 명령어 스케줄링(Dynamic Instruction Scheduling)의 개념과 장점은?
- VLIW(Very Long Instruction Word)에서 명령어 병렬성을 최대한 활용하는 방법은?
- 멀티코어 시스템에서 False Sharing이란 무엇이며, 성능에 미치는 영향은?
- CPU의 마이크로코드(Microcode)란 무엇이며, 어떤 역할을 하는가?


- 실행 흐름 예측(Execution Flow Prediction)이란 무엇이며, 어떤 방식으로 이루어지는가?
- Instruction Window의 개념과 크기를 조절할 때 고려해야 할 요소는?
- Data Forwarding(데이터 포워딩)이란 무엇이며, 파이프라인에서 어떻게 활용되는가?
- Reorder Buffer(재정렬 버퍼)의 역할과 필요성은?
- 페이징과 세그먼테이션을 혼합하여 사용하는 Hybrid 메모리 관리 기법이란?
- Row Hammer 공격이란 무엇이며, 이를 방어하기 위한 하드웨어 기술은?
- TLB Shootdown이란 무엇이며, 멀티프로세서 환경에서 이를 효율적으로 관리하는 방법은?
- Memory Fence(메모리 펜스)란 무엇이며, 동기화에서 어떤 역할을 하는가?


- Write Amplification(쓰기 증폭)이란 무엇이며, 이를 최소화하기 위한 SSD 최적화 방법은?
- Near Memory Computing(NMC)의 개념과 기존 메모리 구조와의 차이점은?
- Load/Store Queue의 개념과 역할은?
- 메모리 주소 공간에서 Direct Mapped Cache와 Set-Associative Cache의 차이점은?
- 메모리 컨트롤러의 역할과 성능 최적화 방법은?
- Prefetch Buffer(프리페치 버퍼)의 역할과 성능 향상 효과는?
- CPU와 FPGA의 차이점과 FPGA의 장점은?


- GPGPU에서 SIMD(Single Instruction Multiple Data)와 SIMT(Single Instruction Multiple Threads)의 차이점은?
- Tensor Processing Unit(TPU)의 구조와 GPU와의 차이점은?
- Pipelined ALU와 Non-Pipelined ALU의 차이점과 성능 비교는?
- Coarse-Grained Multithreading과 Fine-Grained Multithreading의 차이점은?
- Shared Memory와 Distributed Memory 모델의 차이점과 활용 사례는?


- GPU에서 Thread Divergence(스레드 분기)란 무엇이며, 성능 저하를 방지하는 방법은?
- Heterogeneous Computing(이기종 컴퓨팅)이란 무엇이며, 어떤 환경에서 사용되는가?
- Asynchronous Execution(비동기 실행)과 동기 실행의 차이점은?
- Persistent Memory(지속성 메모리)의 개념과 활용 사례는?
- PCIe(PCI Express)에서 Lane과 Bandwidth의 관계는?
- Direct I/O와 Memory-Mapped I/O의 차이점은?
- RDMA(Remote Direct Memory Access)의 개념과 기존 TCP/IP 기반 데이터 전송과의 차이점은?


- Zero-Copy I/O 기술의 개념과 성능 향상 효과는?
- 인터럽트의 벡터 처리 방식(Vector Interrupt Processing)이란?
- NVMe Direct Storage의 개념과 기존 스토리지 방식과의 차이점은?
- Adaptive Interrupt Moderation(적응형 인터럽트 조절)의 개념과 활용 사례는?
- Infiniband 네트워크의 개념과 고성능 컴퓨팅에서의 활용 사례는?
- Host Channel Adapter(HCA)란 무엇이며, 고성능 네트워크에서 어떤 역할을 하는가?
- 고속 네트워크에서 Jumbo Frame(점보 프레임)이란 무엇이며, 장점과 단점은?


- CPI(Cycles Per Instruction)와 IPC(Instructions Per Cycle)의 차이점은?
- Benchmarks(SPEC, TPC, LINPACK 등)의 종류와 사용 목적은?
- Amdahl’s Law를 적용할 때, 병렬 처리 속도 향상 한계는?
- Roofline Model이란 무엇이며, 프로세서 성능을 평가할 때 어떻게 활용되는가?
- Hot Spot Analysis(핫스팟 분석)의 개념과 성능 최적화 기법은?
- CPU 사용률(Usage)과 CPU Stall(스톨) 간의 관계는?


- Performance Counter(성능 카운터)란 무엇이며, 이를 활용한 시스템 성능 분석 방법은?
- Instruction Fusion이란 무엇이며, 어떻게 성능을 최적화하는가?
- Latency와 Throughput의 차이점과 성능 측정 시 고려해야 할 요소는?
- Processor Affinity(프로세서 친화도)란 무엇이며, 성능 최적화에 어떤 영향을 미치는가?
- Quantum Supremacy(양자 우위)의 개념과 현재 연구 동향은?
- Memristor의 개념과 기존 트랜지스터 기반 메모리와의 차이점은?


- DNA Computing이란 무엇이며, 기존 컴퓨팅과의 차이점은?
- RRAM(Resistive RAM)의 개념과 기존 NAND Flash와의 차이점은?
- Optical Computing(광 컴퓨팅)의 원리와 기존 전자 기반 컴퓨팅과의 차이점은?
- Edge AI 프로세서의 개념과 기존 클라우드 AI와의 차이점은?
- 3D TSV(Through-Silicon Via) 기술이란 무엇이며, 기존 반도체 설계와의 차이점은?
- Nanosheet Transistor(나노시트 트랜지스터)란 무엇이며, 기존 FinFET과의 차이점은?


- 탄소 나노튜브 기반 트랜지스터의 개념과 기존 실리콘 반도체와의 차이점은?
- Photonic Neural Networks(광 뉴럴 네트워크)의 개념과 활용 가능성은?
- CPU에서 Instruction Fusion이란 무엇이며, 성능 최적화에 어떤 영향을 미치는가?
- Tomasulo’s Algorithm이란 무엇이며, 명령어 스케줄링에서 어떤 역할을 하는가?
- Value Prediction(값 예측)의 개념과 성능 최적화 기법은?
- Micro-Op Cache(마이크로 연산 캐시)란 무엇이며, 성능 향상에 어떤 역할을 하는가?


- Speculative Execution에서 Spectre와 Meltdown 공격이 발생하는 원리는?
- Loop Unrolling(루프 언롤링)이란 무엇이며, CPU 성능 최적화에서 어떤 역할을 하는가?
- Operand Forwarding(피연산자 포워딩)이란 무엇이며, 파이프라인 성능에 어떤 영향을 미치는가?
- Register Bypassing(레지스터 바이패싱)이란 무엇이며, 연산 속도를 높이기 위한 전략은?
- Shadow Register(섀도 레지스터)의 개념과 활용 사례는?
- Stack Machine과 Register Machine의 차이점은?


- DRAM에서 Bank Interleaving(뱅크 인터리빙)이란 무엇이며, 성능 향상 효과는?
- DDR3, DDR4, DDR5의 차이점과 성능 개선 요소는?
- Persistent Memory(지속성 메모리)와 기존 DRAM/NAND Flash의 차이점은?
- Hybrid Memory Cube(HMC)와 High Bandwidth Memory(HBM)의 차이점과 장점은?
- DRAM의 Row Buffer Hit과 Miss가 성능에 미치는 영향은?
- Virtual Memory에서 Inverted Page Table 구조의 장점과 단점은?
- 메모리 압축 기술(Memory Compression)의 원리와 성능 향상 효과는?


- Software-Managed Cache와 Hardware-Managed Cache의 차이점은?
- Address Alias(주소 중복) 문제와 이를 해결하는 방법은?
- Memory Consistency Model(메모리 일관성 모델)이란 무엇이며, 대표적인 모델은?
- Spinlock과 Mutex의 차이점과 활용 사례는?
- Read-Copy-Update(RCU) 기법이란 무엇이며, 멀티스레드 환경에서 어떤 역할을 하는가?
- Cache Coherence Protocol에서 MOESI와 MESI의 차이점은?


- Directory-Based Cache Coherence의 개념과 장점은?
- Thread-Level Speculation(TLS)란 무엇이며, 성능 최적화에서 어떻게 활용되는가?
- Data Prefetching과 Instruction Prefetching의 차이점과 성능 최적화 효과는?
- NUMA-aware Memory Allocation이란 무엇이며, 성능 최적화 방법은?
- Weak Consistency와 Sequential Consistency의 차이점은?
- Write Combining 기술이란 무엇이며, 메모리 쓰기 성능에 미치는 영향은?


- Hardware Transactional Memory(HTM)란 무엇이며, 병렬 프로그래밍에서의 역할은?
- I/O 성능 최적화를 위한 Direct Memory Access (DMA)와 RDMA의 차이점은?
- PCIe에서 Link Width(x1, x4, x8, x16)가 성능에 미치는 영향은?
- Non-Uniform Memory Access (NUMA)에서 I/O 디바이스 배치가 성능에 미치는 영향은?
- I/O Bottleneck을 해결하기 위한 Overlapping I/O 기법이란?
- Thunderbolt와 USB의 차이점과 성능 비교는?


- Polling 기반 I/O와 Interrupt 기반 I/O의 차이점과 성능 비교는?
- Persistent Storage에서 Write Cliff 현상이란 무엇이며, 해결 방법은?
- Hot Plugging(핫 플러깅)이 가능한 인터페이스(USB, PCIe 등)에서 고려해야 할 요소는?
- Network-on-Chip(NoC)의 개념과 기존 버스 아키텍처와의 차이점은?
- Direct I/O와 Memory-Mapped I/O의 차이점과 성능 영향은?
- CPU의 Superpipeline(슈퍼파이프라이닝)이란 무엇이며, 기존 파이프라인과의 차이점은?


- Data Dependency Hazard(데이터 종속 해저드)의 종류와 해결 방법은?
- Loop-Carried Dependency(루프 의존성)이란 무엇이며, 이를 해결하기 위한 기법은?
- Register Windowing(레지스터 윈도잉) 기법이란 무엇이며, RISC 아키텍처에서 어떻게 활용되는가?
- Fetch-Decode-Execute 사이클에서 각 단계의 주요 역할과 병목을 줄이기 위한 최적화 방법은?
- Control Flow Graph(CFG) 기반 최적화 기법에는 어떤 것들이 있는가?
- Hardware Prefetching과 Software Prefetching의 차이점은?


- Instruction-Level Parallelism(ILP)과 Data-Level Parallelism(DLP)의 차이점은?
- Out-of-Order Execution과 Speculative Execution의 차이점과 보완 관계는?
- Branch Target Buffer(BTB)의 개념과 Branch Prediction 성능에 미치는 영향은?
- 메모리 액세스에서 Temporal Locality와 Spatial Locality의 개념과 차이점은?
- Adaptive Replacement Cache(ARC)와 Least Recently Used(LRU) 캐시 교체 알고리즘의 차이점은?
- Cache Associativity와 Conflict Miss의 관계는?


- Cache Write Allocation Policy(쓰기 할당 정책)에는 어떤 것이 있으며, 각각의 특징은?
- Cache Partitioning(캐시 파티셔닝)의 개념과 멀티코어 환경에서의 활용 사례는?
- Translation Lookaside Buffer(TLB)의 Multi-Level 구조가 필요한 이유는?
- Address Space Layout Randomization(ASLR)이 메모리 보안에서 어떤 역할을 하는가?
- Memory Bandwidth(메모리 대역폭)과 Latency(지연시간)의 차이점과 성능 최적화 방법은?
- ECC (Error Correcting Code) 메모리의 개념과 Single-Bit, Multi-Bit 오류를 처리하는 방법은?
- Memory Scrubbing이란 무엇이며, 데이터 무결성을 유지하는 방법은?


- False Sharing이 발생하는 원인과 이를 방지하는 방법은?
- Multi-Threading 환경에서 Load Balancing(부하 균형) 기법에는 어떤 것들이 있는가?
- Directory-Based Cache Coherence에서 Full Map과 Limited Map의 차이점은?
- Prefetching이 캐시 일관성(Cache Coherency)에 미치는 영향은?
- Lock-Free Data Structure(락 프리 자료구조)의 개념과 구현 방법은?
- Parallel Reduction(병렬 축소)의 개념과 SIMD/SIMT에서의 활용 사례는?


- HyperTransport와 QuickPath Interconnect(QPI)의 차이점은?
- Synchronization Barrier(동기화 장벽)이란 무엇이며, 멀티스레딩 환경에서의 역할은?
- Multi-Core 환경에서 Memory Contention을 줄이는 방법은?
- CC-NUMA(Cache-Coherent Non-Uniform Memory Access)에서 성능을 최적화하는 기법은?
- PCIe(Peripheral Component Interconnect Express)에서 Lane Width(x1, x4, x8, x16)와 대역폭 관계는?
- Direct I/O와 Memory-Mapped I/O의 차이점과 성능 비교는?


- Latency-Hiding Techniques(지연시간 은폐 기법)의 개념과 예시는?
- USB 3.0과 USB 4.0의 차이점과 성능 비교는?
- Storage Class Memory(SCM)란 무엇이며, 기존 DRAM 및 NAND 플래시와의 차이점은?
- I/O Bandwidth와 I/O Throughput의 차이점은?
- Hot Plugging(핫 플러깅) 지원 인터페이스에서 발생할 수 있는 기술적 문제와 해결 방법은?
- M.2와 U.2 인터페이스의 차이점과 활용 사례는?
- NVMe Queueing Mechanism이 기존 SATA 인터페이스와 비교했을 때 갖는 장점은?


- Memory-Mapped I/O가 고속 I/O 처리에서 갖는 장점과 단점은?
- DNA Computing이란 무엇이며, 기존 컴퓨터 아키텍처와의 차이점은?
- Optical Computing(광 컴퓨팅)의 개념과 기존 전자기반 컴퓨팅과의 차이점은?
- Zero Trust Architecture(제로 트러스트 아키텍처)가 하드웨어 보안에서 중요한 이유는?
- Edge AI 프로세서의 개념과 기존 클라우드 AI와의 차이점은?
- Near-Memory Computing(NMC)의 개념과 기존 메모리 계층구조와의 차이점은?


- Neuromorphic Computing(뉴로모픽 컴퓨팅)이란 무엇이며, AI 연산에서 어떻게 활용되는가?
- Quantum Error Correction(양자 오류 정정) 기술이란 무엇이며, 왜 필요한가?
- Nanosheet Transistor(나노시트 트랜지스터)란 무엇이며, 기존 FinFET과의 차이점은?
- 3D Heterogeneous Integration(3D 이기종 집적)의 개념과 활용 가능성은?
- Human Brain-Inspired Computing(인간 뇌 모방 컴퓨팅)이란 무엇이며, 기존 AI 아키텍처와의 차이점은?
- Instruction Window(명령어 윈도우) 크기가 성능에 미치는 영향은?


- Indirect Branch Prediction(간접 분기 예측)의 개념과 최적화 방법은?
- Register File의 크기와 CPU 성능 간의 관계는?
- ALU Pipeline과 FPU Pipeline의 차이점과 활용 사례는?
- Decoupled Access/Execute Architecture(분리형 접근/실행 아키텍처)란 무엇인가?
- Hardware Loop Buffer(하드웨어 루프 버퍼)란 무엇이며, 성능 향상 효과는?
- SIMD Execution Unit과 일반 ALU의 차이점과 장점은?


- Power Gating과 Clock Gating의 개념과 전력 절감 효과는?
- Register Scoreboarding이란 무엇이며, Out-of-Order Execution에서 어떤 역할을 하는가?
- Hybrid Branch Prediction(하이브리드 분기 예측) 기술의 개념과 성능 최적화 방법은?
- Page Coloring 기법의 개념과 캐시 활용도 향상 효과는?
- Direct Mapped Cache에서 Conflict Miss를 줄이는 방법은?
- Sub-Block Placement Policy(서브 블록 배치 정책)이란 무엇이며, 성능 최적화 방법은?


- Page Walk Overhead(페이지 탐색 오버헤드)을 줄이기 위한 최적화 기법은?
- Cache Write Throttling(캐시 쓰기 제어)이란 무엇이며, 성능에 미치는 영향은?
- Cache Reuse Distance(캐시 재사용 거리) 분석이란 무엇이며, 이를 활용한 성능 최적화 방법은?
- Virtual Memory에서 Zero Page Optimization이란 무엇이며, 성능 향상 효과는?
- Flash Memory의 Program/Erase Cycle이 성능과 내구성에 미치는 영향은?
- Adaptive Page Replacement Algorithm(적응형 페이지 교체 알고리즘)이란 무엇인가?


- Memory Prefetching이 성능에 미치는 영향을 측정하는 방법은?
- Shared Memory System에서 False Sharing을 방지하는 방법은?
- MESIF(Cache Coherency Protocol)와 기존 MESI 프로토콜의 차이점은?
- Distributed Shared Memory(분산 공유 메모리)의 개념과 활용 사례는?
- Task-Level Parallelism(TLP)과 Data-Level Parallelism(DLP)의 차이점은?
- Multi-Chip Module(MCM)과 단일 다이 프로세서의 성능 및 설계 차이점은?


- Parallel Programming에서 Load Imbalance(부하 불균형)의 원인과 해결 방법은?
- Work Stealing Algorithm(작업 도둑 알고리즘)이란 무엇이며, 성능 최적화 효과는?
- Compute-Bound Task와 Memory-Bound Task의 차이점은?
- CC-NUMA와 SC-NUMA의 차이점과 성능 최적화 방법은?
- Parallel Reduction과 Scan Operation(스캔 연산)의 차이점은?
- NVMe Over Fabrics(NVMe-oF)의 개념과 기존 스토리지 인터페이스와의 차이점은?


- Persistent Memory에서 Read Disturbance 현상이란 무엇이며, 이를 해결하는 방법은?
- DMA에서 Bounce Buffering이란 무엇이며, 성능에 미치는 영향은?
- PCIe Atomics(PCIe 원자적 연산)의 개념과 활용 사례는?
- I/O Virtualization(IOV)이란 무엇이며, 가상 환경에서의 역할은?
- Adaptive Queue Depth Management(적응형 큐 깊이 관리)의 개념과 성능 최적화 효과는?
- RDMA에서 On-Demand Paging의 개념과 메모리 관리 최적화 효과는?


- DPDK(Data Plane Development Kit)의 개념과 고속 네트워크 성능 최적화에 미치는 영향은?
- Queue Pair(QP) 기반 I/O 성능 최적화 기법이란 무엇인가?
- SR-IOV(Single Root I/O Virtualization)와 MR-IOV(Multi-Root I/O Virtualization)의 차이점은?
- Quantum Annealing(양자 어닐링)과 Gate-Based Quantum Computing의 차이점은?
- DNA-Based Data Storage(생체 기반 데이터 저장 기술)의 개념과 기존 스토리지와의 차이점은?
- Brain-Inspired Computing(뇌 모방 컴퓨팅)에서 Spiking Neural Network(SNN)의 개념과 활용 사례는?


- Analog Computing의 개념과 기존 디지털 컴퓨팅과의 차이점은?
- Near-Data Processing(NDP)의 개념과 기존 메모리 계층구조와의 차이점은?
- Optane Persistent Memory와 기존 DRAM/NAND 플래시와의 차이점은?
- Compute Express Link(CXL)의 개념과 기존 PCIe 인터커넥트와의 차이점은?
- Quantum-Classical Hybrid Computing의 개념과 활용 사례는?
- Multi-Tenant Accelerator(다중 사용자 가속기)의 개념과 클라우드 환경에서의 활용 사례는?
- Space Computing(우주 컴퓨팅)의 개념과 기존 지구 기반 컴퓨팅과의 차이점은?


- Instruction Packing(명령어 패킹)이란 무엇이며, 실행 효율성에 미치는 영향은?
- Dynamic Binary Translation(동적 바이너리 변환)이란 무엇이며, 성능 최적화에서 어떻게 활용되는가?
- Instruction Set Simulator(ISS)의 개념과 활용 사례는?
- CPU 설계에서 Operand Fetch Optimization(피연산자 가져오기 최적화) 기법은?
- Hardware Multithreading에서 Coarse-Grained과 Fine-Grained 기법의 차이점은?
- Loop Perforation(루프 생략) 기법이란 무엇이며, 성능과 정확도 사이의 트레이드오프는?


- CPU에서 Conditional Move(조건부 이동) 명령어의 개념과 활용 사례는?
- Micro-Op Decomposition(마이크로 연산 분해)이란 무엇이며, 실행 효율성을 높이는 방법은?
- Operand Bypass(피연산자 바이패스) 기법이란 무엇이며, 파이프라인 해저드를 줄이는 방법은?
- Wide Issue Processor(광폭 발행 프로세서)의 개념과 기존 프로세서와의 차이점은?
- NUMA 환경에서 Thread Affinity(스레드 친화성)를 조정하는 방법은?
- Page Rank Algorithm과 메모리 접근 패턴 간의 관계는?
- Hardware Managed Prefetching과 Software Managed Prefetching의 차이점은?


- Scratchpad Memory(스크래치패드 메모리)의 개념과 일반 캐시 메모리와의 차이점은?
- Multi-Banked DRAM과 Single-Bank DRAM의 차이점과 성능 영향은?
- Cache Coloring과 Heap Memory Allocation의 관계는?
- DRAM에서 Access Granularity(접근 단위 크기)가 성능에 미치는 영향은?
- Subthreshold Leakage(서브스레숄 누설 전류)가 메모리 설계에서 문제가 되는 이유는?
- Out-of-Bounds Memory Access(경계 초과 메모리 접근) 문제를 방지하는 하드웨어 기술은?
- Page Frame Reclamation(페이지 프레임 회수) 기법이란 무엇이며, 성능에 미치는 영향은?


- Dynamic Voltage and Frequency Scaling(DVFS)와 병렬 처리 성능 간의 관계는?
- Spinning과 Blocking 동기화 기법의 차이점과 활용 사례는?
- Load-Linked/Store-Conditional(LL/SC) 명령어의 개념과 원자적 연산에서의 역할은?
- Non-Blocking Synchronization(비차단 동기화)와 기존 락 기반 동기화의 차이점은?
- Vector Processing Unit(VPU)와 일반 SIMD의 차이점은?
- Hardware Transactional Memory(HTM)에서 Conflict Detection(충돌 감지) 기법의 종류는?
- Deep Learning Accelerator(딥러닝 가속기)에서 Dataflow Optimization의 개념은?


- Fine-Grained Synchronization(세밀한 동기화)의 개념과 성능 최적화 효과는?
- Data Dependency Graph를 활용한 병렬 실행 최적화 기법은?
- Dynamic Thread Migration(동적 스레드 이동) 기술이란 무엇이며, 성능 최적화 방법은?
- DMA에서 Descriptor Ring Buffer 방식과 Linked List 방식의 차이점은?
- Non-Volatile Main Memory(NVMM)의 개념과 기존 DRAM 대비 장점은?
- USB-C와 Thunderbolt 4의 차이점과 성능 비교는?
- NVMe Namespace의 개념과 멀티 테넌트 환경에서의 활용 사례는?
- PCIe Switch Fabric의 개념과 클라우드 환경에서의 활용 사례는?


- Network I/O에서 Interrupt Coalescing(인터럽트 합병) 기법이 성능에 미치는 영향은?
- SR-IOV(Single Root I/O Virtualization)에서 Virtual Function(VF)과 Physical Function(PF)의 차이점은?
- Hardware Queueing(하드웨어 큐잉) 기법이 Storage I/O에서 성능에 미치는 영향은?
- Multi-Path I/O(MPIO)와 Storage Load Balancing의 개념과 차이점은?
- RDMA에서 Zero Copy Data Transfer의 개념과 기존 TCP 기반 전송과의 차이점은?
- Spintronics(스핀트로닉스) 기반 컴퓨팅의 개념과 기존 CMOS 기술과의 차이점은?
- 3D-IC(3D 집적 회로)의 개념과 기존 2D IC 대비 장점과 단점은?


- Probabilistic Computing(확률적 컴퓨팅)이란 무엇이며, 기존 불확정성 기반 컴퓨팅과의 차이점은?
- Ising Model을 활용한 최적화 문제 해결 방법은?
- Bio-Inspired Computing(생체 모방 컴퓨팅)의 개념과 활용 사례는?
- Compute-in-Memory(CIM)와 Processing-in-Memory(PIM)의 차이점과 각각의 활용 가능성은?
- Edge TPU의 개념과 기존 AI 가속기 대비 차이점은?
- Resistive Switching Device(RRAM, Memristor)의 개념과 기존 NAND 플래시와의 차이점은?
- Energy-Efficient AI Accelerator(저전력 AI 가속기)의 개념과 설계 원리는?
- DNA Computing에서 Hybrid Molecular Electronics의 개념과 활용 가능성은?


- Triple Modular Redundancy(TMR)이란 무엇이며, 신뢰성 향상을 위해 어떻게 활용되는가?
- FPGA에서 Dynamic Partial Reconfiguration(DPR)의 개념과 활용 사례는?
- VLIW 아키텍처에서 Bundled Execution(번들 실행) 방식이란 무엇인가?
- SIMD Vectorization과 Loop Unrolling을 조합하여 성능을 최적화하는 방법은?
- Wavefront Scheduling이란 무엇이며, GPU 연산에서 어떻게 활용되는가?
- Cache Pipeline Stalls(캐시 파이프라인 스톨)이 발생하는 원인과 이를 해결하는 방법은?
- Speculative Store Bypass(SB)와 보안 취약점의 관계는?


- Hardware Prefetcher Throttling(하드웨어 프리패처 스로틀링)이 성능에 미치는 영향은?
- Out-of-Order Execution에서 Load-Store Queue(LSQ)의 역할은?
- Address Generation Interlock(주소 생성 인터록)이란 무엇이며, 이를 줄이기 위한 최적화 방법은?
- Hybrid Cache Architecture(하이브리드 캐시 아키텍처)의 개념과 장점은?
- Direct Segment 기반 가상 메모리 구조의 개념과 기존 페이지 기반 메모리 관리 방식과의 차이점은?
- Cache Miss Penalty를 최소화하기 위한 최적화 기법은?
- Virtual Address Space Fragmentation(가상 주소 공간 단편화)의 원인과 해결 방법은?


- Decoupled Access-Execute Memory Architecture(분리형 접근-실행 메모리 구조)의 개념과 활용 사례는?
- Last-Level Cache(LLC)의 개념과 Multi-Core CPU에서의 역할은?
- Bank-Level Parallelism(BLP)이 DRAM 성능에 미치는 영향은?
- Soft Errors(소프트 오류)가 메모리 안정성에 미치는 영향과 이를 보완하는 기술은?
- Read Disturb Issue가 NAND Flash의 수명에 미치는 영향과 이를 해결하는 방법은?
- Transparent Memory Compression(투명 메모리 압축)의 개념과 성능 최적화 방법은?
- Multi-Threaded Processor에서 TLP(Thread-Level Parallelism)와 ILP(Instruction-Level Parallelism)의 관계는?


- Graph Analytics에서 PageRank Algorithm이 병렬 실행에서 갖는 특수성은?
- Global Synchronization(글로벌 동기화)이 대규모 병렬 시스템에서 성능 저하를 일으키는 이유는?
- Scalable Weak Memory Consistency Model(확장 가능한 약한 메모리 일관성 모델)이란 무엇인가?
- Persistent Memory 기반 시스템에서 데이터 일관성을 유지하는 방법은?
- Cache Contention을 줄이기 위한 Cooperative Cache Management 기법이란?
- Hierarchical Coherency Domains(계층적 일관성 도메인)이란 무엇이며, 멀티코어 CPU에서 어떻게 활용되는가?


- Dynamic Voltage and Frequency Scaling(DVFS)과 캐시 성능의 관계는?
- Hardware Barrier Synchronization(하드웨어 장벽 동기화)이란 무엇이며, 성능 향상 효과는?
- NUMA와 GPU Unified Memory의 차이점과 각각의 활용 사례는?
- PCIe Resizable BAR(Resizeable Base Address Register)의 개념과 성능 향상 효과는?
- Adaptive Routing(적응형 라우팅)이 NoC(Network-on-Chip) 성능에 미치는 영향은?
- Optically Interconnected Memory Systems(광 연결 메모리 시스템)의 개념과 기존 전자 기반 연결과의 차이점은?


- RDMA에서 Memory Registration과 Key Caching이 성능에 미치는 영향은?
- Next-Generation Storage Interfaces에서 Zoned Namespaces(ZNS) 기술의 개념과 기존 블록 스토리지와의 차이점은?
- I/O Stack Bypassing(입출력 스택 우회)의 개념과 성능 최적화 사례는?
- Zero Copy Networking(제로 카피 네트워킹)과 TCP/IP Offloading Engine(TOE)의 차이점은?
- Persistent Memory 기반 스토리지에서 Mixed Workload Optimization(혼합 작업 부하 최적화)의 개념은?
- Hardware Message Passing과 Software Message Passing의 차이점과 각각의 활용 사례는?


- Inline Data Deduplication(인라인 데이터 중복 제거)이 SSD 성능에 미치는 영향은?
- 3D TSV(Through-Silicon Via) 기술이 기존 2.5D 패키징 기술과 비교하여 갖는 장점과 단점은?
- Coarse-Grained Reconfigurable Architecture(CGRA)란 무엇이며, FPGA와의 차이점은?
- Near-Sensor Computing(센서 근접 컴퓨팅)이란 무엇이며, 엣지 AI에서의 활용 가능성은?
- Reversible Computing(가역 컴퓨팅)이란 무엇이며, 에너지 효율성에 미치는 영향은?
- Quantum Tunneling Transistor(양자 터널링 트랜지스터)의 개념과 기존 CMOS 트랜지스터와의 차이점은?
- DNA Storage에서 Information Density(정보 밀도) 증가가 기존 HDD 및 SSD와 비교했을 때 갖는 이점은?


- Stochastic Computing(확률적 컴퓨팅)이란 무엇이며, AI 모델 학습에서 어떻게 활용되는가?
- Self-Healing Hardware(자가 치유 하드웨어)의 개념과 고장 허용 컴퓨팅에서의 활용 가능성은?
- Photonic Neuromorphic Computing(광 뉴로모픽 컴퓨팅)이란 무엇이며, 기존 디지털 뉴로모픽 시스템과의 차이점은?
- Molecular Electronics(분자 전자공학)의 개념과 기존 반도체 소자와의 차이점은?