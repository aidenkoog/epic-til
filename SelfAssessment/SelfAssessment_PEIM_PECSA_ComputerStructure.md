# Concepts, Features, Types and Pros and Cons

Organize concepts, features, types and Pros and Cons

## Computer Structure

- 폰 노이만(John von Neumann) 구조와 특징 설명
    - 정의
        - 폰 노이만 구조(Von Neumann Architecture)는 컴퓨터의 기본적인 구조를 정의한 개념
        - 1945년 존 폰 노이만(John von Neumann)이 제안한 컴퓨터 설계 방식
        - 프로그램 내장 방식(stored-program concept)을 기반으로 하며, 현재 대부분의 컴퓨터가 이 구조 유지 중
    - 폰 노이만 구조의 주요 특징
        - 프로그램 내장 방식 (Stored-Program Concept)
            - 프로그램(명령어)과 데이터를 동일한 메모리(RAM)에 저장하고, 필요할 때 CPU가 이를 가져와 실행하는 방식
            - 과거의 컴퓨터(예: 초기 기계식 컴퓨터)는 하드웨어 배선을 변경해야 명령어를 바꿀 수 있었지만, 폰 노이만 구조에서는 소프트웨어적으로 프로그램을 변경 가능
        - 하나의 메모리를 명령어와 데이터가 공유
            - 프로그램의 명령어와 데이터가 같은 메모리 공간에 저장
            - CPU는 이를 순차적으로 가져와(fetch) 실행
        - 중앙처리장치(CPU)가 연산 수행
            - CPU는 명령어를 실행하는 중심 역할을 하며, 다음과 같은 구성 요소로 이루어짐
                - 연산장치(ALU, Arithmetic Logic Unit): 덧셈, 뺄셈, 논리 연산 등 수행
                - 제어장치(CU, Control Unit): 명령어를 해석하고 실행 순서를 제어
                - 레지스터(Register): 연산에 필요한 임시 데이터를 저장
        - 순차적 실행 방식
            - 프로그램 명령어는 순차적으로(fetch-decode-execute cycle) 실행
            - 현대 컴퓨터는 성능 향상을 위해 파이프라이닝, 캐시 메모리, 분기 예측 등을 활용하여 속도를 높이고 있음
        - 메모리 병목 현상(Von Neumann Bottleneck)
            - CPU와 메모리가 단일 버스를 공유하기 때문에, CPU가 처리할 데이터보다 메모리에서 데이터를 가져오는 속도가 느려지는 현상 발생 가능성 있음
            - 해결하기 위해 캐시 메모리(Cache), 다중 코어 CPU, 병렬 처리 등의 기술 등장
    - 폰 노이만 구조 / 하버드 구조 비교
        - 메모리 분리 여부
          - 명령어와 데이터가 같은 메모리 사용
          - 명령어와 데이터를 별도 메모리에 저장
        - 버스(Bus) 구조
          - 명령어와 데이터 전송을 같은 버스로 처리
          - 명령어와 데이터 전송을 개별 버스로 처리
        - 장점
          - 설계가 단순하고 범용성이 높음
          - 메모리 병목 현상이 적고 성능이 높음
        - 단점
          - 메모리 병목 문제 발생 가능
          - 하드웨어 설계가 복잡하고 비용 증가
        - 적용 사례
          - 대부분의 범용 컴퓨터(PC, 서버)
          - 임베디드 시스템, DSP(Digital Signal Processor)
    - 결론
      - 현대 컴퓨터의 기초를 제공
      - 대부분의 범용 컴퓨터가 폰 노이만 구조를 기반으로 설계됨
        - 프로그램 내장 방식으로 소프트웨어 변경 가능
      - 하드웨어 배선 변경 없이 소프트웨어적으로 기능을 수정할 수 있음
        - 컴퓨터 아키텍처 발전의 기틀 제공
      - 이후 하버드 구조, 캐시 메모리, 다중 코어 CPU 등 성능 개선 기술이 등장하는 기반이 됨
      - 재 정리
        - 폰 노이만 구조는 프로그램 내장 방식을 기반으로 하며, CPU, 메모리, 단일 버스를 활용하여 순차적으로 명령을 실행하는 컴퓨터 구조
        - 하지만 메모리 병목 문제가 발생할 수 있어, 현대 컴퓨터에서는 캐시 메모리, 병렬 처리 등의 기술로 이를 보완하고 있음

- 하버드 아키텍처(Harvard Architecture)와 폰 노이만 구조의 차이점
    - 폰 노이만 구조 / 하버드 구조 비교
        - 메모리 분리 여부
          - 폰 노이만: 명령어와 데이터가 같은 메모리 사용
          - 하버드 구조: 명령어와 데이터를 별도 메모리에 저장
        - 버스(Bus) 구조
          - 폰 노이만: 명령어와 데이터 전송을 같은 버스로 처리
          - 하버드 구조: 명령어와 데이터 전송을 개별 버스로 처리
        - 장점
          - 폰 노이만: 설계가 단순하고 범용성이 높음
          - 하버드 구조: 메모리 병목 현상이 적고 성능이 높음
        - 단점
          - 폰 노이만: 메모리 병목 문제 발생 가능
          - 하버드 구조: 하드웨어 설계가 복잡하고 비용 증가
        - 적용 사례
          - 폰 노이만: 대부분의 범용 컴퓨터(PC, 서버)
          - 하버드 구조: 임베디드 시스템, DSP(Digital Signal Processor)

- CISC와 RISC의 개념과 차이점
    - CISC
        - Complex Instruction Set Computer
        - CISC(복잡한 명령어 집합 컴퓨터, Complex Instruction Set Computer)는 하나의 명령어로 복잡한 연산을 수행할 수 있도록 설계된 프로세서 아키텍처
        - 명령어 개수가 많고, 하나의 명령어가 여러 개의 연산을 수행할 수 있음.
        - 어셈블리 코드가 간결해지고, 프로그래밍이 쉬워짐.
        - 특징
	        - 명령어 개수 多, 명령어 길이 가변적
	        - 하나의 명령어로 여러 연산 수행 가능 → 고급 언어와 유사한 기능 지원
	        - 메모리 접근 방식이 다양함 → 다양한 주소 지정 방식 지원
	        - 마이크로코드(Microcode) 사용 → 하드웨어가 복잡해짐
	        - CPU 설계가 복잡하지만, 코드 크기를 줄일 수 있음
        - 대표적인 CISC 프로세서
	        - Intel x86 (Pentium, Core i7, i9 등)
	        - AMD Ryzen
    - RISC
        - Reduced Instruction Set Computer
        - RISC(축소 명령어 집합 컴퓨터, Reduced Instruction Set Computer)는 명령어를 단순화하여 빠르게 실행할 수 있도록 설계된 프로세서 아키텍처
        - 단순한 명령어를 빠르게 실행하는 방식으로, 하드웨어가 단순하고 최적화가 쉬움.
        - 명령어가 동일한 크기로 고정되어 있어 파이프라이닝(Pipelining) 성능이 우수함.
        - 특징
	        - 명령어 개수 少, 명령어 길이 고정적
	        - 하나의 명령어는 한 가지 연산만 수행 (LOAD/STORE 방식)
	        - 메모리 접근 제한 (메모리 연산은 LOAD/STORE 명령어로만 가능)
	        - 하드웨어 설계가 단순하고, 실행 속도가 빠름
	        - CPU 내부에서 명령어 실행이 균일하여 병렬 처리(Pipelining) 최적화 가능
        - 대표적인 RISC 프로세서
	        - ARM (스마트폰, 태블릿, 임베디드 시스템 등 대부분의 모바일 CPU)
	        - Apple M1/M2, Qualcomm Snapdragon, Samsung Exynos
	        - IBM PowerPC, RISC-V, MIPS
    - 핵심 차이점 정리
        - CISC는 명령어가 강력하여 코드가 짧지만, 실행 속도가 상대적으로 느림.
        - RISC는 단순한 명령어를 빠르게 실행하여 성능이 뛰어나며, 저전력 설계에 적합함
    - CISC vs RISC 실제 적용 예시
        - CISC (Intel x86)
            - 데스크톱, 서버, 고성능 컴퓨팅에 사용
            - 명령어 하나로 여러 연산을 수행
            - Windows PC, 고성능 워크스테이션, 서버에서 주로 사용
            - 예제 (Intel x86 어셈블리)
            - 한 번의 명령어로 메모리에서 값을 가져오고 연산 가능
            - CISC 구조에서는 복잡한 연산을 적은 명령어로 표현 가능
        - RISC (ARM, Apple M1)
            - 모바일, 임베디드, 저전력 환경에 사용
            - 단순한 명령어를 빠르게 실행하여 전력 효율성이 높음
            - 스마트폰, 태블릿, IoT, 클라우드 서버에서 사용
            - 예제 (ARM 어셈블리)
            - LOAD/STORE 방식을 사용하여 메모리 접근 제한
            - 파이프라이닝이 용이하여 실행 속도가 빠름
    - 최신 트렌드: RISC 기반의 시장 확대
        - 최근 RISC 아키텍처(특히 ARM)가 점점 시장을 확장하고 있음
            - Apple M1, M2 칩 → MacBook에서 ARM 기반으로 전환
            - Qualcomm Snapdragon → 모바일 & 노트북 시장 확대
            - AWS Graviton → 클라우드 서버에서도 ARM 기반 RISC CPU 사용 증가
        - 이유
            - RISC는 전력 효율이 높아 모바일, 클라우드, 서버 환경에 적합
            - CISC(Intel, AMD)는 고성능 컴퓨팅에 강하지만, 전력 소모가 많음
            - ARM 기반 CPU가 계속 발전하면서 서버 및 데스크톱 시장에서도 사용 증가
    - 결론
        - CISC는 복잡한 명령어 집합을 제공하여 코드 크기를 줄일 수 있지만, 실행 속도가 상대적으로 느림.
        - RISC는 단순한 명령어를 빠르게 실행하여 성능이 뛰어나고, 전력 효율이 높음.
        - Intel, AMD CPU는 CISC 기반, ARM 및 Apple M1/M2, Qualcomm은 RISC 기반.
        - 최근에는 RISC 기반 프로세서가 성능을 향상시키면서 시장 점유율이 증가하는 추세.
        - 전통적인 고성능 컴퓨팅(CISC) vs 저전력, 고효율(RISC)의 차이가 있으며, 최근에는 RISC 기반 CPU가 데스크톱/서버 시장에서도 성장 중

- 컴퓨터의 기본 구성 요소(입력, 출력, 기억장치, 연산장치, 제어장치)에 대해 설명
    - 개요
        - 컴퓨터의 기본 구성 요소
            - 입력 장치(Input Device), 출력 장치(Output Device), 기억 장치(Memory), 연산 장치(ALU, Arithmetic Logic Unit), 제어 장치(Control Unit)
    - 기본 구성 요소들
        - 입력 장치(Input Device)
            - 사용자가 데이터를 컴퓨터에 입력할 수 있도록 하는 장치
            - 키보드, 마우스, 터치스크린, 마이크, 스캐너, 카메라 등이 포함
            - 입력된 데이터는 기억 장치에 저장되거나 연산 장치에서 처리됨
        - 출력 장치(Output Device)
            - 컴퓨터가 처리한 데이터를 사용자에게 전달하는 장치
            - 모니터, 프린터, 스피커, 프로젝터 등이 포함
        - 기억 장치(Memory)
            - 데이터를 저장하고 유지하는 역할을 하는 장치
            - 주기억 장치(메인 메모리, RAM) 와 보조 기억 장치(스토리지) 로 나뉨
                - 주기억 장치 (Primary Memory, RAM)
                    - RAM(Random Access Memory): 실행 중인 프로그램과 데이터를 저장하는 임시 저장 공간
                    - ROM(Read-Only Memory): 부팅 시 기본적인 시스템 정보를 저장하는 읽기 전용 메모리
                - 보조 기억 장치 (Secondary Storage)
                    - 데이터를 영구적으로 저장하는 장치
                    - 하드 디스크(HDD), 솔리드 스테이트 드라이브(SSD), USB, CD/DVD 등이 포함
            - 예시
                - RAM: 프로그램 실행 속도와 멀티태스킹을 담당
                - HDD(하드 디스크 드라이브): 대용량 데이터를 저장하지만 속도가 느림
                - SSD(솔리드 스테이트 드라이브): HDD보다 빠른 저장 장치
                - USB 메모리: 휴대용 저장 장치
                - 클라우드 스토리지: 인터넷을 통해 데이터를 저장하는 공간 (예: Google Drive, Dropbox)
        - 연산 장치(ALU, Arithmetic Logic Unit)
            - 컴퓨터에서 모든 연산(산술 및 논리 연산)을 담당하는 핵심 부품
            - CPU(중앙처리장치) 내에 존재하며, 덧셈, 뺄셈, 곱셈, 나눗셈 같은 산술 연산과 AND, OR, NOT 같은 논리 연산을 수행
        - 제어 장치(Control Unit)
            - 컴퓨터의 모든 구성 요소를 조정하고 제어하는 역할을 하는 장치
            - 입력 → 연산 → 기억 → 출력 순서대로 명령을 처리하도록 지시
            - CPU의 일부이며, 프로그램 명령을 해석하고 실행
    - 컴퓨터의 동작 과정
        - 입력장치 통해 데이터 입력 받음
        - 제어 장치가 명령을 해석하고 실행 순서를 결정
        - 연산 장치가 계산 / 논리 연산 수행
        - 결과가 기억 장치에 저장
        - 최종 결과가 출력 장치를 통해 사용자에게 제공

- CPU의 기본 동작 사이클(Fetch-Decode-Execute)
    - 개요
        - CPU는 프로그램을 실행할 때 Fetch-Decode-Execute라는 기본적인 동작 사이클 수행
        - CPU가 명령어를 가져오고, 해석하고, 실행하는 과정
    - Fetch (명령어 가져오기)
        - CPU는 프로그램 카운터(PC)에 저장된 주소에서 다음에 실행할 명령어를 가져옴
        - 명령어는 메모리(RAM)에서 명령어 레지스터(IR, Instruction Register)로 로드
        - 프로그램 카운터의 값을 증가시켜 다음 명령어를 가리키도록 설정
        - 주요 역할
            - 명렁어를 메모리에서 읽어옴
            - PC(Program Counter)를 증가
    - Decode (명령어 해석)
        - CPU의 명령어 해독기(Instruction Decoder)가 명령어를 분석하여 어떤 연산을 수행할 지 해석
        - 명령어가 필요한 데이터(레지스터 또는 메모리)를 확인하고 연산을 수행할 장치를 결정
        - ALU(산술 논리 연산 장치) 또는 제어 장치로 필요한 정보를 전달
        - 주요 역할
            - 명령어를 분석, 필요한 연산 결정
            - 사용될 레지스터 또는 메모리를 확인
    - Execute (명령어 실행)
        - 명령어에 따라 실제 연산을 수행
            - 산술 연산(+,-,*,/) -> ALU 사용
            - 데이터 이동(메모리 <-> 레지스터) -> 레지스터 연산
            - 조건 분기(점프) -> 프로그램 카운터 변경
        - 연산 결과는 메모리 또는 레지스터에 저장
        - 필요한 경우, CPU는 인터럽트를 처리하거나 결과를 출력 장치로 보낼 수 있음
        - 주요 역할
            - 명령어를 실행하고 결과를 저장
            - 메모리, 레지스터, ALU를 이용하여 연산 수행
    - 반복되는 사이클
        - CPU가 프로그램을 실행하는 동안 지속적으로 반복
            - Fetch -> Decode -> Execute
            - 프로그램이 끝날 때까지 반복(PC가 종료 명령어를 가리킬 때 종료)
    - 예제 (CPU가 실행하는 과정)
        - 예를 들어 A = B + C 연산 수행될 때
            - Fetch: 메모리에서 A = B + C 명령어를 가져옴
            - Decode: A에 B + C를 저장하는 명령어임을 해석
            - Execute: B와 C 값을 ALU에서 더하고, 결과를 A에 저장
    - 추가 개념
        - 파이프라이닝 (Pipelining): Fetch, Decode, Execute 단계를 동시에 수행하여 CPU 성능을 향상
        - 캐시 메모리: Fetch 단계에서 캐시 메모리를 활용하여 명령어 접근 속도를 증가
        - 인터럽트(Interrupt): 실행 도중 이벤트가 발생하면 중단하고 특정 처리를 수행
    - 결론, 요약
        - Fetch: 메모리에서 명령어 읽기 (구성 요소: 프로그램 카운터(PC), 명령어 레지스터(IR))
        - Decode: 명령어 분석/해석 및 연산 준비 (구성 요소: 명령어 해독기(Decoder))
        - Execute: 연산 수행 및 결과 저장 (구성 요소: ALU, 레지스터, 메모리)

- 명령어 파이프라이닝(Instruction Pipelining)의 개념과 장단점
    - 개념
        - CPU에서 하나의 명령어를 처리하는 동안 다음 명령어를 동시에 실행하는 기술
        - 여러 개의 명령어를 중첩하여 실행함으로써 CPU의 성능을 향상시키는 기법
        - 비유
            - 전통적인 방식 > 빵 한개씩 완성한 후 다음 빵 만들기
            - 파이프라이닝 > 반죽, 굽기, 포장 단계 구분 후 여러 개의 빵을 동시에 제작
    - 동작 과정 (파이프라인의 단계)
        - 1단계: IF (Instruction Fetch), 명령어를 메모리에서 가져옴
        - 2단계: ID (Instruction Decode), 명령어를 해석하여 필요한 연산과 레지스터를 결정
        - 3단계: EX (Execute), 연산 수행(ALU 연산, 주소 계산 등)
        - 4단계: MEM (Memory Access), 메모리에서 데이터 읽기/쓰기 수행
        - 5단계: WB (Write Back), 연산 결과를 레지스터에 저장
        - 참고 사항
            - 1번째 명령어가 IF, 두번째는 ID, 세번째는 EX 단계에 있을 수 있음, 즉 여러개의 명렁어가 동시에 실행되면서 CPU 활용도 높임
    - 명령어 파이프라이닝의 장점
        - 성능 향상 (Throughput 증가)
            - 여러 개의 명령어를 동시에 처리하므로 전체적인 처리 속도가 증가함.
            - CPU가 한 번에 하나의 명령어만 처리하는 순차 실행 방식보다 빠름.
            - 예를 들어, 5단계 파이프라인이면 이론적으로 최대 5배의 성능 향상 가능.
        - CPU 활용도 증가
            - 파이프라인이 없으면 CPU의 일부 유닛(ALU, 메모리 등)이 한 번에 하나의 명령어만 실행하여 자원이 비효율적으로 사용됨.
            - 하지만 파이프라이닝을 사용하면 각 유닛이 연속적으로 동작하므로 CPU의 자원 활용도가 높아짐.
        - 프로그램 실행 시간 단축
            - 개별 명령어의 실행 시간은 변하지 않지만, 전체 프로그램의 실행 시간이 단축됨.
            - 여러 명령어가 동시에 실행되므로 CPU의 명령어 처리 속도(Instruction Throughput)가 증가
    - 명령어 파이프라이닝의 단점
        - 파이프라인 해저드 (Pipeline Hazards) 발생
            - 파이프라이닝 과정에서 여러 개의 명령어가 동시에 실행되므로, 서로 간섭할 수 있는 문제가 발생할 수 있음
            - 데이터 해저드 (Data Hazard)
                - 이전 명령어의 실행 결과가 다음 명령어에 필요할 때 발생
                - 해결법: 데이터 포워딩 (Data Forwarding), 파이프라인 버블 삽입 (Stall)
            - 제어 해저드 (Control Hazard)
                - 분기문(조건문, if, jmp)이 실행될 때 발생
                - 해결법: 분기 예측 (Branch Prediction), 지연 슬롯 (Delayed Branch)
            - 구조적 해저드 (Structural Hazard)
                - 하드웨어 자원이 충분하지 않아 발생 (예: 한 번에 여러 명령어가 메모리에 접근)
                - 해결법: 리소스 추가 (멀티포트 메모리 사용)
        - 복잡한 설계 (Implementation Complexity)
            - CPU 내부에서 각 단계가 동시에 실행되도록 설계해야 하므로 하드웨어 구현이 복잡함
            - 명령어 간의 동기화 문제가 발생할 수 있음
            - 따라서 단순한 마이크로컨트롤러에서는 사용하기 어려울 수 있음
        - 예외(Interrupt) 처리 어려움
            - 파이프라이닝 중간에 인터럽트(Interrupt)가 발생하면 모든 파이프라인을 중단해야 할 수도 있음
            - 특정 명령어가 실패하거나 예외가 발생하면 롤백(Rollback) 처리가 필요함
    - 결론
        - 파이프라이닝은 CPU의 성능을 크게 향상시키는 기술이지만, 해저드 문제를 해결해야 효율적으로 동작할 수 있음
        - 분기 예측(Branch Prediction), 데이터 포워딩(Data Forwarding) 등 다양한 기법과 함께 사용

- 슈퍼스칼라(Superscalar) 구조란 무엇이며, 장점과 단점
    - 개념
        - CPU에서 여러개의 명령어를 동시에 실행할 수 있도록 다중 실행 파이프라인을 제공하는 아키텍쳐
        - 단일 클럭 사이클(Clock Cycle) 내에서 여러 개의 명령어를 병렬로 실행할 수 있도록 설계된 CPU 구조
        - 비유
            - 기본적인 파이프라인 구조: 한 줄로 서서 음식 받는 줄
            - 슈퍼스칼라 구조: 여러 줄(멀티 카운터) 동시에 가능
    - 슈퍼스칼라의 동작 방식
        - 명령어 디코더(Instruction Decoder)를 통해 명령어를 분석하고, 여러 개의 실행 유닛(Execution Units)에 나눠서 실행
            - CPU가 여러 개의 실행 유닛(ALU, FPU 등)을 가지며, 가능한 경우 여러 개의 명령어를 동시에 실행
            - 명령어의 의존성(Dependencies)이 없는 경우에만 병렬 실행 가능
            - 현대적인 CPU는 명령어 발행(Instruction Issue), 명령어 재배열(Out-of-Order Execution), 분기 예측(Branch Prediction) 등을 활용하여 효율적으로 실행
    - 슈퍼스칼라와 파이프라이닝 차이
        - 파이프라이닝은 명령어 실행을 연속적으로 수행하지만, 슈퍼스칼라는 여러 개의 명령어를 병렬로 실행하여 더 빠르게 처리
    - 슈퍼스칼라의 장점
        - 성능 향상 (Throughput 증가)
            - 한 번에 여러 개의 명령어를 실행할 수 있기 때문에 처리 속도가 빨라짐
            - 단일 클럭 사이클에서 여러 개의 명령어를 동시에 완료할 수 있음
            - n개의 실행 유닛이 있다면, 최적의 경우 n배의 성능 향상이 가능
        - 효율적인 CPU 활용
            - CPU 내부의 ALU, FPU(부동소수점 연산 유닛), 로드/스토어 유닛 등의 자원을 더 효율적으로 사용할 수 있음
            - Out-of-Order Execution(명령어 재배열)과 Register Renaming(레지스터 리네이밍) 같은 기술을 활용하면 실행 효율을 높일 수 있음
        - 멀티스레드(Multithreading) 성능 향상
            - CPU의 여러 실행 유닛을 활용하여 멀티스레드 환경에서 성능이 향상됨
            - 고성능 서버, 게임 엔진, 데이터베이스 처리 등에 적합
        - 현대 CPU의 필수 기술
            - x86, ARM, RISC-V 등의 모든 최신 프로세서가 슈퍼스칼라 구조를 채택하고 있음
            - 예: Intel, AMD, Apple M 시리즈, ARM Cortex 등의 CPU
    - 슈퍼스칼라의 단점
        - 명령어 병렬성이 낮으면 효과 감소
            - 슈퍼스칼라는 독립적인 명령어를 동시에 실행할 때만 성능이 향상됨
            - 명령어 간 데이터 의존성이 높으면 병렬 실행이 어렵고 성능 향상이 제한됨
            - 예: 연속적인 a = b + c; d = a + e; 같은 코드에서는 병렬 실행이 어려움
                - a가 종속적 
        - 하드웨어 설계 복잡성 증가
            - 여러 개의 실행 유닛을 운영하기 위해 CPU 내부가 복잡해짐
            - 명령어를 분석하여 어떤 명령어를 동시에 실행할 수 있는지 결정하는 로직(Instruction Dispatch Logic)이 필요
            - 분기 예측(Branch Prediction) 실패 시 성능 저하
        - 전력 소모 증가
            - 여러 개의 실행 유닛이 동작하기 때문에 단순한 프로세서보다 전력 소비가 많음
            - 배터리 효율이 중요한 모바일 기기에서는 전력 최적화가 필요
        - 소프트웨어 최적화 필요
            - 일부 경우 컴파일러나 프로그래머가 명령어 병렬성을 높이는 코드 최적화가 필요
            - 예: 명령어 재배열(Instruction Reordering), 루프 펼치기(Loop Unrolling) 등을 통해 병렬성을 증가시켜야 함
    - 슈퍼스칼라 vs 슈퍼파이프라인(Superpipelining)
        - 슈퍼스칼라 (Superscalar) → 한 번에 여러 개의 명령어를 병렬 실행
        - 슈퍼파이프라인 (Superpipelining) → 파이프라인 단계를 더 세분화하여 클럭당 실행 속도를 증가
💡          - 최신 CPU는 둘 다 사용
                - Intel, AMD, ARM 프로세서는 슈퍼스칼라 + 슈퍼파이프라인을 결합하여 고성능을 달성
    - 결론
        - 슈퍼스칼라는 현대 CPU에서 필수적인 성능 향상 기법이지만, 명령어 병렬성을 확보해야 효과적
        - 최신 CPU는 슈퍼스칼라 + 파이프라이닝 + Out-of-Order Execution + 분기 예측을 조합하여 성능을 극대화

- 명령어 집합 구조(ISA, Instruction Set Architecture)란 무엇이며, 설계 시 고려해야 할 요소
    - 개념
        - 컴퓨터의 하드웨어와 소프트웨어 간의 인터페이스를 정의하는 명령어들의 집합
        - CPU가 실행할 수 있는 명령어들의 집합과 이들 명령어가 수행하는 동작을 규정하는 추상적인 명세
        - ISA는 주어진 프로세서가 지원하는 명령어 형식, 데이터 유형, 주소 지정 방식, 레지스터 구조 등을 포함
        - 동일한 ISA를 따르는 프로세서들은 같은 바이너리 명령어를 실행할 수 있으며, 이로 인해 소프트웨어의 이식성이 보장됨
    - ISA 설계 시 고려해야 할 요소
        - 개요
            - ISA를 설계할 때는 성능, 전력 소비, 구현 비용 등을 균형 있게 고려해여 함
        - 주요 설계 고려 요소
            - 명령어 형식(Instruction Format)
	            - 명령어의 길이(고정 길이 vs 가변 길이)
	            - 명령어 필드(Opcode, 피연산자 등)의 크기 및 배치
	            - 인코딩 방식 (CISC vs RISC)
                    - 예시
	                    - RISC(Reduced Instruction Set Computing): 고정된 길이의 명령어, 단순한 인코딩 방식
	                    - CISC(Complex Instruction Set Computing): 가변 길이 명령어, 복잡한 인코딩 방식
            - 주소 지정 방식(Addressing Modes)
                - CPU가 데이터를 메모리에서 읽거나 저장하는 방법을 정의하는 방식
	            - 즉시 주소 지정(Immediate Addressing): 명령어 내에 직접 값이 포함됨
	            - 레지스터 주소 지정(Register Addressing): 연산 대상이 레지스터에 저장됨
	            - 직접 주소 지정(Direct Addressing): 메모리 주소가 명령어에 직접 포함됨
	            - 간접 주소 지정(Indirect Addressing): 명령어가 가리키는 메모리 주소에 실제 주소가 저장됨
	            - 기타(Indexed, Base + Offset, PC-relative 등)
            - 데이터 유형(Data Types)
                - ISA가 지원하는 데이터 유형과 연산 방식을 정의
	                - 정수(Integer)
	                - 부동소수점(Floating Point)
	                - 벡터(Vector, SIMD)
	                - 기타 복합 데이터 유형(예: MMX, AVX 등)
            - 연산 유형(Operations)
                - CPU가 수행할 수 있는 기본 연산의 종류
                    - 산술 연산(ADD, SUB, MUL, DIV)
                    - 논리 연산(AND, OR, XOR, NOT)
                    - 비교 연산(CMP)
                    - 데이터 이동(Load/Store, Move)
                    - 흐름 제어(Jump, Branch, Call, Return)
                    - 특수 명령어(SIMD, 멀티스레딩 관련 명령어)
            - 레지스터 구조(Register Architecture)
                    - 일반 목적 레지스터(GPR, General Purpose Register) 개수
                        - 예: x86(8개), ARM(16개 이상), RISC-V(32개)
                    - 전용 레지스터(Special Purpose Register)
                        - 예: PC(Program Counter), SP(Stack Pointer), FP(Frame Pointer)
                    - 명령어 인코딩에서 레지스터의 위치
                    - RISC는 대부분 레지스터 간 연산을 기반으로 설계됨
            - 메모리 모델 및 관리(Memory Model & Management)
                - 메모리 정렬(Alignment)
                - 엔디언(Endianness, Little-endian vs Big-endian)
                - 메모리 보호 및 가상 메모리 지원 여부
                - 캐시 및 페이지 테이블 관리 방식
            - 입출력 방식(I/O Model)
                - 메모리 매핑 I/O (Memory-mapped I/O)
                - 포트 매핑 I/O (Port-mapped I/O)
                - 인터럽트 기반 I/O (Interrupt-driven I/O)
                - DMA(Direct Memory Access) 지원 여부
            - 명령어 실행 방식 및 성능 최적화
	            - 파이프라이닝(Pipelining): 명령어를 단계적으로 실행하여 성능 향상
	            - 슈퍼스칼라(Superscalar): 여러 명령어를 동시에 실행하는 구조
	            - VLIW(Very Long Instruction Word): 병렬 실행을 위해 긴 명령어를 사용하는 방식
	            - Out-of-Order Execution: 명령어를 순서에 관계없이 실행하여 성능 향상
	            - Branch Prediction: 분기 예측 기술로 성능 향상
            - 에너지 효율성 및 저전력 설계
	            - 모바일 및 IoT 기기에서는 저전력 소비가 필수
	            - 단순한 명령어와 연산 구조(RISC 계열 ISA) 활용
	            - 전력 소모를 줄이기 위한 명령어 집합 제공(예: ARM의 Thumb 모드)
            - 소프트웨어 및 호환성 고려
	            - 이전 세대 ISA와의 하위 호환성 (Backward Compatibility)
	            - 가상화 및 보안 기능 지원 (예: Intel VT-x, ARM TrustZone)
	            - 운영체제(OS) 및 컴파일러 지원 고려
    - 결론
        - ISA 설계는 성능, 전력 소비, 하드웨어 구현 비용, 소프트웨어 호환성 등의 요소를 균형 있게 고려해야 함
        - RISC와 CISC와 같은 철학적 차이뿐만 아니라, 데이터 유형, 메모리 모델, 주소 지정 방식, 파이프라이닝 등의 다양한 요소를 최적화하는 것이 중요
        - 최근의 ISA 설계는 병렬 처리, AI 가속, 저전력 소비 등의 특성을 반영하여 발전하고 있으며, 대표적인 ISA로는 x86, ARM, RISC-V 등이 존재


- 명령어 형식(Format)과 종류(Type)
    - 명령어 형식(Instruction Format)
        - 컴퓨터의 CPU가 해석하고 실행할 수 있도록 구성된 명령어의 구조를 의미
        - 명령어는 일반적으로 연산 코드(Opcode)와 피연산자(Operand)로 구성되며, 하드웨어 및 명령어 집합 아키텍처(ISA, Instruction Set Architecture)에 따라 다양한 형식이 존재

    - 명령어 형식의 기본 요소
	    - 연산 코드(Opcode):
	        - 수행할 연산을 지정하는 코드 (예: ADD, SUB, MOV 등)
	    - 피연산자(Operand):
	        - 연산에 사용될 데이터나 레지스터, 메모리 주소 등을 지정
	    - 주소(Addressing Mode, 선택적 요소):
	        - 데이터가 저장된 위치를 결정하는 방식
	    - 기타 제어 비트(Control Bits, 선택적 요소):
	        - 명령 실행을 위한 추가적인 정보 (예: 조건부 실행, 인터럽트 제어 등)

    - 명령어 형식(Format)의 종류
        - 고정 길이 명령어(Fixed-length Instruction)
	        - 명령어의 길이가 일정하게 고정된 구조
	        - 장점: 해석 속도가 빠르고, 하드웨어 설계가 단순함
	        - 단점: 메모리 사용 비효율 발생 가능
	        - 예: RISC 프로세서(ARM, MIPS), x86 32비트 명령어

        - 가변 길이 명령어(Variable-length Instruction)
	        - 명령어의 길이가 연산 종류와 피연산자의 수에 따라 달라지는 구조
	        - 장점: 메모리 공간을 효율적으로 사용 가능
	        - 단점: 명령어 해석이 복잡하고, 실행 속도가 느려질 수 있음
	        - 예: CISC 프로세서(x86, x86-64)

    - 명령어 종류(Instruction Type)
        - 데이터 전송 명령어(Data Transfer Instructions)
	        - 데이터를 이동하는 명령어 (레지스터 간, 메모리 간, I/O 장치 간 이동)
	        - 예시
	            - MOV R1, R2 → R2의 값을 R1으로 복사
	            - LOAD R1, [1000] → 메모리 주소 1000번지의 값을 R1으로 로드
	            - STORE R1, [1000] → R1의 값을 1000번지에 저장

        - 연산 명령어(Arithmetic and Logical Instructions)
            - 산술 및 논리 연산을 수행하는 명령어
            - 예시
                - ADD R1, R2, R3 → R2 + R3 결과를 R1에 저장
                - SUB R1, R2, R3 → R2 - R3 결과를 R1에 저장
                - AND R1, R2, R3 → R2와 R3의 AND 연산 결과를 R1에 저장
                - OR R1, R2, R3 → R2와 R3의 OR 연산 결과를 R1에 저장

        - 제어 명령어(Control Instructions)
            - 프로그램 실행 흐름을 변경하는 명령어 (분기, 점프, 조건문)
            - 예시
                - JMP 200 → 200번지로 무조건 점프
                - BEQ R1, R2, 100 → R1과 R2가 같으면 100번지로 점프
                - CALL func → 함수 호출
                - RET → 함수 종료 후 복귀

        - 입출력 명령어(Input/Output Instructions)
	        - 외부 장치와 데이터를 주고받는 명령어
	        - 예시
	            - IN R1, 300 → 300번 포트에서 데이터를 입력받아 R1에 저장
	            - OUT 300, R1 → R1의 값을 300번 포트로 출력

        - 시스템 명령어(System Instructions)
	        - 운영 체제와 관련된 명령어 (인터럽트, 특수 레지스터 제어)
	        - 예시
	            - INT 0x80 → 소프트웨어 인터럽트 호출 (리눅스 시스템 호출)
	            - HLT → CPU 동작 중지(프로그램 종료)

    - 결론
	    - 명령어 형식(Format): CPU가 처리하는 명령어의 구조로, 고정 길이 vs. 가변 길이가 있음.
	    - 명령어 종류(Type): 기능에 따라 데이터 전송, 연산, 제어, 입출력, 시스템 명령어로 나뉨.
	    - RISC(고정 길이) vs. CISC(가변 길이): CPU 구조에 따라 명령어 형식과 종류가 다름.

- 레지스터(Register)의 종류와 역할
    - 레지스터(Register) 정의
	    - CPU 내부에서 데이터 및 명령어를 임시로 저장하는 초고속 메모리
	    - 일반적인 RAM보다 훨씬 빠른 속도를 가지며, CPU가 직접 접근하여 연산을 수행하는 저장 공간
	    - 연산 과정에서 데이터 임시 저장, 명령어 실행, 주소 지정 등의 역할을 수행

    - 레지스터의 주요 종류와 역할
        - 범용 레지스터 (General-Purpose Register): GPR, 연산 및 데이터 저장을 위한 다목적 레지스터
        - 누산기 (Accumulator Register): AC, 연산 결과 저장 및 연산 수행
        - 메모리 주소 레지스터 (Memory Address Register): MAR, 메모리에서 읽거나 쓸 주소를 저장
        - 메모리 버퍼 레지스터 (Memory Buffer Register): MBR, 메모리에서 읽거나 쓴 데이터를 임시 저장
        - 명령어 레지스터 (Instruction Register): IR, 현재 실행 중인 명령어 저장
        - 프로그램 카운터 (Program Counter): PC, 다음 실행할 명령어의 주소 저장
        - 스택 포인터 (Stack Pointer): SP, 스택의 최상위 주소를 가리킴
        - 상태 레지스터 / 플래그 레지스터 (Status Register / Flag Register), SR / FR, 연산 결과에 대한 상태 정보 저장

    - 레지스터 상세 설명
        - (1) 범용 레지스터 (General-Purpose Register, GPR)
            - 개념
	            - CPU에서 임시 데이터 저장 및 연산 수행을 위한 다목적 레지스터
	            - 특정한 용도로 제한되지 않으며, 프로그래머가 자유롭게 사용 가능

            - 역할
	            - 덧셈, 뺄셈, 논리 연산 등의 중간 결과 저장
	            - CPU 내부에서 데이터 이동 및 연산 수행

            - 예시
	            - x86 아키텍처의 경우, EAX, EBX, ECX, EDX 등이 범용 레지스터로 사용됨
	            - ARM 아키텍처에서는 R0 ~ R15 레지스터가 범용 레지스터로 사용됨

        - (2) 누산기 (Accumulator Register, AC)
            - 개념
	            - 산술 연산(ALU 연산)의 결과를 저장하는 레지스터
	            - 과거에는 범용 레지스터가 부족하여 누산기를 별도로 사용
                - 현대 CPU에서는 범용 레지스터가 이 역할을 대신 수행하기도 함

            - 역할
	            - 연산(Arithmetic & Logical Unit, ALU) 수행 시 중간 결과 저장
	            - 빠른 연산을 위해 주로 사용

            - 예시
	            - 명령어 실행 예시:
                    - ADD A, B : A와 B를 더한 결과를 누산기에 저장

        - (3) 메모리 주소 레지스터 (Memory Address Register, MAR)
            - 개념 (주소 저장)
	            - 메모리에서 데이터를 읽거나 쓸 때 참조할 주소를 저장하는 레지스터
	            - CPU가 메모리와 통신할 때 사용됨

            - 역할
	            - 메모리 접근 주소 지정
	            - CPU가 특정 주소의 데이터를 가져오거나 저장할 때 주소값을 저장

            - 예시
	            - LOAD R1, (MAR)
	                - MAR에 저장된 주소에서 데이터를 읽어와 R1에 저장.

        - (4) 메모리 버퍼 레지스터 (Memory Buffer Register, MBR)
            - 개념
	            - 메모리에서 읽거나 저장할 데이터를 임시로 저장하는 레지스터
	            - MAR과 함께 동작하여 메모리와 CPU 간의 데이터 전송을 담당

            - 역할
	            - 메모리에서 읽어온 데이터 임시 저장
	            - 메모리에 쓸 데이터를 저장

            - 예시 (데이터 저장)
	            - MBR ← [MAR]
	                - MAR에 저장된 주소에서 데이터를 가져와 MBR에 저장.

        - (5) 명령어 레지스터 (Instruction Register, IR)
            - 개념
	            - 현재 실행 중인 명령어를 저장하는 레지스터
	            - CPU가 명령어를 해석(Decoding)하고 실행하는 과정에서 사용됨.

            - 역할
	            - CPU가 현재 실행할 명령어를 저장하고 해석
	            - 명령어 페치(Fetch) 후 실행

            - 예시
	            - 명령어 패치(Fetch): IR ← [PC]
	            - 명령어 해석(Decode): 명령어 분석 후 실행

        - (6) 프로그램 카운터 (Program Counter, PC)
            - 개념
	            - 다음 실행할 명령어의 주소를 저장하는 레지스터
	            - CPU가 실행할 명령어의 흐름을 제어하는 역할

            - 역할
	            - 명령어 실행 후, 자동으로 다음 명령어 주소 증가
	            - 분기문(Jump, Call) 실행 시, 특정 주소로 이동

            - 예시
	            - 일반적인 실행 흐름: PC ← PC + 1
	            - 분기 명령어 실행: PC ← 0x2000 (특정 주소로 점프)

        - (7) 스택 포인터 (Stack Pointer, SP)
            - 개념
	            - 스택(Stack) 구조의 최상위 주소를 가리키는 레지스터
	            - 함수 호출, 지역 변수 저장 등에 사용됨.

            - 역할
	            - 함수 호출 시 반환 주소 저장 (PUSH)
	            - 함수 종료 시 저장된 주소 복원 (POP)

            - 예시
                - PUSH R1 : 스택에 R1 값 저장 (SP 감소)
                - POP R1 : 스택에서 R1 값 복원 (SP 증가)

        - (8) 상태 레지스터 / 플래그 레지스터 (Status Register / Flag Register)
            - 개념
	            - 연산 수행 후의 상태를 나타내는 비트 플래그(Flags)를 저장하는 레지스터
	            - 연산 결과에 따라 Z(Zero), C(Carry), S(Sign), O(Overflow) 등의 상태 비트를 설정

            - 역할
	            - 연산 결과의 조건을 저장하여 프로그램 흐름 결정
	            - 분기(조건문) 실행 시 사용됨 (예: if문 역할)

            - 플래그 비트 예시
                - Z (Zero Flag): 연산 결과가 0이면 설정
                - C (Carry Flag): 덧셈 시 자리올림 발생 시 설정
                - S (Sign Flag): 연산 결과가 음수이면 설정
                - O (Overflow Flag): 오버플로우 발생 시 설정

            - 예시
                - CMP R1, R2 : R1과 R2 비교
                - JZ LABEL : Zero 플래그(ZF)가 설정되면 LABEL로 점프

    - 레지스터 종류별 역할 요약
        - 범용 레지스터: GPR, 데이터 저장 및 연산 수행
        - 누산기: AC, 연산 결과 저장
        - 메모리 주소 레지스터:	MAR, 메모리 접근 주소 저장
        - 메모리 버퍼 레지스터:	MBR, 메모리에서 읽거나 쓸 데이터 저장
        - 명령어 레지스터: IR, 현재 실행 중인 명령어 저장
        - 프로그램 카운터: PC, 다음 실행할 명령어 주소 저장
        - 스택 포인터: SP, 스택 최상위 주소 관리
        - 상태/플래그 레지스터: SR / FR, 연산 결과 상태 저장 (Zero, Carry, Overflow 등)

    - 결론
	    - 레지스터는 CPU 내부에서 가장 빠른 메모리이며, 연산 및 데이터 저장을 담당
	    - 각 레지스터는 특정 역할을 가지며, CPU가 효율적으로 작동할 수 있도록 지원
	    - 범용(GPR)과 특수 목적(SP, PC, IR 등) 레지스터가 함께 동작하여 명령어 실행과 데이터 처리를 수행함

- 산술논리연산장치(ALU)의 역할과 기능
    - 개념 및 역할
        - 산술논리연산장치(ALU, Arithmetic Logic Unit) 는 CPU 내에서 연산을 수행하는 핵심 부품으로, 컴퓨터 연산의 핵심 역할을 담당한다.
        - CPU가 처리하는 모든 산술 및 논리 연산이 이 장치에서 실행된다.
    - 주요 기능
        - 산술 연산 (Arithmetic Operations)
            - 덧셈, 뺄셈, 곱셈, 나눗셈 등의 연산 수행
        - 논리 연산 (Logical Operations)
            - AND, OR, NOT, XOR 등의 비트 단위 논리 연산 수행
        - 비교 연산 (Comparison Operations)
            - 두 데이터 값을 비교하고 결과를 플래그에 저장
        - Shift 연산
            - 비트 이동(Shift Left, Shift Right)을 통한 데이터 처리

- CPU 내부의 주요 레지스터의 역할
    - PC (Program Counter)
        - 다음에 수행할 명령어의 주소를 저장하는 레지스터. 명령어의 실행 순서를 관리한다.

    - IR (Instruction Register)
        - 메모리에서 인출된 현재 실행 중인 명령어를 임시 저장하는 레지스터.

    - MAR (Memory Address Register)
        - 메모리에 접근할 데이터의 주소를 임시 저장하는 레지스터.

    - MDR (Memory Data Register)
        - 메모리에서 읽거나 쓸 데이터를 임시 저장하는 레지스터.

    - ACC (Accumulator)
        - 산술 및 논리 연산의 결과를 저장하거나, 연산 시 데이터를 임시 저장하는 레지스터.

- 정수 연산과 부동소수점 연산의 차이점과 처리 방식
    - 정수 연산(Integer Operation)
        - 특징: 고정된 크기의 이진 정수 데이터로 처리, 연산 속도가 빠름.
        - 처리 방식: 덧셈, 뺄셈 등의 기본 산술 연산을 단순한 비트 연산으로 처리.
    - 부동소수점 연산(Floating-point Operation)
        - 특징: 매우 큰 수나 매우 작은 수, 소수를 정밀하게 표현 가능, 처리 속도는 느림.
        - 처리 방식: 지수(Exponent)와 가수(Mantissa)로 나누어 연산 수행(IEEE 754 표준을 따름).

- IEEE 754 부동소수점 표준의 구조와 특징
    - 구조
        - 부호(Sign bit) + 지수(Exponent) + 가수(Mantissa or Fraction)로 구성됨.
        - 예시(32비트 단정밀도 기준): [1비트 부호 | 8비트 지수 | 23비트 가수]
    - 특징
        - 정규화된 표현: 가수를 정규화하여 표현함으로써 숫자의 표현 범위를 확대
        - 지수 편향(Bias): 지수를 표현할 때 Bias를 사용하여 음수 표현을 없애고 양수로 표현
        - 단정밀도는 Bias가 127, 배정밀도는 1023
        - 특수값 표현: 무한대(infinity), NaN(Not a Number), 0 등을 정의하여 예외 상황 처리 가능

- Overflow와 Underflow의 개념과 발생 원인
    - Overflow (오버플로우)
        - 개념: 연산 결과가 CPU 레지스터가 표현 가능한 범위를 초과할 때 발생
        - 발생 원인: 덧셈/곱셈 등 연산의 결과 값이 자료형이 나타낼 수 있는 최대 범위를 초과할 때
    - Underflow (언더플로우)
        - 개념: 연산 결과가 표현 가능한 가장 작은 값 이하로 너무 작아 0으로 처리될 때 발생
        - 발생 원인: 부동소수점 연산 시 매우 작은 값끼리 곱하거나 나눌 때 발생

- Carry, Borrow, Overflow, Zero 플래그의 역할
    - Carry Flag (캐리 플래그)
        - 연산 시 발생하는 자리 올림(덧셈 시 최상위 비트에서의 캐리)을 표시하는 플래그.
        - 주로 무부호 정수 연산에서 사용.
    - Borrow Flag (보로우 플래그)
        - 뺄셈 연산에서 상위 비트로부터 빌려올 때 설정되는 플래그로, 캐리 플래그와 동일한 플래그를 공유하여 사용되기도 한다.
        - 주로 무부호 정수 연산의 뺄셈에서 발생.
    - Overflow Flag (오버플로우 플래그)
        - 연산 결과가 정수 자료형이 표현 가능한 최대 범위를 넘어서서 부호가 잘못 표현된 경우 설정됨.
        - 주로 부호 있는 정수 연산에서 사용됨.
    - Zero Flag (제로 플래그)
        - 연산 결과가 0이면 설정되는 플래그로, 조건 분기에서 자주 사용됨.
        - 연산의 결과 값이 0인지 아닌지 판단할 때 사용됨.

- CPU의 클럭 속도(Clock Speed)와 CPI(Cycles Per Instruction)의 관계
    - 개념 정의
        - 클럭 속도(Clock Speed):
            - CPU가 초당 처리할 수 있는 클럭 신호의 수로, 일반적으로 GHz(기가헤르츠) 단위로 표시됨.

        - CPI(Cycles Per Instruction):
            - 하나의 명령어를 수행하는 데 필요한 평균 클럭 사이클 수.
            - 하나의 명령어를 수행하는 데 클럭 사이클 수가 적으면 적을수록 좋다

    - 관계 및 성능 평가
        - CPU의 성능은 클럭 속도와 CPI의 조합으로 결정된다.
        - CPU 성능은 아래의 식으로 나타낼 수 있다:
            - CPU 실행 시간 = 명령어 수 × CPI × 클럭 주기(Clock cycle time)
        - 성능을 높이려면 클럭 속도를 높이거나, CPI를 낮추는 것이 효과적이다.

- 연산 속도를 높이기 위한 주요 기술
    - 파이프라이닝 (Pipelining)
        - 개념: 명령어 실행 과정을 여러 단계로 나누어 병렬로 수행하는 기술.
        - 장점:
            - 명령어 처리 속도 증가
            - CPU의 자원 활용률 향상
        - 단점:
            - 데이터 해저드, 제어 해저드로 인한 성능 저하 가능성 존재

    - 슈퍼스칼라 (Superscalar)
        - 개념: 한 클럭 주기 동안 여러 개의 명령어를 동시에 실행하여 처리 성능을 높이는 구조.
        - 장점:
            - 병렬 처리를 통한 명령어 처리율 향상
            - 파이프라인 병렬성 증대
        - 단점:
            - 복잡한 하드웨어 설계 필요
            - 명령어 의존성 관리 복잡

    - VLIW (Very Long Instruction Word)
        - 개념: 컴파일러가 명령어 레벨 병렬성(ILP)을 찾아 여러 개의 연산을 긴 명령어에 묶어 동시에 처리.
        - 장점:
            - 단순한 하드웨어 구조로 병렬 처리 구현 가능
            - 컴파일러 최적화로 높은 성능 확보
        - 단점:
            - 컴파일러의 의존도가 높고 명령어 코드 크기 증가

- RISC에서 Load/Store 아키텍처의 의미와 장점
    - 의미
        - Load/Store 아키텍처는 메모리에 직접 접근하는 명령어를 Load와 Store 두 가지로 제한하고, 나머지 연산은 모두 레지스터 간에서 수행하는 구조이다.
    - 장점
        - 명령어 집합 단순화로 빠른 명령어 처리 가능
        - 고정된 명령어 길이로 파이프라이닝 효율성 증대
        - 명령어 실행 시간 예측 가능성 향상

- VLIW(Very Long Instruction Word) 구조의 개념과 장점
    - 개념
        - VLIW는 하나의 매우 긴 명령어(예: 128비트, 256비트)에 여러 독립적인 연산을 함께 포함하여 동시에 실행하는 방식
    - 장점
        - 병렬성 활용 극대화 (명령어 수준 병렬성)
        - 하드웨어 설계가 상대적으로 단순하고 비용 효율적임
        - 컴파일러가 병렬성을 미리 분석하여 효율적 명령어 실행 보장

- 캐시 메모리(Cache Memory)의 역할과 동작 원리
    - 역할
        - CPU와 주기억장치 간 속도 차이로 인한 병목현상 완화
        - 자주 사용하는 데이터나 명령어를 미리 저장하여 접근 속도를 향상
    - 동작 원리
        - CPU가 데이터를 요청하면, 먼저 캐시 메모리를 확인
        - 캐시 히트(Cache Hit): 요청 데이터가 캐시에 존재하면 빠르게 처리
        - 캐시 미스(Cache Miss): 캐시에 데이터가 없으면 주기억장치에서 데이터를 로드하여 캐시에 저장하고 CPU에 제공

- 캐시 메모리의 매핑 방식의 차이점
    - 직접 사상(Direct Mapping)
        - 개념: 메모리의 각 블록이 캐시의 특정 위치에만 대응
        - 장점: 구현이 단순하고 저렴함, 빠른 검색 속도
        - 단점: 충돌로 인해 캐시 미스 발생 가능성 높음
    - 연관 사상(Associative Mapping)
        - 개념: 메모리의 블록이 캐시의 임의 위치에 저장 가능
        - 장점: 충돌 확률 낮아 히트율 향상
        - 단점: 검색 속도가 느리고 구현 복잡성 증가, 비용 상승
    - 집합 연관 사상(Set-Associative Mapping)
        - 개념: 캐시를 여러 개의 집합(Set)으로 나누고 각 메모리 블록이 특정 집합 내 임의 위치에 저장 가능
        - 장점: 직접 사상과 연관 사상의 장점 결합 (적당한 검색 속도와 높은 히트율)
        - 단점: 구조적 복잡성과 비용이 직접 사상보다 높음

- 캐시 히트(Cache Hit)와 캐시 미스(Cache Miss)의 개념과 영향
    - 캐시 히트(Cache Hit)
        - 개념: CPU가 요청한 데이터가 이미 캐시에 존재하여 빠르게 접근 가능한 상태
        - 영향:
            - 메모리 접근 시간이 줄어들어 성능이 향상됨
            - 시스템 효율성이 증가하고 CPU 대기 시간이 최소화됨
    - 캐시 미스(Cache Miss)
        - 개념: CPU가 요청한 데이터가 캐시에 없어 주기억장치에서 데이터를 읽어와야 하는 상태
        - 영향:
            - 메모리 접근 시간이 늘어나 시스템 성능이 저하됨
            - 캐시 미스 빈도가 높으면 시스템 성능이 현저히 떨어짐

- 미스 패널티(Miss Penalty)의 개념
    - 정의
        - 캐시 미스가 발생했을 때, 데이터를 주기억장치로부터 가져오는 데 소요되는 추가 시간이다.
    - 영향
        - 미스 패널티가 크면 성능이 크게 저하됨
        - 캐시 설계 시 미스 패널티를 줄이는 것이 매우 중요함 (예: 빠른 주기억장치, 다단계 캐시 사용 등)

- 캐시 교체 알고리즘의 종류와 특징
    - FIFO(First-In-First-Out)
        - 개념: 캐시에 가장 먼저 들어온 데이터를 가장 먼저 교체
        - 특징:
            - 구현이 간단하나, 최적의 교체가 아님
            - 사용 빈도를 고려하지 않기 때문에 효율이 상대적으로 낮음
    - LRU(Least Recently Used)
        - 개념: 가장 오랫동안 참조되지 않은 데이터를 교체
        - 특징:
            - 데이터 접근 패턴을 반영하여 성능이 우수함
            - 관리하기 위한 오버헤드가 상대적으로 큼 (구현 복잡성 증가)
    - LFU(Least Frequently Used)
        - 개념: 사용 빈도가 가장 낮은 데이터를 교체
        - 특징:
            - 데이터 사용 빈도 기반으로 효율적인 교체 가능
            - 초기 사용 빈도 집계 오차로 인해 잘못된 판단 가능성 있음
            - 구현 시 관리 오버헤드 존재

- 캐시 일관성(Coherency) 문제와 해결 방법
    - 캐시 일관성 문제의 개념
        - 멀티 프로세서 시스템에서 여러 개의 CPU가 자신만의 캐시를 가지고 있을 때, 동일 데이터에 대한 사본이 서로 달라지는 문제
    - 해결 방법
        - Write-through 방식
            - 캐시에 데이터를 쓸 때마다 주기억장치에도 즉시 기록하여 일관성 유지
        - Write-back 방식
            - 데이터를 캐시에만 기록하고 나중에 한꺼번에 주기억장치에 기록 (지연 기록)
        - MESI 프로토콜
            - Modified, Exclusive, Shared, Invalid 상태를 두어 데이터 일관성 유지
        - 스누핑(Snooping) 방식
            - 다른 CPU의 캐시 접근 요청을 지속적으로 관찰하며 일관성 유지

- 멀티레벨 캐시(L1, L2, L3)의 개념과 필요성
    - 개념
        - 캐시 메모리를 성능 및 크기에 따라 여러 계층(L1, L2, L3 등)으로 구성한 구조
        - L1 캐시: CPU 내부에 위치, 가장 작고 속도 빠름
        - L2 캐시: L1보다 크고 느리며, 보통 CPU 코어 내/외부에 존재
        - L3 캐시: 크기가 크고 가장 느림, 일반적으로 CPU 공유 캐시로 사용
    - 필요성
        - CPU와 메모리 간 속도 격차를 효율적으로 줄이기 위함
        - 자주 접근되는 데이터는 상위 캐시(L1)에, 덜 접근되는 데이터는 하위 캐시(L2, L3)에 배치하여 효율적 성능 관리
        - 미스 패널티 감소 및 성능 최적화

- 가상 메모리(Virtual Memory)의 개념과 동작 방식
    - 개념
        - 물리적인 메모리 크기를 넘어 가상 주소 공간을 제공하여 프로그램이 더 많은 메모리를 사용할 수 있게 하는 기술
        - 주기억장치(RAM)와 보조기억장치(디스크)의 공간을 가상적으로 연결하여 사용
    - 동작 방식
        - 페이지(Page) 단위로 메모리 관리
        - MMU(Memory Management Unit)를 통해 가상 주소를 물리 주소로 변환
        - 요청한 페이지가 주기억장치에 없을 때 페이지 폴트(Page Fault) 발생
        - 페이지 폴트 시 디스크에서 필요한 페이지를 로딩하여 주기억장치로 전송 (페이지 교체 발생)
        - 빈번한 페이지 폴트는 성능 저하를 일으키며, 이를 최소화하기 위해 페이지 교체 알고리즘(LRU, FIFO 등)을 사용하여 관리

- 페이지 테이블(Page Table)과 TLB의 역할
    - 페이지 테이블(Page Table)
        - 역할
            - 가상 메모리의 가상 주소를 실제 물리 주소로 변환하는데 사용되는 테이블이다.
        - 특징
            - 각 프로세스마다 별도의 페이지 테이블을 가진다.
            - 페이지 단위로 메모리를 관리하며, 메모리 관리 장치(MMU)가 페이지 테이블을 참조하여 주소 변환을 수행한다.
            - 주로 메모리에 저장되어 있으며, 대용량 메모리 환경에서 효율적이다.

    - TLB(Translation Lookaside Buffer)
        - 역할
            - 페이지 테이블 접근 속도를 높이기 위한 고속 캐시 메모리이다.
        - 특징
            - MMU 내부에 위치한 빠른 메모리로, 최근 사용된 주소 변환 결과를 저장한다.
            - 페이지 테이블 접근 횟수를 줄여 메모리 접근 성능을 크게 향상시킨다.
            - 히트율(TLB Hit rate)이 높을수록 시스템 성능이 향상된다.

- MMU(Memory Management Unit)의 역할과 동작 방식
    - MMU의 역할
        - CPU가 사용하는 가상 주소(Virtual Address)를 물리 주소(Physical Address)로 변환하는 하드웨어 장치이다.
        - 메모리 보호, 주소 변환 및 페이지 관리 역할을 수행한다.
    - MMU의 동작 방식
        - CPU가 가상 주소를 사용해 메모리에 접근할 때 MMU는 해당 주소를 물리 주소로 변환한다.
        - 이 과정에서 페이지 테이블과 TLB를 사용하여 빠르게 변환을 수행한다.
        - 변환 실패(페이지 폴트) 시 운영체제에 요청하여 페이지를 메모리로 로딩한다.
        - MMU는 접근 권한을 체크하여 허용되지 않은 접근 시 예외(Exception)를 발생시킨다.

- DRAM과 SRAM의 차이점
    - DRAM (Dynamic RAM)
        - 특징
            - 하나의 트랜지스터와 하나의 캐패시터로 구성.
            - 캐패시터에 전하를 저장하는 방식으로 데이터를 유지함.
            - 지속적인 Refresh(재충전) 필요.
        - 장점
            - 구조가 단순해 대용량 구현이 쉽고 가격이 저렴.
        - 단점
            - 속도가 상대적으로 느림.
            - Refresh 과정으로 전력 소비가 큼.
    - SRAM (Static RAM): 주로 캐시메모리는 SRAM으로 구성됨
        - 특징
            - 4~6개의 트랜지스터로 구성된 플립플롭 회로를 이용하여 데이터를 저장.
            - 전원이 공급되는 동안 데이터 유지 가능(Refresh 불필요).
        - 장점
            - 접근 속도가 매우 빠름.
            - Refresh가 없어 전력 소비가 적음.
        - 단점
            - 구조가 복잡하고 트랜지스터 수가 많아 제조 비용이 높음.
            - 같은 면적 대비 데이터 저장 밀도가 낮음.

- DRAM에서 Refresh가 필요한 이유
    - Refresh의 필요성
        - DRAM은 캐패시터에 전하를 저장하는 방식으로 데이터를 유지하는데, 시간이 지나면서 전하가 누설되어 데이터가 손실될 수 있다.
        - 이를 방지하기 위해 주기적으로 재충전(Refresh)하여 데이터를 유지한다.
    - 영향
        - Refresh 과정은 추가적인 전력 소모와 성능 저하를 초래한다.
        - 시스템 설계 시 Refresh 주기를 최적화하여 성능과 안정성을 유지한다.

- EEPROM과 Flash Memory의 차이점
    - EEPROM (Electrically Erasable Programmable ROM)
        - 특징
            - 전기적으로 데이터를 바이트 단위로 쓰기 및 지우기 가능.
            - 지우고 쓰는 속도가 느림.
        - 장점
            - 바이트 단위 접근이 가능하여 미세한 데이터 관리에 용이.
            - 데이터 보존성이 우수.
        - 단점
            - 쓰기 속도가 느리고 수명이 제한적임.
    - Flash Memory
        - 특징
            - 데이터를 블록 단위로 쓰기 및 삭제가 가능함.
            - EEPROM보다 쓰기 속도가 빠름.
        - 장점
            - EEPROM 대비 저렴하고 대용량 구현 가능.
            - 빠른 쓰기 속도로 널리 사용됨(SSD, USB, 스마트폰 등).
        - 단점
            - 블록 단위로만 삭제 가능해 세밀한 데이터 관리 어려움.
            - 반복적인 쓰기·지우기로 수명이 제한됨.

- HDD와 SSD의 구조적 차이와 장단점
    - HDD (Hard Disk Drive)
        - 구조
            - 자기 기록을 위한 회전하는 원형 플래터와 이를 읽고 쓰는 헤드로 구성.
        - 장점
            - 가격 대비 저장 용량이 크고, 데이터 복구 가능성이 상대적으로 높음.
        - 단점
            - 기계적인 회전 구조로 인해 소음과 진동 발생.
            - 데이터 접근 속도가 느리고 충격에 약함.
    - SSD (Solid State Drive)
        - 구조
            - NAND Flash 메모리 칩을 사용하여 데이터를 저장.
        - 장점
            - 기계적 움직임이 없어 소음과 진동이 없고, 데이터 접근 속도가 빠름.
            - 소비전력 적고 충격에 강함.
        - 단점
            - 같은 가격 대비 저장 용량이 상대적으로 작음.
            - 일정 횟수의 쓰기 횟수 제한(수명 제한)이 존재함.

- RAID의 개념과 다양한 RAID 수준 차이점
    - RAID(중복 배열 독립 디스크)의 개념
        - 여러 개의 디스크를 묶어 데이터의 성능, 신뢰성, 가용성을 높이는 기술.
        - 주요 목적은 성능 향상(병렬 처리), 데이터 안전성(중복성 확보).
    - 주요 RAID 수준별 차이점
        - RAID 0 (Striping)
            - 데이터를 여러 디스크에 나누어 병렬로 저장.
            - 장점: 속도 향상(읽기/쓰기 성능 우수)
            - 단점: 안정성 없음(하나라도 손상 시 전체 데이터 손실)
        - RAID 1 (Mirroring)
            - 데이터를 두 개의 디스크에 동시에 저장(복사본).
            - 장점: 데이터 안정성 및 복구 용이
            - 단점: 저장 용량 효율성 낮음(총 용량의 절반만 사용 가능)
        - RAID 5 (Striping with Parity)
            - 데이터를 여러 디스크에 분산 저장하고 패리티 정보를 생성하여 한 디스크 손상 시 복구 가능.
            - 장점: 효율적 용량 사용, 신뢰성과 성능 모두 양호
            - 단점: 패리티 계산으로 인한 쓰기 성능 저하, 디스크 복구 시 부하 발생
        - RAID 10 (1+0)
            - RAID 1과 RAID 0의 조합. 데이터를 미러링(중복)한 후 스트라이핑.
            - 장점: 빠른 성능과 높은 데이터 안정성 제공
            - 단점: 저장 효율이 50%로 낮고 비용이 높음(디스크가 많이 필요)

- 하드디스크의 주요 성능 지표
    - Access Time (접근 시간)
        - 데이터 요청 후 실제 데이터 읽기까지 걸리는 총 시간.
        - Access Time = Seek Time + Latency
    - Seek Time (탐색 시간)
        - 헤드가 원하는 데이터가 있는 트랙까지 이동하는 데 걸리는 시간.
    - Latency (지연 시간, 회전 지연 시간)
        - 헤드가 트랙에 도착한 후 디스크가 회전하여 데이터가 헤드 아래로 이동할 때까지 기다리는 시간.
        - 평균적으로 디스크 회전 속도에 따라 결정됨.
    - RPM (분당 회전 속도)
        - 디스크 플래터의 1분당 회전 횟수.
        - 높을수록 데이터 전송 속도가 빨라지고 Latency가 감소함.
        - 일반적으로 HDD는 5400rpm, 7200rpm, 고성능 HDD는 10000~15000rpm.

- 페이징(Paging)과 세그먼테이션(Segmentation)의 차이점
    - 페이징(Paging)
        - 물리 메모리와 가상 메모리를 동일 크기의 블록(페이지) 단위로 나누어 관리.
        - 내부 단편화 발생 가능성 존재.
        - 단순한 메모리 관리로 구현이 쉬움.
    - 세그먼테이션(Segmentation)
        - 가상 메모리를 논리적인 의미(코드, 데이터, 스택)에 따라 가변적인 크기의 세그먼트로 나누어 관리.
        - 내부 단편화는 없으나 외부 단편화가 발생 가능.
        - 보다 유연한 메모리 관리 가능.
    - 차이점 요약
        - 페이징: 고정 크기 블록, 내부 단편화 발생 가능, 관리가 쉽고 효율적.
        - 세그먼테이션: 가변 크기 블록, 외부 단편화 발생 가능, 논리적으로 의미 있는 단위로 관리 가능.

- 페이지 폴트(Page Fault)의 개념과 처리 방식
    - 개념
        - CPU가 접근하려는 페이지가 실제 메모리(RAM)에 존재하지 않아 접근 실패가 발생한 상태.
    - 페이지 폴트 발생 시 처리 과정
        - CPU가 접근한 페이지가 실제 메모리에 없는 경우 MMU가 예외(Page Fault) 발생.
        - 운영체제는 페이지 폴트 처리 루틴 실행.
        - 페이지 교체 알고리즘에 따라 보조기억장치(디스크)에서 해당 페이지를 메모리로 가져옴.
        - 페이지 테이블 갱신 후 다시 명령 실행.
        - 빈번한 페이지 폴트 발생 시 성능 저하 초래.

- Thrashing(스래싱)의 개념과 발생 원인
    - 개념
        - 지속적으로 페이지 폴트가 발생하여 메모리와 디스크 간 데이터 이동만 빈번히 일어나면서 시스템 성능이 크게 저하되는 현상.
    - 발생 원인
        - 메모리 부족: 너무 많은 프로세스나 페이지들이 메모리에 동시에 상주할 때.
        - 페이지 교체 알고리즘의 비효율성: 적절하지 않은 페이지 교체로 인해 자주 접근하는 페이지가 반복 교체됨.
    - 해결 방법
        - 메모리 증설 또는 가상 메모리 크기 확대.
        - 동시 실행 프로세스 개수 제한.
        - 효율적인 페이지 교체 알고리즘 채택(예: LRU 알고리즘)

- 메모리 단편화(Fragmentation)의 개념과 해결 방법
    - 내부 단편화(Internal Fragmentation)
        - 고정 크기의 메모리 블록 할당 시 실제 필요한 용량보다 큰 블록을 할당하여 메모리가 낭비되는 현상.
        - 주로 페이징에서 발생.
    - 외부 단편화(External Fragmentation)
        - 가변 크기 메모리 할당 시 작은 빈 공간들이 산재되어 있어 총 공간은 충분하지만 연속적인 큰 메모리 할당이 불가능한 현상.
        - 주로 세그먼테이션에서 발생.
    - 해결 방법
        - 내부 단편화 해결: 페이지 크기를 줄이거나, 페이지 크기를 다르게 하는 복합 페이징 방식.
        - 외부 단편화 해결: 메모리 압축(compaction) 기법(조각난 메모리를 하나로 모아 재배치), 페이징과 세그먼테이션을 혼합한 세그먼트 페이징 사용.

- 멀티코어 프로세서(Multi-Core Processor)의 개념과 장점
    - 개념
        - 한 개의 물리적인 CPU 안에 여러 개의 독립된 프로세서 코어를 포함한 프로세서.
        - 각각의 코어가 독립적으로 명령어를 처리하고 연산을 수행할 수 있음.
    - 장점
        - 병렬 처리 성능 향상: 여러 작업을 동시에 처리 가능.
        - 전력 소모 효율화: 클럭 속도를 높이지 않고도 성능 향상 가능, 전력 대비 효율이 뛰어남.
        - 발열 문제 완화: 단일 코어 대비 발열과 열 밀도가 낮아져 냉각이 쉬움.
        - 멀티태스킹 최적화: 여러 애플리케이션을 동시에 실행 시 성능 저하 최소화 가능.

- SMT(Simultaneous Multithreading)와 하이퍼스레딩(Hyper-Threading)의 차이점
    - SMT (Simultaneous Multithreading)의 개념
        - 한 개의 물리적 프로세서 코어 내에서 여러 개의 스레드를 동시에 실행하여 CPU 자원의 활용률을 높이는 기술.
    - 하이퍼스레딩(Hyper-Threading)의 개념
        - 인텔(Intel)이 개발한 SMT의 상용 기술로, 한 코어를 두 개 이상의 논리적 코어로 나누어 병렬 처리를 지원하는 방식.
    - 차이점
        - SMT: 일반적인 기술 개념(프로세서 자원의 동시 활용을 위한 다중 스레딩 기법)
        - 하이퍼스레딩: SMT 개념을 인텔이 상용화한 특정 브랜드 명칭으로, 실제로 프로세서를 논리적으로 나누어 운영체제에서 여러 코어로 인식하게 함.

- 다중 프로세서 시스템(Symmetric vs Asymmetric Multiprocessing)의 차이점
    - 대칭 다중처리(Symmetric Multiprocessing, SMP)
        - 모든 프로세서가 동등한 자격으로 메모리 및 I/O 자원을 공유하고, 각 프로세서가 모든 작업을 균등하게 수행 가능.
        - 단일 OS가 전체 프로세서를 통합 관리.
    - 비대칭 다중처리(Asymmetric Multiprocessing, AMP)
        - 프로세서마다 역할이 분리되어 있음. 특정 프로세서는 전용 작업(예: OS 관리)에 특화.
        - 각 프로세서가 수행하는 기능이 서로 다름.
    - 차이점 요약
        - SMP: 모든 프로세서가 동등한 권한과 역할을 가지고 효율적 부하 분산 가능.
        - AMP: 특정 프로세서에 역할을 한정하여 특정 기능의 효율을 높임.

- NUMA(Non-Uniform Memory Access) 구조의 개념
    - 개념
        - 여러 프로세서가 메모리를 공유하는 구조에서 각 프로세서가 접근하는 메모리 위치에 따라 접근 속도가 달라지는 구조.
        - 각 프로세서가 직접 연결된 메모리(local memory)는 빠르게 접근하지만, 원격 메모리(remote memory)는 상대적으로 느리게 접근.
    - 특징
        - 멀티프로세서 시스템에서 병목현상을 최소화하는 구조로 활용.
        - OS는 프로세서와 메모리 간 거리를 고려하여 최적화된 메모리 할당 및 스케줄링 필요.

- GPGPU(General-Purpose GPU)의 개념과 활용 사례
    - 개념
        - GPU의 강력한 병렬 처리 능력을 그래픽 처리 외에도 범용적인 계산 작업에 활용하는 기술.
    - GPU가 일반 CPU보다 유리한 이유
        - 수천 개의 코어로 구성되어 있어 병렬 연산 처리에 탁월함.
        - 부동소수점 연산에 매우 뛰어난 성능 제공.
    - 활용 사례
        - 딥러닝 및 인공지능 모델 학습
        - 과학적 계산 및 시뮬레이션(기상 예측, 유체 역학)
        - 이미지 및 비디오 처리(렌더링, 이미지 인식)
        - 금융 분야(주식 시뮬레이션, 리스크 관리)

- Flynn의 분류(SISD, SIMD, MISD, MIMD)
    - Flynn의 분류는 병렬 컴퓨팅 구조를 데이터 흐름과 명령어 흐름 기준으로 구분한 개념이다.
        - SISD (Single Instruction, Single Data)
            - 하나의 명령어가 하나의 데이터를 처리.
            - 전통적인 단일 프로세서 컴퓨터 구조.
        - SIMD (Single Instruction, Multiple Data)
            - 하나의 명령어가 여러 개의 데이터 요소를 동시에 처리.
            - 병렬 연산에 적합(예: GPU, 벡터 프로세서).
        - MISD (Multiple Instruction, Single Data)
            - 여러 개의 프로세서가 각각 다른 명령어로 같은 데이터를 처리.
            - 이론적 분류이며 실제 사례는 거의 없음. 주로 고신뢰성 병렬 처리에 개념적으로만 존재.
        - MIMD (Multiple Instruction, Multiple Data)
            - 여러 프로세서가 서로 다른 명령어로 서로 다른 데이터를 독립적으로 처리.
            - 다중 프로세서 컴퓨터 및 현대 멀티코어 시스템에서 가장 흔히 사용되는 구조.

- 멀티스레딩(Multithreading)의 개념과 장점
    - 개념
        - 멀티스레딩이란 하나의 프로세스 내에서 여러 개의 스레드(Thread)가 동시에 독립적으로 작업을 수행하는 방식이다.
    - 장점
        - 응답성 향상: 하나의 스레드가 대기 상태여도 다른 스레드가 작업을 계속 수행하여 시스템 응답성을 높임.
        - 자원 공유 용이: 같은 프로세스 내에서 메모리, 파일 등 자원을 쉽게 공유 가능.
        - 컨텍스트 스위칭 비용 절감: 프로세스보다 스레드 간의 전환이 훨씬 빠르고 비용이 적게 듦.
        - 병렬성 증가: 멀티코어 CPU에서 스레드를 병렬로 처리하여 성능 향상 가능.

- 병렬 프로세싱(Parallel Processing)에서 Amdahl’s Law
    - 개념
        - 암달의 법칙(Amdahl's Law)은 병렬 처리 시 프로그램의 최대 성능 향상도를 예측하는 공식이다.
    - 핵심 원리
        - 프로그램의 성능 향상은 전체 작업 중 병렬화 가능한 부분에 제한됨.
        - 아무리 많은 프로세서를 추가해도 병렬화되지 않는 직렬 부분이 존재하면 전체 성능 향상은 제한됨.
        - 성능 향상(속도향상비율) = 1 / [(1 - 병렬화 가능한 부분) + (병렬화 가능한 부분 / 프로세서 수)]
    - 의의
        - 병렬화 가능한 부분이 높을수록 병렬 처리를 통해 큰 성능 향상을 얻을 수 있음.
        - 병렬화를 무조건 늘리는 것이 아니라 병렬화되지 않는 부분(병목)을 줄이는 것이 중요함을 강조.

- 데이터 흐름 컴퓨팅(Data Flow Computing)의 개념
    - 개념
        - 데이터 흐름 컴퓨팅은 명령어가 실행되는 순서가 고정된 것이 아니라 데이터의 흐름에 따라 명령어가 동적으로 실행되는 방식이다.
    - 특징
        - 명령어는 데이터의 준비 여부에 따라 비동기적으로 실행됨.
        - 기존 폰 노이만(von Neumann) 아키텍처와 달리 명령어 실행 순서가 고정되지 않음.
        - 병렬성이 매우 높아 다중 프로세서 환경에서 성능 향상에 효과적임.
    - 활용 분야
        - 데이터 기반의 동시성 처리가 많은 병렬 프로세싱 환경
        - 대규모 병렬 계산 및 슈퍼컴퓨터 구조

- 분산 시스템(Distributed System)과 병렬 시스템(Parallel System)의 차이점
    - 분산 시스템(Distributed System)
        - 서로 독립적인 다수의 컴퓨터가 네트워크로 연결되어 하나의 시스템처럼 동작.
        - 지리적으로 분산된 환경에서 컴퓨팅 자원을 공유하고 협력.
        - 주 목적은 자원 공유, 신뢰성 향상, 확장성 강화.
    - 병렬 시스템(Parallel System)
        - 여러 프로세서나 코어가 밀접하게 연결된 시스템 내에서 동시에 작업 처리.
        - 작업의 수행 속도 향상을 위한 병렬 계산이 주 목적.
        - 일반적으로 동일한 위치에서 강력한 결합성을 가짐.
    - 핵심 차이점
        - 분산 시스템: 분리된 위치에서 자원 공유 및 협력 중심.
        - 병렬 시스템: 같은 위치에서 성능 향상 및 계산 효율 중심.

- 시스템 버스(System Bus)의 개념과 역할
    - 개념
        - 시스템 버스는 CPU, 메모리, 입출력 장치 등 컴퓨터의 주요 구성 요소를 연결하여 데이터와 신호를 주고받는 통로이다.

    - 주요 역할
        - 주소 버스(Address Bus): CPU가 데이터를 읽거나 쓸 메모리 위치를 지정.
        - 데이터 버스(Data Bus): CPU와 메모리 간 데이터를 전송.
        - 제어 버스(Control Bus): 구성 요소 간 제어 신호 전달(읽기/쓰기, 인터럽트, 타이밍 등).
    - 중요성
        - 시스템 내 데이터와 명령어의 이동을 원활하게 하여 성능에 직접적인 영향을 줌.

- 버스 중재 방식(Arbitration)의 종류와 특징
    - 버스 중재는 여러 장치가 동시에 버스 사용을 요청할 때 어떤 장치에 버스 접근 권한을 줄지 결정하는 방식이다.

    - 종류 및 특징
        - (1) 중앙집중식 중재(Centralized Arbitration)
            - 단일 중재자(Arbiter)가 존재
            - 중재자가 버스 접근 권한을 관리하고 배분함.
            - 구현이 간단하지만, 중재자가 병목 현상이 될 수 있음.
        - (2) 분산식 중재(Distributed Arbitration)
            - 각 장치가 독립적으로 중재
            - 장치 간에 서로 협상하거나 미리 정해진 프로토콜을 사용하여 결정.
            - 신뢰성이 높고, 병목이 없으나 구현이 복잡함.
        - (3) 고정 우선순위 방식(Fixed Priority)
            - 우선순위가 높은 장치가 항상 우선적으로 버스를 점유함.
            - 구현이 간단하지만 우선순위가 낮은 장치는 오래 기다려야 하는 문제가 있음.
        - (4) 순환 우선순위 방식(Rotating Priority)
            - 버스 접근 기회가 순서대로 장치 간에 회전하여 제공됨.
            - 모든 장치가 공평하게 접근 가능하지만 긴급 처리가 어려움.
        - (5) 데이지 체인(Daisy Chain)
            - 장치들이 직렬로 연결되어 있고, 가장 앞에 있는 장치부터 우선권이 주어짐.
            - 매우 간단한 구조이나 앞쪽 장치가 고장 시 뒤쪽 장치들이 접근 불가능해짐.

- PCI Express(PCIe)와 기존 PCI의 차이점
    - PCI (Peripheral Component Interconnect)
        - 병렬 전송 기반의 버스 구조
        - 모든 장치가 동일한 버스를 공유하며 대역폭도 공유
        - 동기식 전송 방식, 최대 대역폭이 제한됨
    - PCI Express (PCIe)
        - 직렬 전송 기반의 포인트 투 포인트(Point-to-Point) 구조
        - 각 장치가 독립적인 통신 채널을 갖기 때문에 병목 현상 줄어듦
        - 레인(Lane)을 이용한 확장성 제공 (x1, x4, x8, x16 등)
        - 동기/비동기 혼합 가능하며 전송 속도 훨씬 빠름
    - 주요 차이점 요약
        - 전송 방식: PCI는 병렬, PCIe는 직렬
        - 구조: PCI는 공유 버스, PCIe는 전용 링크
        - 확장성/속도: PCIe가 훨씬 우수함

- USB(Universal Serial Bus)의 특징과 동작 방식
    - 특징
        - 범용성: 하나의 인터페이스로 다양한 장치 지원
        - 핫 플러깅: 장치 연결/해제 시 시스템 재시작 불필요
        - 계층적 구조: 호스트-허브-장치 구조로 여러 장치 연결 가능
        - 전원 공급 가능: 전력 공급과 데이터 전송을 동시에 수행
    - 동작 방식
        - 호스트 중심 구조: 데이터 전송은 항상 호스트(PC)가 시작
        - 트랜잭션 기반 통신: IN, OUT, SETUP 패킷으로 통신 구성
        - 장치 인식 및 설정: 장치 연결 시 Enumeration 과정을 통해 ID와 드라이버 지정

- 직렬 전송(Serial Transmission)과 병렬 전송(Parallel Transmission)의 차이점
    - 직렬 전송
        - 데이터를 한 비트씩 순차적으로 전송
        - 전송선 수가 적어 노이즈가 적고, 장거리 전송에 유리
        - PCIe, USB, SATA 등에서 사용
    - 병렬 전송
        - 여러 비트를 동시에 전송
        - 단거리 전송에 유리하지만, 클럭 불일치(Skew)로 장거리 전송은 어려움
        - 구식 PCI, IDE(Parallel ATA) 등에서 사용
    - 핵심 차이
        - 직렬: 속도/거리 우위, 최신 기술 기반
        - 병렬: 구현 간단, 단거리 고속처리에 적합

- DMA(Direct Memory Access)의 개념과 장점
    - 개념
        - CPU의 개입 없이 입출력 장치가 메모리와 직접 데이터를 주고받는 방식
        - DMA 컨트롤러가 데이터 전송을 중계
    - 장점
        - CPU의 개입을 줄여 CPU 자원 절약
        - 전송 속도 향상
        - 시스템 전체 성능 개선
    - 활용 예시
        - 고속 입출력이 필요한 저장장치, 네트워크 카드 등

- 인터럽트(Interrupt)의 개념과 종류
    - 개념
        - 장치나 소프트웨어에서 이벤트가 발생했을 때 CPU의 실행 흐름을 일시적으로 중단하고 특정 루틴(핸들러)을 수행하는 방식
    - 종류
        - 하드웨어 인터럽트
            - 외부 장치(키보드, 디스크, 네트워크 등)에서 발생
            - CPU가 장치 요청에 빠르게 반응 가능
        - 소프트웨어 인터럽트
            - 프로그램 실행 중 명령어(INT 등)을 통해 발생
            - 시스템 호출, 예외 처리 등에 사용

- 인터럽트 벡터 테이블(Interrupt Vector Table)
    - 개념
        - 인터럽트 발생 시 해당 인터럽트의 처리 루틴(Handler)의 주소를 저장한 테이블
        - CPU는 인터럽트 발생 시 이 테이블에서 해당 인터럽트 번호에 해당하는 주소를 읽고 해당 루틴으로 점프
    - 특징
        - 일반적으로 메모리의 낮은 주소에 위치
        - 운영체제 또는 펌웨어가 초기화 시 설정
        - 인터럽트 핸들러의 진입점을 효율적으로 관리

- 메모리 매핑 방식: Memory-mapped I/O vs. Port-mapped I/O
    - Memory-mapped I/O
        - I/O 장치를 메모리 주소 공간의 일부처럼 할당하여 접근
        - CPU는 일반 메모리 접근 명령어(MOV, LOAD, STORE)를 통해 장치와 통신
        - 명령어 집합이 단순하며 메모리 주소 공간을 일부 소모함
        - RISC 구조 등에서 주로 사용
    - Port-mapped I/O
        - I/O 전용 명령어(IN, OUT 등)를 통해 장치 접근
        - I/O 공간이 메모리 주소 공간과 분리되어 있음
        - CISC 구조에서 주로 사용 (예: x86)
    - 차이점 요약
        - 메모리매핑은 명령어 호환성과 속도 측면에서 유리
        - 포트매핑은 주소 공간 절약에 유리

- I/O 방식: 폴링, 인터럽트, DMA
    - 폴링(Polling)
        - CPU가 주기적으로 장치를 검사하여 작업 수행
        - 구현이 간단하지만 CPU 자원 낭비가 큼
    - 인터럽트(Interrupt-driven I/O)
        - 장치가 작업 준비가 되면 CPU에 인터럽트 요청
        - CPU는 즉시 처리 루틴으로 전환
        - 효율적이지만 인터럽트 처리 비용 존재
    - DMA(Direct Memory Access)
        - 장치가 CPU 개입 없이 메모리와 직접 통신
        - CPU 부하 감소, 고속 데이터 전송에 유리
        - DMA 컨트롤러 필요

- 양자 컴퓨팅(Quantum Computing)의 개념과 기존 컴퓨터와의 차이점
    - 개념
        - 정보의 최소 단위를 비트 대신 큐비트(Qubit)로 표현
        - 큐비트는 중첩(Superposition)과 얽힘(Entanglement) 상태를 활용해 계산 수행
    - 기존 컴퓨터와의 차이
        - 기존 컴퓨터는 0 또는 1의 이진 상태만 표현
        - 양자 컴퓨터는 동시에 여러 상태를 표현할 수 있어 병렬 계산 능력이 탁월
        - 특정 문제(암호 해독, 최적화, 양자 시뮬레이션)에서 압도적 성능 기대

- 뉴로모픽 컴퓨팅(Neuromorphic Computing)
    - 개념
        - 인간의 뇌 신경망(뉴런, 시냅스) 구조를 모방한 컴퓨팅 구조
        - 비이진적, 비동기적 처리 중심
        - 신경망 기반 AI 처리에 특화된 아키텍처
    - 특징
        - 초저전력, 고속 처리, 병렬 처리 능력
        - 이미지 인식, 감지, 로보틱스 등 실시간 반응 요구되는 분야에 적합

- 엣지 컴퓨팅 vs. 클라우드 컴퓨팅
    - 엣지 컴퓨팅
        - 데이터 생성지 근처(센서, 기기)에서 처리
        - 지연(latency) 최소화, 실시간 반응성 필요 시 적합
        - IoT, 자율주행, 스마트 팩토리 등에서 활용

    - 클라우드 컴퓨팅
        - 중앙 집중형 데이터 센터에서 처리
        - 확장성, 자원 활용 최적화에 유리
        - 대규모 데이터 처리, 저장, 분석에 적합

    - 차이점 요약
        - 엣지는 속도 중심, 클라우드는 규모 중심

- NVMe vs. SATA SSD
    - NVMe (Non-Volatile Memory Express)
        - PCIe 기반 고속 인터페이스
        - 병렬 처리 최적화, CPU와 직접 통신
        - 매우 낮은 레이턴시와 높은 처리량 제공

    - SATA SSD
        - 전통적인 하드디스크 인터페이스(SATA) 사용
        - 속도 제한이 있으며, 병렬 처리에 약함
    
    - 핵심 차이
        - NVMe는 성능 중심, SATA는 비용 효율 중심
        - NVMe는 PCIe, SATA SSD는 SATA 인터페이스 사용

- HBM vs. GDDR의 차이점
    - HBM (High Bandwidth Memory)
        - 3D 스택형 구조로 다이를 위로 적층하고 TSV(Through-Silicon Via)를 통해 통신
        - 메모리와 GPU/CPU를 패키지 내에서 근접 배치하여 초고속 대역폭과 낮은 지연 제공
        - 에너지 효율이 우수하며 고성능 AI/ML, HPC(고성능 컴퓨팅)에 사용

    - GDDR (Graphics Double Data Rate)
        - 2D 평면 구조, 주로 그래픽 카드에 사용되는 고속 메모리
        - HBM보다 저렴하고 범용성이 높음
        - 단일 모듈당 대역폭은 낮지만, 병렬 확장이 용이

- 칩렛(Chiplet) 구조의 개념과 기존 단일 다이 설계의 차이점
    - 칩렛 구조
        - 여러 개의 소형 다이(Chiplet)를 패키지로 통합한 설계 방식
        - 기능별로 분리된 칩(예: CPU 코어, I/O, 메모리 컨트롤러 등)을 고속 인터커넥트로 연결

    - 기존 단일 다이
        - 모든 기능이 하나의 거대한 다이에 집적되어 설계 및 제조
    
    - 차이점
        - 칩렛은 제조 효율성, 설계 유연성, 수율 개선에 유리
        - 단일 다이는 고속 내부 통신에 유리하지만 크기가 커질수록 생산 난이도 증가

- TPU vs. GPU의 차이점
    - TPU (Tensor Processing Unit)
        - Google이 설계한 AI 전용 칩셋으로, 특히 딥러닝 추론 및 학습에 최적화
        - 행렬 곱셈, 텐서 연산에 특화된 고정 기능 가속기
        - 전력 효율과 처리 속도 측면에서 GPU보다 뛰어남 (특정 작업에 한정)

    - GPU (Graphics Processing Unit)
        - 범용 병렬 처리 장치로, AI 외에도 그래픽 렌더링, GPGPU 등 다양한 용도에 사용
        - 프로그래머블하고 유연하지만, 전용 하드웨어(TPU)보다 특정 AI 작업 속도는 낮을 수 있음

- AI 가속기 (NPU, Neural Processing Unit)의 개념과 역할
    - 개념
        - 인공지능 연산(특히 딥러닝)을 전용 하드웨어 회로로 처리하는 칩
        - 주요 기능은 행렬 곱셈, 컨볼루션, 정규화, 활성화 함수 등을 고속 병렬 수행
    - 역할
        - 스마트폰, IoT 디바이스, 엣지 컴퓨팅 등에서 로컬 AI 처리를 가능케 함
        - GPU/TPU 대비 저전력 설계, 실시간 처리, 경량 AI 모델 처리에 적합

- 사이드 채널 공격(Side Channel Attack)의 개념과 방어 기법
    - 개념
        - 암호 알고리즘 자체가 아닌 하드웨어 특성(전력, 시간, 전자파 등)을 분석하여 정보를 탈취하는 공격 방식
        - 예: 전력 분석(Power Analysis), 타이밍 분석, 캐시 분석

    - 방어 기법
        - 무작위화(Randomization): 연산 순서/타이밍 무작위화
        - 마스킹(Masking): 데이터에 랜덤 값을 섞어 처리
        - 정적 타이밍 사용: 시간 기반 공격 방지
        - 차폐(Shielding): 전자파 차단

- 최신 CPU 보안 강화 기술 (Meltdown, Spectre 방어 기법 등)
    - Meltdown & Spectre 개요
        - 투기적 실행(Speculative Execution)을 악용하여 권한이 없는 메모리 접근 가능
        - 캐시 타이밍을 통해 비인가 정보 유출 가능
    - 주요 방어 기술
        - KPTI (Kernel Page Table Isolation): 커널 주소공간을 분리하여 Meltdown 방어
        - Retpoline: 간접 분기 예측 차단으로 Spectre 방어
        - Microcode 업데이트: 하드웨어 수준 보안 패치
        - SMEP/SMAP: 사용자 공간에서 커널 영역 접근 차단

- Out-of-Order Execution(명령어 비순차 실행)의 개념과 장점
    - 개념
        - 프로그램 순서가 아닌 CPU 내부에서 의존성 없는 명령어를 우선 실행하는 방식
        - 데이터 의존성이 없는 명령어들을 먼저 실행시켜 유휴 시간 최소화
    - 장점
        - CPU 자원 활용도 향상 (파이프라인이 비는 시간 줄임)
        - 전체 성능 개선 (특히 병목이 있는 프로그램에서 효율적으로 작동)
        - 메모리 지연 등 외부 자원 대기 시간 중에도 실행 지속

- 데이터 흐름 아키텍처(Data Flow Architecture)의 개념과 특징
    - 개념
        - 명령어 실행을 데이터의 흐름에 따라 결정하는 아키텍처
        - 명령어는 입력 데이터가 준비되었을 때만 실행됨 (즉, 데이터가 흐르면 실행됨)
    - 특징
        - 완전한 병렬성 제공 가능 (명령 간 제어 흐름이 없음)
        - 명령어 간의 의존성만으로 실행 순서 결정
        - 제어 구조가 단순하지만 하드웨어 구현 복잡

- 브랜치 프레딕션(Branch Prediction)의 개념과 중요성
    - 개념
        - 조건 분기(예: if, for, while 등) 명령어가 있을 때 분기 여부를 미리 예측하여 명령어 파이프라인의 끊김을 방지하는 기법
    - 중요성
        - 파이프라인의 깊이가 깊어질수록 잘못된 예측 시 성능 손실이 커짐
        - 정확한 예측은 CPU 처리 속도 및 효율성에 직접적인 영향

- 정적(Static) vs. 동적(Dynamic) 브랜치 예측 기법의 차이점
    - 정적 브랜치 예측 (Static Branch Prediction)
        - 컴파일 시점에 분기 방향을 고정된 규칙에 따라 예측
        - 예: 항상 "분기 안 함"으로 예측
        - 하드웨어 부담이 적지만 정확도가 낮음

    - 동적 브랜치 예측 (Dynamic Branch Prediction)
        - 실행 중 CPU가 분기 결과를 학습하여 예측
        - 예: 1비트, 2비트 카운터, BHT(Branch History Table) 기반 예측
        - 실행 기록을 바탕으로 예측 정확도 높임
        - 하드웨어 복잡도는 증가하지만 성능 향상 효과 큼

- 투기 실행(Speculative Execution)이란 무엇인가?
    - 개념
        - 브랜치 명령 이후의 결과를 미리 예측하여 명령어를 실행하는 방식
        - 예측이 맞으면 그대로 진행하고, 틀리면 롤백(Rollback) 수행
    - 장점
        - 파이프라인 활용률을 극대화하여 성능 향상
        - 특히 Out-of-Order Execution 및 브랜치 예측과 결합 시 효과 극대화
    - 보안 측면
        - Meltdown, Spectre와 같은 취약점이 이 기술을 악용

- VLIW vs. EPIC의 차이점
    - VLIW (Very Long Instruction Word)
        - 여러 개의 연산을 하나의 긴 명령어로 구성하여 병렬 실행
        - 병렬 실행 명령어는 컴파일러가 정적으로 결정
    - EPIC (Explicitly Parallel Instruction Computing)
        - VLIW 개념을 기반으로 하되, 병렬성 정보와 명령 의존성까지 명시
        - 인텔의 Itanium 아키텍처에서 활용
        - 더 풍부한 병렬 실행 정보 제공, 성능 향상 가능
    - 차이점 요약
        - VLIW는 단순한 병렬 실행에 초점을 두고 컴파일러 의존성이 높음
        - EPIC은 명령 간 병렬성과 제약까지 포함한 명확한 실행 모델을 제시함

- CPU의 성능을 측정하는 주요 벤치마크(Benchmark) 지표
    - 주요 지표
        - 클럭 속도 (GHz): 초당 수행할 수 있는 사이클 수
        - CPI (Cycles Per Instruction): 명령어 1개당 평균 사이클 수
        - IPC (Instructions Per Cycle): 1사이클에 처리 가능한 명령어 수
        - MIPS (Million Instructions Per Second): 초당 처리 가능한 명령어 수
        - FLOPS (Floating Point Operations Per Second): 부동소수점 연산 성능
        - SPECint / SPECfp: 실제 프로그램 기반의 표준 벤치마크

- GPU와 CPU의 연산 처리 방식의 차이점
    - CPU (Central Processing Unit)
        - 소수의 고성능 코어로 일반적인 계산과 제어 흐름 처리
        - 복잡한 연산 처리에 최적화 (직렬 연산, 분기 등)
        - 다양한 캐시 계층과 분기 예측 기능 내장

    - GPU (Graphics Processing Unit)
        - 수천 개의 단순한 코어로 대량 병렬 연산 처리에 최적화
        - SIMD 방식 기반으로 행렬, 벡터, 이미지, AI 연산 등 대규모 병렬 연산에 강점
        - 복잡한 제어보다 데이터 중심 처리에 특화

- SIMD vs. MIMD의 차이점
    - SIMD (Single Instruction Multiple Data)
        - 하나의 명령어로 여러 데이터를 동시에 처리
        - 병렬 데이터 처리에 효율적
        - 예: GPU, 벡터 프로세서, SSE/AVX 명령어

    - MIMD (Multiple Instruction Multiple Data)
        - 각각 독립적인 명령어와 데이터를 처리하는 완전한 병렬 구조
        - 다중 프로세서 환경에서 사용됨 (예: 멀티코어 CPU, 클러스터 시스템)

- ILP vs. TLP의 차이점
    - ILP (Instruction Level Parallelism)
        - 한 쓰레드 내에서 명령어 단위의 병렬성을 극대화
        - 파이프라이닝, 슈퍼스칼라, Out-of-Order 등의 기술로 구현
    
    - TLP (Thread Level Parallelism)
        - 여러 쓰레드를 동시에 실행하여 병렬성 확보
        - 멀티코어, SMT, 하이퍼스레딩 등의 기술로 구현
        - ILP는 미세 병렬성, TLP는 거시 병렬성에 해당

- 캐시 프리페칭(Cache Prefetching)의 개념과 장점
    - 개념
        - CPU가 메모리에 접근하기 전, 앞으로 필요할 것으로 예상되는 데이터를 미리 캐시에 로드
    - 장점
        - 캐시 미스 확률 감소
        - 메모리 접근 지연 시간 감소
        - 전반적인 명령어 처리 속도 향상

- 캐시 코히어런시(Cache Coherency) 유지 기법
    - 필요성
        - 멀티코어 환경에서 동일한 데이터가 여러 캐시에 존재할 수 있으며, 이로 인해 데이터 불일치 문제가 발생
    - 유지 기법
        - Bus Snooping: 모든 캐시가 버스를 감시하여 데이터 변경 여부 탐지
        - Directory-based Coherence: 중앙 디렉터리가 데이터 상태와 위치를 관리
        - MESI 프로토콜: 캐시 블록 상태를 관리하여 일관성 유지

- MESI 프로토콜의 개념과 역할
    - 개념
        - 캐시 블록의 상태를 4가지 상태로 관리하는 일관성 프로토콜
            - M (Modified): 캐시에만 존재하며 메모리와 불일치
            - E (Exclusive): 캐시에만 존재하지만 메모리와 일치
            - S (Shared): 여러 캐시에 존재하며 메모리와 일치
            - I (Invalid): 유효하지 않은 상태

    - 역할
        - 멀티코어 시스템에서 캐시 데이터의 일관성 유지
        - 데이터 불일치 최소화, 성능 저하 방지

- NUMA vs. UMA의 차이점
    - UMA (Uniform Memory Access)
        모든 프로세서가 동일한 접근 시간으로 메모리에 접근
        메모리 공유 방식으로 단순한 설계
        SMP(Symmetric Multiprocessing) 시스템에서 사용
    - NUMA (Non-Uniform Memory Access)
        - 각 프로세서에 로컬 메모리가 있고, 다른 프로세서의 메모리에 접근 시 지연 시간 증가
        - 메모리 접근 속도가 위치에 따라 다름
        - 대규모 멀티코어 시스템에서 확장성 향상

- 가상 메모리 주소 변환 중 TLB 미스 처리 방식
    - 주소 변환 과정
        - CPU가 가상 주소 생성
        - TLB에서 해당 주소의 물리 주소 매핑 확인
        - TLB 히트 시 → 물리 주소 바로 사용
        - TLB 미스 시 → 페이지 테이블을 참조하여 변환 수행 후 TLB에 업데이트
    - 처리 방식
        - 하드웨어 방식: MMU가 직접 페이지 테이블을 참조
        - 소프트웨어 방식: 커널이 TLB 미스 예외를 처리하고 매핑 삽입

- 세그먼트(Segment) vs. 페이지(Page)
    - 세그먼트
        - 논리적 단위: 코드, 데이터, 스택 등으로 구분
        - 가변 크기의 메모리 블록
        - 보호 및 접근 권한 부여에 유리
    - 페이지
        - 고정 크기의 메모리 블록 (ex: 4KB)
        - 메모리 단편화 최소화
        - 단순한 주소 변환과 효율적인 메모리 관리

- TLB의 동작 방식과 성능 향상 기법
    - 동작 방식
        - 캐시 역할을 하는 고속 메모리로, 가상 주소 → 물리 주소 매핑 저장
        - MMU가 가상 주소 요청 시 TLB 먼저 확인
        - 히트 시 변환 시간 단축, 미스 시 페이지 테이블 참조
    - 성능 향상 기법
        - TLB 크기 증가
        - 다단계 페이지 테이블과 병행 사용
        - TLB prefetching 기술
        - 하드웨어-소프트웨어 협력 미스 처리

- 캐시 메모리 쓰기 정책: Write Through vs. Write Back
    - Write Through
        - 데이터를 캐시와 메모리 동시에 기록
        - 장점: 일관성 유지 용이
        - 단점: 쓰기 속도 저하, 메모리 접근 빈도 증가
    - Write Back
        - 데이터를 캐시에만 먼저 저장, 나중에 메모리에 반영
        - 장점: 성능 향상, 버스 트래픽 감소
        - 단점: 복잡한 제어 필요, 데이터 손상 위험 존재

- 페이지 교체 알고리즘 비교
    - LRU (Least Recently Used)
        - 가장 오래 전에 사용된 페이지 교체
        - 장점: 현실적인 접근 패턴 반영
        - 단점: 구현 복잡도 높음
    - FIFO (First-In First-Out)
        - 가장 먼저 들어온 페이지 교체
        - 장점: 구현 간단
        - 단점: 참조 패턴 무시 → Belady's Anomaly 발생 가능
    - LFU (Least Frequently Used)
        - 사용 빈도가 가장 낮은 페이지 제거
        - 장점: 장기 미사용 페이지 제거
        - 단점: 오래됐지만 과거에 많이 사용된 페이지도 제거될 수 있음
    - Optimal
        - 앞으로 가장 멀리 참조될 페이지를 교체
        - 장점: 이론상 가장 적은 페이지 폴트
        - 단점: 미래의 접근을 알아야 하므로 실사용 불가, 비교 기준으로 사용

- Working Set Model (작업 집합 모델)
    - 개념
        - 특정 시간 구간 내에 프로세스가 자주 참조하는 페이지들의 집합을 의미.
        - 시간 간격 Δ 동안 참조된 페이지들의 집합을 작업 집합이라고 부름.
    - 역할
        - 스래싱(Thrashing)을 방지하는 데 활용.
        - 각 프로세스의 작업 집합을 유지하면 필요한 메모리 양을 예측할 수 있음.
        - 작업 집합 기반으로 메모리 프레임을 할당하여 효율적인 자원 관리 가능.

- Amdahl’s Law vs. Gustafson’s Law
    - Amdahl’s Law
        - 고정된 문제 크기를 기준으로 병렬화의 한계를 설명.
        - 전체 시스템 성능은 병렬화가 안 되는 부분에 의해 제한됨.
        - 병렬화된 부분이 많아져도, 순차적인 부분이 성능 향상의 병목이 됨.
    - Gustafson’s Law
        - 문제 크기를 증가시킴으로써 병렬 시스템의 성능 향상이 가능하다는 이론.
        - 실용적인 병렬 컴퓨팅 환경에서는 문제 크기를 키워 병렬 성능을 활용할 수 있음.

- SIMD(벡터 프로세싱) vs. MIMD(멀티프로세서)
    - SIMD (Single Instruction Multiple Data)
        - 하나의 명령어로 여러 데이터를 동시에 처리.
        - GPU, 벡터 프로세서에서 사용.
        - 데이터 병렬 처리에 적합, 예: 이미지 필터링, 행렬 연산
    - MIMD (Multiple Instruction Multiple Data)
        - 각 프로세서가 다른 명령어, 다른 데이터를 처리.
        - 다중 코어 CPU, 클러스터 환경에서 사용.
        - 보다 범용적인 병렬 처리 방식.

- TLP(Thread-Level Parallelism) vs. DLP(Data-Level Parallelism)
    - Thread-Level Parallelism (TLP)
        - 여러 스레드가 동시에 실행되며 병렬성을 높임.
        - 예: 웹 서버에서 다중 요청 처리, 멀티스레딩 애플리케이션
    - Data-Level Parallelism (DLP)
        - 동일한 작업을 여러 데이터에 적용하는 병렬 처리.
        - SIMD, GPU 기반 연산에서 활용.
        - 예: 대규모 배열 계산, 벡터 연산

- 다중 프로세서 시스템에서 캐시 일관성 유지 방법
    - 문제점
        - 각 프로세서의 캐시가 동일 데이터를 다르게 유지할 수 있음 → 일관성(Coherency) 문제
    - 해결 기법
        - MESI 프로토콜: Modified, Exclusive, Shared, Invalid 상태로 구분해 일관성 유지
        - Bus Snooping: 각 캐시가 버스를 감시해 동기화
        - Directory-Based Protocol: 중앙 디렉터리가 캐시 상태를 추적하여 제어

- 멀티코어에서 공유 메모리 접근 동기화 방법
    - 주요 동기화 문제
        - Race Condition, Deadlock, Live Lock 등
        - 동시에 공유 자원 접근 시 데이터 정합성 문제 발생
    - 해결 방법
        - 뮤텍스(Mutex): 상호 배제를 통한 단일 접근 제어
        - 세마포어(Semaphore): 카운터 기반 제어
        - 모니터(Monitor): 조건 변수 + 뮤텍스
        - Atomic Operation: CPU 명령 수준의 동기화 (compare-and-swap, test-and-set)
        - Memory Barrier(Fence): 명령 실행 순서 강제 제어

- Spin Lock과 Mutex의 차이점
    - Spin Lock
        - 락을 획득할 때까지 CPU를 쉬지 않고 반복(Spin)하면서 기다리는 방식.
        - 락이 짧은 시간 내에 해제될 것이라 예상되는 경우에 유리.
        - 컨텍스트 스위치가 발생하지 않음 → 성능 유지 가능.
        - 하지만 CPU 자원을 소모하며 기다리기 때문에 장기 대기에는 비효율적.
    - Mutex (Mutual Exclusion)
        - 락을 획득하지 못하면 스레드가 잠들고, 스케줄러에 의해 재활성화됨.
        - CPU 리소스를 절약하지만 컨텍스트 스위치 오버헤드 존재.
        - 장기 대기 상황에서 유리함

- OpenMP vs MPI
    - OpenMP
        - 공유 메모리 기반 병렬 프로그래밍 라이브러리.
        - C/C++/Fortran 코드에 프라그마(pragma) 형태로 병렬화 추가.
        - 멀티코어 CPU 환경에 적합하며, 프로세스 간 데이터 공유가 쉬움.
    - MPI (Message Passing Interface)
        - 분산 메모리 기반 병렬 처리 프로그래밍 모델.
        - 각 프로세스는 자신의 메모리를 갖고 메시지를 주고받음.
        - 클러스터, 슈퍼컴퓨터, 고성능 컴퓨팅(HPC)에 적합

- Hyper-Threading의 개념과 장단점
    - 개념
        - 하나의 물리적 코어를 두 개의 논리적 스레드처럼 동작시켜 자원을 효율적으로 사용하는 기술 (인텔 기반).
        - ALU, FPU 등은 공유하되, 레지스터와 명령어 큐 등을 분리해 스레드 수준 병렬성(TLP) 향상.
    - 장점
        - 유휴 자원 활용 증가 → 성능 향상.
        - 낮은 비용으로 병렬 처리 성능 개선.
    - 단점
        - 자원 경쟁에 의해 성능 저하 가능.
        - 모든 워크로드에서 효과적인 것은 아님.

- Thread Affinity (스레드 결속)의 개념과 장점
    - 개념
        - 특정 스레드를 특정 코어에 고정하여 실행하도록 지정하는 방법.
    - 장점
        - 캐시 지역성(Locality) 확보 → 캐시 히트율 증가.
        - 컨텍스트 스위치 감소.
        - NUMA 환경에서 메모리 접근 효율 향상

- NUMA 시스템에서 성능 최적화 방법
    - 주요 전략
        - 메모리 로컬리티 유지: 스레드와 메모리를 동일 노드에 배치.
        - NUMA-aware 스케줄링: OS 레벨에서 로컬 메모리 우선 사용하도록 설정.
        - 페이지 마이그레이션 방지: 프로세스 이동 시 메모리도 같이 이동.
    - 도구 및 방법
        - numactl, taskset, hwloc 등으로 제어 및 튜닝 가능.

- MIPS vs ARM 아키텍처의 차이점
    - 공통점
        - 모두 RISC(Reduced Instruction Set Computer) 기반 아키텍처.
    - 차이점
        - MIPS
            - 교육용 및 네트워크 장비에서 주로 사용.
            - 비교적 단순한 명령어 구조.
        - ARM
            - 모바일, 임베디드, IoT, 최근에는 서버 시장까지 확장.
            - 전력 효율성이 뛰어나고 다양한 확장 기능 존재 (TrustZone, SIMD 등).
    - 요약
        - ARM은 저전력 고성능 시장에서 우세하며, 에코시스템도 넓음.
        - MIPS는 소형 기기 및 교육 목적에 더 적합.

- x86과 ARM의 설계 철학과 차이점
    - x86 (인텔/AMD 계열)
        - CISC 기반 설계 철학: 복잡한 명령어를 한 번에 처리.
        - 명령어 길이가 가변적이고 복잡함.
        - 전통적으로 데스크탑/서버에서 사용.
        - 고성능 위주, 복잡한 명령어 디코딩 및 마이크로코드 사용.
    - ARM
        - RISC 기반 설계 철학: 단순하고 짧은 명령어로 빠른 처리.
        - 명령어 길이 고정, 디코딩이 간단.
        - 모바일, 임베디드, IoT에 적합.
        - 전력 효율 우수, 점점 서버/클라우드 시장으로 확장 중.

- CISC vs RISC의 차이점 및 대표 아키텍처
    - CISC (Complex Instruction Set Computing)
        - 명령어 개수 많고 복잡, 하나의 명령어가 여러 연산 수행 가능.
        - 하드웨어 복잡성↑, 메모리 접근이 자유로움.
        - 대표 아키텍처: x86, VAX, IBM System/360.
    - RISC (Reduced Instruction Set Computing)
        - 단순한 명령어, 대부분 한 사이클 내 수행.
        - 하드웨어 단순성↑, 파이프라이닝 및 병렬 처리에 유리.
        - 대표 아키텍처: ARM, MIPS, SPARC, RISC-V.

- ARM의 Thumb 모드
    - Thumb 모드는 ARM의 16비트 압축 명령어 세트를 사용하는 실행 모드.
    - 코드 크기 절감 및 메모리 대역폭 절약에 효과적.
    - 성능은 약간 저하되지만, 임베디드 시스템에서 전력 효율성 확보에 유리.
    - 일반적인 ARM 명령어(32비트)와 병행 사용 가능.
    
- PowerPC, SPARC, MIPS의 특징 비교
    - PowerPC
        - IBM, Motorola, Apple이 공동 개발한 RISC 기반 아키텍처.
        - 고속 처리와 병렬 연산에 강함.
        - 산업, 임베디드 시스템 및 일부 서버에 사용됨.
    - SPARC (Scalable Processor ARChitecture)
        - 선 마이크로시스템즈가 개발.
        - 확장성과 병렬 처리에 강함, 서버/워크스테이션 위주.
        - 윈도우 레지스터 구조 등 독특한 설계.
    - MIPS
        - 교육 및 임베디드 환경에 적합한 단순 RISC 구조.
        - 일관된 명령어 포맷과 레지스터 기반 처리로 학습 및 설계 용이.
        - 점차 ARM 등으로 대체되는 추세.

- Superscalar vs VLIW의 차이점
    - Superscalar
        - 하드웨어가 명령어 간의 병렬성 분석 후 동시 실행.
        - 동적 명령어 발행 (out-of-order 가능).
        - 제어 논리 복잡하지만 범용성이 높음.
    - VLIW (Very Long Instruction Word)
        - 컴파일러가 병렬 실행 가능한 명령어를 묶어 고정된 형식으로 배치.
        - 실행 유닛이 많아도 제어는 단순.
        - 성능은 컴파일러 최적화 수준에 의존.

- Microinstruction vs Macroinstruction의 차이
    - Microinstruction (미시 명령어)
        - CPU 내부 제어 유닛이 해석한 하드웨어 수준의 제어 명령어.
        - 레지스터 이동, ALU 연산 등 단위 동작 제어 목적.
        - 마이크로코드(Microcode)로 저장됨.
    - Macroinstruction (매크로 명령어)
        - 어셈블리 언어에서 사용되는 고수준 명령어 묶음.
        - 반복되는 코드 구조를 하나의 매크로로 정의 가능.
        - 프로그래머 생산성 향상에 도움.

- Out-of-Order Execution vs In-Order Execution
    - In-Order Execution (순차 실행)
        - 명령어가 작성된 순서대로 실행.
        - 파이프라인이 단순하지만, 데이터 종속성이나 캐시 미스가 발생하면 대기해야 함.
        - 실행 유닛이 놀게 되는 비효율 발생 가능.
    - Out-of-Order Execution (비순차 실행)
        - 가능한 명령어를 순서에 관계없이 먼저 실행.
        - 레지스터 리네이밍, 명령어 재정렬, 동적 스케줄링 등을 통해 병목 감소.
        - 파이프라인 활용률 향상 → 성능 개선.
        - 구현 복잡도는 높음.

- SIMD 아키텍처에서 벡터 레지스터의 역할
    - SIMD(Single Instruction, Multiple Data)는 하나의 명령어로 여러 데이터를 병렬 처리하는 방식.
    - 벡터 레지스터는 다수의 데이터 요소를 저장하고 한 번에 연산하는 역할.
    - 예: 128비트 레지스터에 4개의 32비트 정수를 담아 동시에 덧셈 수행.
    - 멀티미디어, 신호 처리, 행렬 연산 등에서 성능 극대화.

- MMX, SSE, AVX 명령어 세트의 차이점
    - MMX (MultiMedia eXtension)
        - 인텔의 최초 SIMD 명령어 세트.
        - 64비트 레지스터를 사용하여 정수 데이터 병렬 처리.
        - FPU와 레지스터 공유로 인해 한계가 존재.
    - SSE (Streaming SIMD Extensions)
        - 128비트 레지스터 (XMM) 사용.
        - 부동소수점 연산 지원, 멀티미디어, 그래픽 성능 개선.
        - 이후 SSE2, SSE3, SSE4 등 확장.
    - AVX (Advanced Vector Extensions)
        - 256비트 (AVX1), 512비트 (AVX-512) 지원.
        - 부동소수점뿐만 아니라 정수 연산도 고속 병렬 처리.
        - YMM, ZMM 레지스터 사용, 고성능 컴퓨팅에 활용.

- I/O 서브시스템의 개념과 역할
    - I/O 서브시스템은 CPU와 입출력 장치 간의 데이터 전송을 중계하고 제어하는 구성 요소.
    - 구성 요소에는 디바이스 드라이버, DMA 컨트롤러, 버스, I/O 인터페이스 등이 포함됨.
    - 역할:
        - 장치 독립성 제공 (표준 API를 통한 접근)
        - 버퍼링, 큐잉, 캐싱 등으로 I/O 효율 향상
        - 입출력 동기화 및 에러 처리

- DMA vs Programmed I/O
    - Programmed I/O
        - CPU가 직접 I/O 장치와 통신하여 데이터를 전송.
        - 장치 상태를 확인하며 CPU가 바쁘게 동작해야 함.
        - 단순하지만 비효율적.
    - DMA (Direct Memory Access)
        - DMA 컨트롤러가 CPU를 거치지 않고 메모리와 장치 간 직접 데이터 전송.
        - CPU는 전송 명령만 내리고 다른 작업 수행 가능 → 효율적.
        - 대용량 데이터 처리 시 유리.

- 폴링(Polling) vs 인터럽트(Interrupt)
    - Polling
        - CPU가 주기적으로 I/O 장치 상태를 확인.
        - 구현 간단하나, CPU 자원 낭비.
    - Interrupt
        - 장치가 준비되면 CPU에 신호(인터럽트)를 보내 작업 요청.
        - CPU는 필요할 때만 응답 → 효율적 자원 사용.
    - 비교 요약
        - Polling: 단순하지만 비효율적, 실시간성 떨어짐.
        - Interrupt: 반응형 처리에 유리, 설계는 복잡하지만 성능과 확장성 우수.

- 인터럽트 우선순위와 다중 인터럽트 처리 방식
    - 인터럽트 우선순위 (Interrupt Priority)
        - 여러 인터럽트가 동시에 발생할 경우, 우선순위에 따라 어떤 인터럽트를 먼저 처리할지 결정.
        - 일반적으로 긴급한 처리를 요하는 장치(예: 타이머, 키보드 등)가 높은 우선순위를 가짐.
        - 하드웨어 기반(인터럽트 컨트롤러가 우선순위 결정) 또는 소프트웨어 기반(OS에서 처리).
    - 다중 인터럽트 처리 방식
        - 폴링 방식: CPU가 인터럽트 라인을 순차적으로 검사.
        - 우선순위 인코더 방식: 여러 신호 중 가장 높은 우선순위의 인터럽트만 전달.
        - 중첩 인터럽트(Nested Interrupt): 현재 인터럽트 처리 중 더 높은 우선순위의 인터럽트가 오면 처리 중단 후 새로운 인터럽트 수행.
        - 벡터 인터럽트 방식: 인터럽트 종류마다 고유 주소를 할당하여 빠르게 ISR(Interrupt Service Routine) 호출.

- PCI와 PCIe(PCI Express)의 차이점
    - PCI (Peripheral Component Interconnect)
        - 병렬 방식 버스 구조, 공유 방식.
        - 일정한 클럭 속도(33/66MHz), 속도 제한 존재.
        - 동일 버스를 여러 장치가 공유 → 충돌 가능성, 낮은 대역폭.
    - PCIe (PCI Express)
        - 직렬 방식, 포인트 투 포인트 연결.
        - Lane 기반 구조로, x1, x4, x8, x16 등 확장성 있음.
        - 각 장치가 전용 경로로 통신 → 속도 빠르고 효율적.
        - 현재 대부분의 고속 장치(그래픽카드, NVMe SSD 등)에 사용.

- USB 데이터 전송 방식
    - Control Transfer
        - 장치 제어 정보(설정, 초기화 등) 전송.
        - 짧은 데이터 전송, 필수적인 통신에 사용.
    - Bulk Transfer
        - 대용량 데이터(예: 파일 전송) 처리.
        - 전송 속도는 빠르지만, 실시간성 보장 없음.
    - Interrupt Transfer
        - 짧고 주기적인 데이터 전송(예: 키보드 입력).
        - 빠른 응답이 필요하지만 대역폭은 작음.
    - Isochronous Transfer
        - 실시간 스트리밍(오디오, 비디오)용.
        - 시간은 보장되지만 에러 보정은 없음.

- SCSI, SATA, NVMe의 차이점과 특징
    - SCSI (Small Computer System Interface)
        - 병렬 인터페이스, 고속 데이터 전송.
        - 서버, 워크스테이션 등 고성능 환경에서 사용.
        - 설정 복잡하고 비용 높음.
    - SATA (Serial ATA)
        - 직렬 전송 방식, 범용 저장장치 인터페이스.
        - 하드디스크와 SSD에 모두 사용 가능.
        - 저렴하고 범용적이지만 속도 제한 존재(SATA3 기준 약 600MB/s).
    - NVMe (Non-Volatile Memory Express)
        - PCIe 기반 고속 SSD 인터페이스.
        - CPU와 직접 통신 → 레이지(latency) 낮고 대역폭 큼.
        - 병렬 큐 지원 → 성능 매우 우수.

- 하드디스크 RAID 구성 방식과 장단점
    - RAID 0
        - 데이터 스트라이핑 (분산 저장), 속도 향상.
        - 장애 복구 불가, 안정성 낮음.
    - RAID 1
        - 미러링 (동일 데이터 복제), 높은 안정성.
        - 저장 용량 효율 낮음.
    - RAID 5
        - 데이터 + 패리티 분산 저장.
        - 속도와 안정성 균형. 하나의 디스크 장애 복구 가능.
    - RAID 10 (1+0)
        - RAID 1 + RAID 0 혼합.
        - 속도와 안정성 모두 확보, 저장 효율은 RAID 5보다 낮음.

- SSD에서 TRIM 명령어의 역할
    - TRIM 명령어는 운영체제가 SSD에 삭제된 데이터 블록을 미리 알려주는 명령어.
    - SSD는 덮어쓰기가 불가능하고 삭제 후 재기록을 위해 정리 과정이 필요함.
    - TRIM을 사용하면:
        - 불필요한 블록을 사전에 제거하여 성능 저하를 방지.
        - 쓰기 속도 유지 및 수명 연장.

- 캐시 메모리와 디스크 캐시의 차이점
    - 캐시 메모리
        - CPU와 메인 메모리 사이에서 동작하는 고속 임시 저장장치.
        - 주로 자주 사용하는 명령어나 데이터를 저장하여 접근 속도를 높임.
        - 하드웨어 기반, 매우 빠른 SRAM 사용.
    - 디스크 캐시
        - 스토리지(디스크)와 메모리 사이에서 동작하는 버퍼 역할의 저장 영역.
        - 디스크 I/O 속도 향상을 목적으로, 읽거나 쓴 데이터를 일시 저장.
        - 하드웨어+소프트웨어 모두에 구현 가능, 일반적으로 DRAM 사용.

- 멜트다운(Meltdown)과 스펙터(Spectre) 취약점
    - Meltdown
        - CPU의 권한 경계 우회 취약점.
        - 일반 애플리케이션이 커널 메모리의 데이터를 읽을 수 있음.
        - Out-of-Order Execution 중 잘못된 메모리 접근이 캐시에 남는 것을 이용.
    - Spectre
        - 분기 예측과 투기 실행을 악용해 다른 프로세스의 데이터를 유출.
        - 모든 CPU(ARM, AMD, Intel 등)에 영향, 완전한 차단이 어려움.
    - 대응 방식
        - OS 패치(KPTI), CPU 마이크로코드 업데이트.
        - 성능 저하 감수하며 보안 강화.
        - 컴파일러 레벨의 명령어 삽입으로 투기 실행 제한.

- RISC-V의 특징과 기존 RISC 아키텍처와의 차이점
    - RISC-V의 특징
        - 오픈 소스 명령어 집합 구조(ISA).
        - 모듈화 구조(기본 명령어 + 확장 가능).
        - 상업적 제한 없음, 커스터마이징 쉬움.
        - 저전력부터 고성능까지 폭넓게 적용 가능.
    - 기존 RISC와의 차이점
        - 기존 RISC(ARM 등)는 폐쇄형 ISA.
        - RISC-V는 개방형 ISA로 누구나 구현 가능.
        - 설계 자유도가 높고, 교육 및 연구용으로도 활용도 높음.

- TPU와 GPU의 차이점 (AI/딥러닝 가속기)
    - GPU (Graphics Processing Unit)
        - 병렬 연산에 강함, 행렬 연산 처리에 적합.
        - 범용성과 유연성 뛰어남.
        - 딥러닝 외 게임, 그래픽, 물리 시뮬레이션 등 다용도 사용.
    - TPU (Tensor Processing Unit)
        - 구글이 딥러닝 목적에 맞게 개발한 특화형 하드웨어.
        - 텐서 연산에 최적화된 구조.
        - 고속 추론 및 학습에 최적, 전력 효율 우수.

- 뉴로모픽 컴퓨팅의 개념과 특징
    - 뉴로모픽 컴퓨팅
        - 인간 뇌의 뉴런/시냅스를 모방한 컴퓨팅 아키텍처.
        - 이벤트 기반 연산, 낮은 소비전력.
        - 비동기 처리, 실시간 센싱/분석에 강함.
    - 특징
        - 기존 컴퓨터의 명령어 기반 연산과 다르게 병렬적이고 비선형적.
        - 센서-프로세서-메모리가 결합된 형태.
        - 엣지 AI, 로봇, 신경망 연산 등에 적합.

- 퀀텀 컴퓨팅에서 큐비트(Qubit)의 역할
    - 큐비트(Qubit)
        - 양자 컴퓨팅의 기본 단위.
        - 0과 1의 상태를 동시에 표현할 수 있는 중첩(Superposition) 상태 가능.
        - 얽힘(Entanglement)을 통해 큐비트 간 상호작용을 활용.
    - 역할
        - 기존 비트와 달리 동시에 다양한 계산 가능.
        - 병렬 연산 능력 극대화, 특정 문제(암호 해독, 최적화 등)에서 압도적인 성능 제공.

- FPGA(Field Programmable Gate Array)
    - 개념
        - 사용자가 하드웨어 회로를 프로그래밍할 수 있는 집적 회로.
        - 설계 후에도 소프트웨어처럼 구조 변경이 가능.
        - 논리 게이트, 플립플롭 등의 기본 회로 요소가 블록 형태로 구성됨.
    - 활용 사례
        - 임베디드 시스템, 통신장비, 이미지 처리, 신호 처리.
        - AI 가속기, 자율주행 차량의 센서 전처리.
        - ASIC 설계 전 프로토타입 테스트에도 활용.

- 칩렛(Chiplet) 설계 방식 vs 단일 다이 설계
    - 칩렛 설계
        - CPU, GPU 등의 큰 칩을 여러 개의 작은 칩(칩렛)으로 분리해 연결하는 방식.
        - TSMC의 CoWoS, AMD의 Infinity Fabric 등으로 구현.
    - 차이점
        - 단일 다이는 하나의 큰 실리콘으로 모든 기능 집약 → 공정 불량률이 높고 제조 단가 증가.
        - 칩렛은 기능별로 나눠져 있고 생산 및 조립 효율이 뛰어남 → 유연한 조합과 제조 수율 향상.

- 비휘발성 메모리(NVM, Non-Volatile Memory)
    - 개념
        - 전원이 꺼져도 데이터가 유지되는 메모리 기술.
    - 주요 종류
        - Flash Memory (NAND, NOR), EEPROM
        - Emerging NVM: MRAM, ReRAM, PCM, Intel Optane (3D XPoint)
    - 활용 사례
        - SSD, USB, BIOS, IoT 기기 저장소, AI 모델 저장 등
        - 고속 스토리지 또는 메모리 계층 사이의 브릿지 역할

- HBM vs GDDR 메모리
    - HBM (High Bandwidth Memory)
        - 실리콘 인터포저를 통한 3D 스택 구조의 고대역폭 메모리.
        - 짧은 경로, 넓은 인터페이스 → 고속 처리와 낮은 전력 소비.
        - AI 가속기, 고성능 GPU (NVIDIA A100 등), HPC에 사용.
    - GDDR (Graphics DDR)
        - GPU 전용 고속 메모리, DDR 기반 확장.
        - HBM보다 구조가 단순하고 가격이 저렴.
        - 대부분의 일반 GPU에서 사용.

- 엣지 컴퓨팅 vs 클라우드 컴퓨팅
    - 엣지 컴퓨팅
        - 데이터 발생 지점(센서, 디바이스 등) 근처에서 처리.
        - 지연(latency) 감소, 실시간 반응 가능.
        - 자율주행, 공장 자동화, 스마트 시티 등에 활용.
    - 클라우드 컴퓨팅
        - 중앙 데이터 센터에서 처리.
        - 확장성과 저장 능력이 뛰어나지만, 지연 시간 발생.

- MIPS vs FLOPS
    - MIPS (Million Instructions Per Second)
        - CPU가 초당 실행 가능한 명령어 수.
        - 정수형 연산 중심이며 일반적인 연산량 측정에 사용.

    - FLOPS (Floating Point Operations Per Second)
        - 부동소수점 연산의 처리 속도.
        - 과학 계산, AI 연산 등 정밀 계산 성능 측정에 사용.
        - TeraFLOPS, PetaFLOPS 등으로 단위 확장 가능.

- SPEC Benchmark
    - 개념
        - SPEC(Standard Performance Evaluation Corporation)이 제공하는 국제 표준 벤치마크.
            - 시스템의 CPU, 메모리, 컴파일러, 전체 시스템 성능을 평가하는 데 사용.
    - 목적
        - 하드웨어나 소프트웨어의 정량적 성능 비교.

    - 주요 벤치마크: SPEC CPU, SPECint, SPECfp, SPECjbb, SPECpower 등.

- CPU 성능 향상을 위한 캐시 최적화 방법
    - 대표적인 기법
        - 캐시 친화적 코딩(Cache-friendly coding): 데이터 지역성(Locality) 활용 (ex. 행 우선 접근).
        - 루프 인터체인지(Loop Interchange): 캐시 히트를 유도하도록 반복문의 순서 변경.
        - 루프 타일링(Tile/Blocking): 데이터를 작은 블록 단위로 나누어 캐시에 잘 맞도록 처리.
        - 데이터 정렬 및 구조 최적화: 구조체 크기 조정, padding 제거로 캐시 낭비 최소화.

- 메모리 접근 속도 향상 기법
    - 주요 기법
        - 프리페칭(Prefetching): 데이터가 필요해지기 전에 미리 메모리에서 로딩.
        - 인터리빙(Interleaving): 여러 메모리 뱅크에 데이터를 분산하여 동시 접근 속도 향상.
        - 메모리 계층 구조(Multi-level cache) 활용: L1, L2, L3 캐시 구성으로 접근 속도 개선.
        - 메모리 동시 처리 기술: DMA, 멀티채널 메모리 구성으로 처리 병목 완화.

- 벤치마킹 시 고려 요소
    - 고려해야 할 주요 포인트
        - 성능 지표의 타당성: 실제 사용 시나리오를 반영하는가?
        - 재현성(Reproducibility): 동일 환경에서 동일 결과가 나오는가?
        - 하드웨어/소프트웨어 구성: CPU, GPU, 메모리, OS 등 명확한 스펙 표기.
        - 평균과 분산: 여러 번 측정하고 편차 확인 필요.

- GPU 가속을 통한 성능 향상
    - 주요 활용 기법
        - 병렬 처리 구조 활용: 많은 쓰레드를 동시에 실행 (SIMD 방식).
        - CUDA, OpenCL 등의 병렬 컴퓨팅 프레임워크 사용.
        - 메모리 복사 최소화: CPU↔GPU 전송을 최소화하여 병목 제거.
        - Shared Memory 활용: GPU 내부 공유 메모리를 통해 빠른 접근 실현.

- Branch Prediction Miss 최적화
    - 최적화 방법
        - 조건문 구조 개선: 예측 가능한 분기를 만들도록 if-else 순서 변경.
        - 루프 언롤링(Loop Unrolling): 분기를 줄여 예측 실패 가능성 감소.
        - 컴파일러 힌트 사용: __builtin_expect() 등을 통해 분기 가능성 명시.
        - 정적 분석을 통한 패턴 파악 → 코드 구조 재설계.

- 쓰레드 컨텍스트 스위칭(Context Switching)이 성능에 미치는 영향
    - 개념
        - 스레드 간 전환 시, 현재 쓰레드의 상태(레지스터, PC 등)를 저장하고, 새 쓰레드의 상태를 복원하는 과정.
    - 성능 영향
        - 오버헤드 발생: 캐시 무효화(Cache Flush), TLB 재로딩, 레지스터 저장/복원이 필요.
        - 자주 발생 시 성능 저하: 짧은 시간 동안 많은 전환이 일어나면 CPU 자원이 전환에만 소모됨.
        - 특히 실시간 시스템에 악영향: 예측 불가한 지연이 생김.

- 파이프라인 구조의 주요 해저드와 해결 방법
    - 구조적 해저드(Structural Hazard)
        - 문제: 동일한 하드웨어 자원을 동시에 사용하려는 명령어 충돌.
        - 해결:
            - 자원 복제(예: 메모리와 레지스터 분리)
            - 자원 스케줄링(순서 조정)
    - 데이터 해저드(Data Hazard)
        - 문제: 이전 명령어의 결과가 아직 나오지 않았는데 다음 명령어에서 이를 참조하려 할 때 발생.
        - 해결:
            - 포워딩(Forwarding / Bypassing): 결과를 직접 전달
            - 파이프라인 정지(Stall): 기다리는 시간 삽입
            - 컴파일러 리오더링
    - 제어 해저드(Control Hazard)
        - 문제: 분기(Branch) 명령어가 있을 때 다음 명령어를 결정하지 못하는 상황.
        - 해결:
            - 분기 예측(Branch Prediction)
            - 지연 슬롯(Delay Slot)
            - 파이프라인 플러시 후 분기 확인

- 멀티코어 시스템에서 병목 최소화 기법
    - 주요 기법
        - Lock-Free 알고리즘: 병렬 동기화를 위한 성능 향상.
        - NUMA 최적화: 메모리 접근 로컬리티 확보.
        - Work Stealing: 쓰레드 풀 간 부하 균형 조절.
        - 캐시 일관성 프로토콜(MESI 등) 최적화로 캐시 병목 완화.
        - 스레드 결속(Thread Affinity)으로 캐시 지역성 보장.

- 명령어 파이프라인의 주요 해저드 정리 및 해결
    - 구조적 해저드 → 자원 분리 또는 시간 분할
    - 데이터 해저드 → 포워딩, 스톨, 명령 순서 변경
    - 제어 해저드 → 분기 예측, 조건 지연 슬롯, 명령어 채우기(NOP)

- 지연 슬롯(Delay Slot)의 개념과 효과
    - 개념
        - 분기 명령 이후 한 개 또는 그 이상의 명령어를 실행되도록 예약된 슬롯.
        - 예측 실패 시에도 해당 슬롯의 명령은 항상 실행됨.
    - 효과
        - 분기 지연으로 인한 성능 저하 완화.
        - 컴파일러가 유용한 명령어를 삽입하여 효율 증가 가능.

- 점프 예측(Jump Prediction)과 분기 예측(Branch Prediction)의 차이점
    - 공통점
        - 둘 다 프로그램의 흐름 제어 예측 기법.
    - 차이점
        - Jump Prediction:
            - 정적 또는 간단한 직접 점프(jump) 주소 예측.
            - 주로 함수 호출/복귀(jump and link, return 등) 형태.
        - Branch Prediction:
            - 조건부 분기문(예: if, while)에 대해 분기 여부를 예측.
            - 동적 패턴 기반 (2-bit predictor, BTB 등) 방식이 주로 사용됨.

- Two-Level Adaptive Branch Prediction (2단계 적응형 분기 예측)
    - 개념
        - 과거의 분기 결과 이력(History)과 이력에 따른 분기 동작 예측 패턴을 함께 활용하는 동적 분기 예측 기법.
    - 구성
        - Global History Register (GHR) 또는 Local History Table (LHT)를 사용하여 과거 분기 결과 추적.
        - Pattern History Table (PHT)를 통해 해당 이력에 따른 분기 결과를 예측.
    - 특징
        - 높은 정확도 제공.
        - 조건부 루프나 특정 분기 패턴에 유리.
        - 예: Gshare, PAg, PAp 등의 세부 전략이 존재.

- 명령어 재배치(Instruction Reordering)
    - 개념
        - 명령어 실행 순서를 논리적 결과는 유지하면서, 실행 타이밍 최적화를 위해 재조정하는 기술.
    - 성능 향상
        - 파이프라인 스톨 최소화.
        - 레지스터 충돌 방지, 데이터 의존성 제거.
        - 하드웨어(Out-of-Order Execution) 또는 컴파일러(Static Scheduling) 수준에서 적용.
        
- 레지스터 윈도우(Register Window)
    - 개념
        - 함수 호출 시마다 새로운 레지스터 집합을 사용하는 방식으로, 함수 간 레지스터 저장/복원 오버헤드를 줄이기 위한 구조.
    - 적용 사례
        - SPARC 아키텍처 등에서 사용.
        - 함수 호출 깊이가 얕을수록 성능 향상 효과가 큼.

- 다중 발행(Multiple Issue) 프로세서의 명령어 스케줄링
    - 개념
        - 한 클럭 사이클에 여러 명령어를 병렬로 발행하는 프로세서에서, 충돌을 피하며 효율적으로 명령어를 배치하는 기법.
    - 스케줄링 방법
        - Static Scheduling: 컴파일러가 명령 순서를 정함.
        - Dynamic Scheduling: 하드웨어가 동적으로 제어 (예: Tomasulo 알고리즘).
        - Scoreboarding: 실행 가능 여부를 추적하며 발행.

- 제어 흐름 그래프(Control Flow Graph, CFG)
    - 개념
        - 프로그램의 흐름을 노드(기초 블록)와 간선(제어 흐름)으로 표현한 그래프.
    - 분석 목적
        - 루프 탐지, 도달 가능성 분석, 데이터 흐름 최적화, 컴파일러 최적화 등에 사용.
    - 구성
        - Basic Block: 분기 없이 연속적으로 실행되는 명령어 집합.
        - Edges: 조건 분기, 점프 등 제어 흐름을 나타냄.

- 명령어 인코딩 방식과 성능 영향
    - 인코딩 방식
        - Fixed-Length: RISC에서 주로 사용. 빠른 디코딩.
        - Variable-Length: CISC에서 사용. 코드 압축률은 좋으나 디코딩 복잡.
    - 성능 영향
        - 디코딩 속도, 명령어 캐시 효율성, 하드웨어 복잡도에 직접적인 영향.
        - 고성능 파이프라인 시스템에서는 단일 사이클 디코딩을 위해 고정 길이 선호.

- JIT (Just-In-Time) 컴파일러의 동작 원리와 컴퓨터 구조와의 관계
    - 개념
        - 프로그램 실행 중에 바이트코드를 기계어로 즉시 번역하는 컴파일러.
        - 예: Java의 HotSpot, .NET의 CLR JIT 등.
    - 동작 원리
        - 런타임 중 프로파일링을 통해 자주 사용하는 코드만 최적화 컴파일.
        - 캐시된 기계어 재사용으로 속도 향상.
    - 컴퓨터 구조와의 관계
        - 캐시 친화적인 실행 코드 생성.
        - 동적 최적화로 파이프라인, 분기 예측 성능 향상.
        - Instruction-Level Parallelism (ILP)을 높이기 위한 코드 재배치 가능.

- 페이지 크기(Page Size)를 결정할 때 고려해야 할 요소
    - 내부 단편화 vs. 외부 단편화
        - 큰 페이지는 내부 단편화를 증가시키고, 작은 페이지는 외부 단편화를 줄임.
    - 페이지 테이블 크기
        - 작은 페이지를 사용할수록 많은 페이지 항목이 필요하여 테이블이 커짐.
    - 디스크 접근 효율
        - 큰 페이지는 한 번의 디스크 접근으로 더 많은 데이터를 가져올 수 있어 I/O 효율이 높음.
    - TLB 효율성
        - 큰 페이지는 더 많은 주소 공간을 한 번에 캐시할 수 있어 TLB 히트율이 높아짐.

- 다중 프로세스 환경에서 TLB 관리 방법
    - TLB 플러시(Flush)
        - 컨텍스트 전환 시 TLB를 초기화하여 다른 프로세스의 주소 매핑 정보를 제거.

    - 태그 기반 TLB (Address Space Identifier, ASID)
        - 각 TLB 항목에 프로세스 식별자를 부여해 컨텍스트 전환 시에도 TLB를 유지할 수 있도록 함.

    - TLB Shootdown (다중 CPU 환경)
        - 하나의 CPU에서 TLB 변경 시, 다른 CPU에도 동기화 작업 수행.

- 다중 페이지 테이블 구조의 장단점
    - Inverted Page Table
        - 장점: 물리 메모리 기준으로 하나만 유지하면 되므로 공간 절약.
        - 단점: 주소 변환 시간이 길고, 해시를 이용하므로 복잡함.
    - Multi-Level Page Table
        - 장점: 사용 중인 메모리 영역만 페이지 테이블을 할당하여 효율적인 메모리 사용.
        - 단점: 주소 변환에 여러 단계 접근 필요 (성능 저하 요인).

- Demand Paging vs. Prepaging
    - Demand Paging (요구 페이징):
        - 필요한 시점에만 페이지를 로딩. 초기 메모리 사용량은 적지만 페이지 폴트가 잦을 수 있음.

    - Prepaging (선제적 페이징):
        - 관련 페이지를 미리 로딩하여 페이지 폴트를 줄임. 그러나 불필요한 페이지 로딩으로 낭비 가능성 존재.

- Page Coloring 기법의 개념과 성능 향상 효과
    - 개념: 캐시 라인 충돌을 방지하기 위해 물리 주소의 색(컬러)을 고려하여 가상 페이지를 물리 메모리에 매핑.

    - 효과:
        - 캐시 충돌 감소
        - 캐시 활용률 및 메모리 접근 성능 향상
        - 주로 고성능 시스템이나 OS 커널에서 사용

- 페이지 공유(Page Sharing) vs. Copy-On-Write(COW)
    - Page Sharing:
        - 동일한 내용을 가진 페이지들을 물리적으로 하나로 공유하여 메모리 절약.

    - Copy-On-Write (COW):
        - 초기에는 페이지를 공유하다가 쓰기 요청이 발생하면 복사하여 분리.
        - → 메모리 효율성 + 안전한 프로세스 분리 동시 보장.

- 메모리 압축(Memory Compression) 기법의 원리와 효과
    - 개념:
        - RAM이 부족할 때 일부 페이지를 디스크에 스와핑하는 대신, 메모리에 압축하여 저장.
    - 원리:
        - 메모리 압축 영역을 별도로 두고, 오래된 또는 덜 사용된 페이지를 압축 저장
        - 필요한 경우 압축 해제 후 다시 접근
    - 효과:
        - 스와핑 횟수 감소 → 디스크 I/O 부하 경감
        - 메모리 사용량 절감
        - 일반적으로 모바일 및 저사양 시스템에서 유용

- Swap Space의 개념과 성능 영향
    - 개념:
        - RAM이 부족할 때 디스크의 일부 공간을 가상 메모리처럼 사용하는 영역으로, 주로 비활성 메모리 페이지를 저장.

    - 성능 영향:
        - 메모리가 부족한 상황에서 일시적인 해결책
        - 그러나 디스크는 RAM보다 속도가 현저히 느리므로, 빈번한 스와핑은 시스템 성능 저하의 원인이 됨
        - SSD 기반 시스템에서는 그나마 속도가 빠르지만, 수명 단축 우려가 있음

- 메모리 인터리빙(Memory Interleaving)의 개념과 효과
    - 개념:
        - 연속된 메모리 주소를 여러 개의 메모리 뱅크에 번갈아 배치하여 동시에 접근 가능하도록 만드는 기법.

    - 효과:
        - 메모리 대역폭 증가, 병목 현상 완화
        - CPU가 데이터를 연속적으로 요청할 경우, 병렬로 응답 가능
        - 고성능 시스템이나 서버에서 사용됨

- 하드웨어 지원 가상화(VT-x, AMD-V)의 원리와 필요성
    - 필요성:
        - 기존 소프트웨어 기반 가상화는 성능 저하 및 제어 권한 분리에 한계가 있음

    - 원리:
        - VT-x(Intel), AMD-V는 CPU 수준에서 가상화 계층을 제공
        - 하이퍼바이저가 직접 가상 머신의 상태를 관리하지 않아도 되며, 트랩 발생 없이 직접 명령 실행 가능

    - 장점:
        - 성능 향상
        - 더 강력한 격리
        - 가상 머신 수 증가 가능
        
- SSD의 Wear Leveling 기법과 필요성
    - 필요성:
        - SSD의 플래시 메모리는 쓰기/삭제 횟수에 제한이 있어, 특정 셀만 집중적으로 사용되면 빠르게 마모됨
    - 기법:
        - 정적(SWL): 자주 쓰지 않는 영역도 주기적으로 재배치
        - 동적(DWL): 자주 쓰는 영역의 사용 횟수를 분산시킴
        - 컨트롤러가 블록 사용 횟수를 모니터링하여 균등하게 배분
    - 효과:
        - SSD 수명 연장
        - 성능 균일성 유지
        
5. SSD의 Garbage Collection 개념과 최적화 방법
개념:

SSD는 데이터를 덮어쓸 수 없고, 기존 블록을 삭제 후 새로운 위치에 기록
Garbage Collection은 유휴 시간에 불필요한 데이터를 정리하고, 유효한 데이터를 새로운 블록으로 이동
성능 최적화 방법:

TRIM 명령어 활용: OS가 삭제된 블록을 SSD에 알림
Over-provisioning 영역 확보: 여유 블록을 미리 확보하여 GC 부담 완화
GC 주기 조절: 유휴 시간에 수행되도록 설정
6. HDD의 ZBR(Zone Bit Recording) 기법
개념:

HDD는 바깥쪽 트랙이 더 길기 때문에, 바깥쪽에 더 많은 섹터를 배치하는 기법
트랙을 Zone으로 나누고, 각 Zone에 다른 수의 섹터를 배치하여 저장 효율을 높임
효과:

저장 용량 증가
바깥쪽에서 더 빠른 데이터 전송 속도 가능
다만, 트랙 간 처리 복잡성은 증가

- Caching Disk와 Write Buffering의 개념과 차이점
Caching Disk (디스크 캐싱)

디스크 내장 캐시 메모리나 운영체제 메모리를 이용해 자주 사용하는 데이터 블록을 읽기 중심으로 임시 저장
목적: 읽기 성능 향상 및 디스크 접근 최소화
Write Buffering (쓰기 버퍼링)

쓰기 요청 데이터를 버퍼 메모리에 임시 저장 후, 실제 디스크에 나중에 기록하는 방식
목적: 쓰기 성능 향상 및 동시 요청 처리 능력 증가
차이점:

Caching Disk는 읽기 중심, Write Buffering은 쓰기 중심 최적화
Write Buffering은 정전 시 데이터 손실 위험이 있어 배터리 백업 캐시가 필요할 수 있음
2. NVMe over Fabrics(NVMe-oF)의 개념과 기존 NVMe와의 차이점
NVMe (Non-Volatile Memory Express)

고속 SSD를 위해 개발된 PCIe 기반의 저장 장치 인터페이스
NVMe-oF (over Fabrics)

NVMe 프로토콜을 PCIe 대신 이더넷, 파이버 채널, 인피니밴드 등 네트워크 기반으로 확장한 것
원격 SSD 장치에 접근하면서도 로컬 NVMe에 준하는 속도 제공
차이점:

기존 NVMe: 내부 PCIe 기반, 로컬 고속 접근
NVMe-oF: 네트워크를 통한 외부 장치 연결, 데이터센터 스토리지 확장에 유리
3. DAS, NAS, SAN의 차이점
DAS (Direct Attached Storage)

서버에 직접 연결된 스토리지 (USB, SATA, SAS)
장점: 빠름, 단순
단점: 확장성 부족, 공유 어려움
NAS (Network Attached Storage)

파일 레벨 저장소, TCP/IP 기반으로 여러 클라이언트가 파일 공유
장점: 사용 편리, 파일 시스템 포함
단점: 일반 네트워크 대역폭에 의존
SAN (Storage Area Network)

블록 레벨 저장소, 파이버 채널 등으로 고속 네트워크 기반 스토리지 풀 구성
장점: 대규모, 고성능 스토리지 구성
단점: 구축 복잡, 비용 높음
4. 데이터 무결성을 보장하는 ECC(Error Correcting Code) 방식
ECC의 목적:

전송 중 또는 저장 중 비트 오류 감지 및 자동 정정
일반적으로 1비트 오류 정정, 2비트 오류 감지 가능
주요 방식:

해밍 코드(Hamming Code)
SECDED (Single Error Correction, Double Error Detection)
DRAM, CPU L1~L3 캐시, 서버 메모리 등에서 사용됨
효과:

시스템 안정성 향상, 메모리 오류로 인한 다운타임 방지
5. CRC와 ECC의 차이점
CRC (Cyclic Redundancy Check)

오류 감지에 특화, 오류 정정은 불가
주로 네트워크 통신, 저장 매체의 전송 무결성 검사에 사용
ECC (Error Correcting Code)

오류 감지뿐 아니라 자동 정정까지 가능
메모리, CPU, 저장 장치 등 고신뢰 환경에서 사용
핵심 차이점:

CRC는 오류 감지 전용, ECC는 감지 + 정정 모두 가능
6. RAID 6에서 이중 패리티(Double Parity) 기법의 원리
RAID 6:

최소 4개 이상의 디스크 필요
데이터 + 2개의 패리티 블록(P, Q)을 서로 다른 알고리즘으로 생성
이중 패리티 기법:

하나는 단순 XOR 연산(P)
다른 하나는 Galois Field 기반의 Reed-Solomon 코드(Q) 사용
두 개의 디스크가 동시에 고장나도 정확한 데이터 복구 가능
장점:

RAID 5보다 훨씬 강력한 내결함성
대용량 데이터, 고가용성 요구 환경에 적합

- 파일 시스템에서 저널링(Journaling) 기법의 원리와 장점은?
개념:
저널링은 파일 시스템의 무결성과 복구성을 확보하기 위해, 디스크에 데이터를 쓰기 전에 변경 사항을 로그(저널)에 먼저 기록하는 기법이다.

원리:
변경 작업을 트랜잭션 단위로 저널에 기록하고, 이후 실제 데이터 블록에 반영. 시스템이 비정상 종료되더라도, 재부팅 시 저널을 통해 일관성 있는 상태로 복구 가능.

장점:

시스템 충돌 시 빠른 복구
파일 시스템 손상 방지
데이터 무결성 향상
예: ext3, ext4, NTFS 등에서 사용됨
2. 분산 컴퓨팅(Distributed Computing)과 병렬 컴퓨팅(Parallel Computing)의 차이점은?
병렬 컴퓨팅:
하나의 문제를 여러 프로세서에서 동시에 처리하는 방식. 일반적으로 공유 메모리나 멀티코어 환경에서 수행됨.

분산 컴퓨팅:
물리적으로 분리된 여러 컴퓨터(노드)에서 작업을 나누어 처리하며, 각 노드는 독립적인 메모리/자원을 가짐.

차이점 요약:

병렬 컴퓨팅: 주로 고성능 컴퓨팅(HPC), 동일 시스템 내
분산 컴퓨팅: 네트워크 기반, 지리적 분산 가능
동기/비동기, 장애 허용 등 측면에서 구현 방식 차이 존재
3. MapReduce의 개념과 동작 방식은?
개념:
대용량 데이터를 분산 환경에서 처리하기 위한 프로그래밍 모델로, 구글이 처음 제안.

동작 방식:

Map 단계: 데이터를 <Key, Value> 쌍으로 나누고 분산 처리
Shuffle 단계: 중간 결과를 키 기준으로 그룹화
Reduce 단계: 같은 키를 가진 데이터를 집계 또는 결합
활용:

대용량 로그 분석
텍스트 마이닝, 통계 처리
Hadoop 등 분산 처리 플랫폼에서 채택
4. CPU-GPU 공동 처리(Heterogeneous Computing)의 개념과 활용 사례는?
개념:
서로 다른 아키텍처(예: CPU, GPU, NPU 등)를 하나의 시스템에서 협업하도록 구성한 컴퓨팅 모델이다.

장점:

CPU는 일반적 제어·논리 연산 담당
GPU는 수천 개 코어로 대규모 연산 병렬 처리
성능 및 에너지 효율성 향상
활용 사례:

AI/딥러닝 학습 및 추론
영상처리, 시뮬레이션
자율주행 자동차, 금융 모델링 등
5. OpenCL과 CUDA의 차이점과 활용 사례는?
CUDA (Compute Unified Device Architecture):
NVIDIA GPU 전용 병렬 처리 API, 높은 성능 제공

언어: C/C++ 확장
제한적 하드웨어 종속성
OpenCL (Open Computing Language):
크로스 플랫폼 표준으로 CPU, GPU, FPGA 등 다양한 장치에서 동작

벤더 중립적
유연하지만 복잡성 존재
활용 비교:

CUDA: NVIDIA 기반 AI/과학 시뮬레이션 등에서 광범위 사용
OpenCL: 이종 환경에서 병렬 연산 필요할 때 적합
6. 클러스터(Cluster)와 그리드 컴퓨팅(Grid Computing)의 차이점은?
클러스터 컴퓨팅:
동일한 네트워크에 위치한 서버들을 하나의 시스템처럼 구성하여 높은 가용성과 성능 확보

공유 저장소, 로드 밸런싱, 병렬 처리 지원
tightly coupled 구조
그리드 컴퓨팅:
지리적으로 분산된 자원들을 연결하여 협업 처리

loosely coupled 구조
이기종 자원 활용 가능
차이점 요약:

클러스터: 집중적 자원, 실시간 처리
그리드: 분산된 자원, 대규모 배치 처리에 적합

- MPI에서 동기식(Synchronous) vs 비동기식(Asynchronous) 통신의 차이점
동기식 통신:

송신 측은 메시지를 보내고 수신 측이 받기 전까지 블로킹됨.
양쪽 모두 메시지를 처리할 준비가 되어야 통신 성공.
안정성은 높지만 성능 저하 가능.
비동기식 통신:

송신 측은 메시지를 큐에 넣고 즉시 다음 작업 수행 가능.
수신은 나중에 메시지를 비동기적으로 받아 처리.
성능은 좋지만 동기화 문제 발생 가능.
2. 병렬 프로그래밍에서 레이스 컨디션(Race Condition)과 해결 방법
개념:

두 개 이상의 스레드가 공유 자원에 동시에 접근하면서 실행 순서에 따라 결과가 달라지는 현상.
해결 방법:

뮤텍스(Mutex): 임계영역을 한 번에 하나의 스레드만 접근하도록 제한.
세마포어(Semaphore): 리소스 수 제한이 필요한 경우 사용.
락 프리(lock-free) 구조: CAS(Compare-And-Swap) 등을 통한 동시성 제어.
원자 연산(Atomic Operation): 중단 불가능한 단일 연산으로 상태 변경.
3. 다중 스레드 환경에서 데드락(Deadlock)의 원인과 해결 방법
원인 네 가지 조건 (모두 만족 시 발생):

상호 배제(Mutual Exclusion)
점유와 대기(Hold and Wait)
비선점(No Preemption)
환형 대기(Circular Wait)
해결 방법:

자원 선점 허용 또는 순서 정하기.
타임아웃 설정 및 감지 로직 구현.
자원 점유 순서 강제화 또는 교착상태 감지 알고리즘 사용 (예: 은행가 알고리즘).
4. NUMA 시스템에서 메모리 접근 최적화를 위한 방법
NUMA(Non-Uniform Memory Access) 구조는 CPU가 자신에게 더 가까운 메모리를 접근할 때 더 빠름.

최적화 기법:

메모리 로컬리티 확보: 쓰레드가 자주 접근하는 데이터를 자신의 지역 메모리에 배치.
NUMA-aware 스케줄링: 운영체제 또는 애플리케이션이 스레드를 지역 노드에 할당.
페이지 바인딩: 특정 메모리 페이지를 특정 NUMA 노드에 고정.
CPU Affinity 설정: 특정 쓰레드를 특정 CPU에 고정시켜 캐시 활용도 향상.
5. GPGPU(General-Purpose computing on Graphics Processing Units) 기술이란?
개념:

GPU의 수많은 코어를 이용해 그래픽 이외의 연산 작업을 수행하는 기술.
수천 개의 쓰레드를 동시에 실행 가능하여 대규모 병렬 처리에 강함.
활용 분야:

머신러닝/딥러닝 학습, 과학 시뮬레이션, 금융 모델링, 암호화 해석, 렌더링 등.
기술 스택:

CUDA (NVIDIA), OpenCL 등으로 프로그래밍 가능.
6. TPM(Trusted Platform Module)의 개념과 역할
TPM이란:

하드웨어 기반 보안 칩으로, 암호화 키 관리, 인증, 무결성 검증 등을 수행.
메인보드에 탑재되어 독립적으로 작동함.
주요 기능:

보안 부팅(Secure Boot)
디스크 암호화(BitLocker)
플랫폼 무결성 검증 (PCR 레지스터 활용)
디지털 인증서 저장 및 서명 연산
보안 강화 효과:

OS 해킹이나 부트킷 감염 시에도 하드웨어 수준에서 위협 차단 가능.

- Meltdown과 Spectre 보안 취약점의 원리와 대응 방안
Meltdown 취약점:

CPU의 권한 확인보다 명령 실행이 먼저 이뤄지는 아키텍처적 취약점을 이용.
커널 메모리에 접근할 수 없는 사용자 프로세스가 캐시 타이밍을 통해 민감 데이터 추출 가능.
인텔 아키텍처 중심으로 영향을 받음.
Spectre 취약점:

분기 예측(Branch Prediction)과 투기 실행(Speculative Execution)을 악용해,
잘못된 분기 경로를 따라 실행한 결과를 캐시에 남긴 뒤 측정하여 민감 정보를 유추.
대응 방안:

커널 페이지 격리(KPTI) 적용.
CPU 마이크로코드 업데이트.
소프트웨어 수준에서 투기 실행 무효화.
최신 CPU에서는 하드웨어 레벨 방어 탑재.
2. 사이드 채널 공격(Side-Channel Attack)의 종류와 대응 방법
종류:

시간 분석(Time Analysis): 연산 시간의 차이를 이용해 키 등 민감 정보 유출.
전력 분석(Power Analysis): 연산 중 소비되는 전력량을 기반으로 정보 유추.
전자기파 분석(EM Analysis): 칩에서 방출되는 전자기파 수신 후 분석.
캐시 공격(Cache Timing, Flush+Reload 등): 공유 캐시의 접근 타이밍을 통해 정보 추출.
대응 방법:

일정 시간 내 연산 처리 (타이밍 무작위화).
전력 패턴 무작위화 및 마스킹.
캐시 분리, 메모리 접근 무작위화.
하드웨어 및 컴파일러 수준에서 무작위성 도입.
3. ASLR(Address Space Layout Randomization) 기법
개념:

실행 시점마다 코드, 스택, 힙, 라이브러리 등 메모리 위치를 무작위로 배치함으로써,
공격자가 메모리 주소를 예측하고 악용하는 것을 어렵게 만듦.
역할:

버퍼 오버플로우 등 주소 기반 공격에 대한 기본적 방어 수단으로 작동.
DEP(Data Execution Prevention)과 함께 사용 시 효과 증가.
4. 보안 강화 프로세서의 개념과 역할
Secure Enclave (Intel SGX):

인텔의 보안 기술로, 프로세서 내에 보호된 메모리 영역(Enclave)을 제공.
운영체제나 다른 프로세스도 접근할 수 없는 격리된 실행 환경을 제공함.
ARM TrustZone:

ARM 기반 SoC에서 Secure World와 Normal World를 분리하여 처리.
민감 정보나 키 관리를 Secure World에서 수행.
역할:

키 관리, 디지털 서명, 암호 연산, 개인정보 처리 등 민감 연산을 격리하여 보호.
5. 메모리 암호화(Memory Encryption) 기술
개념:

시스템 메모리에 저장되는 데이터를 하드웨어 수준에서 암호화하여,
메모리 덤프 공격, 냉각 공격(Cold Boot Attack) 등의 물리적 접근 위협에 대응.
기술 예시:

AMD SEV (Secure Encrypted Virtualization)
Intel TME (Total Memory Encryption)
특징:

암복호화는 메모리 컨트롤러에서 실시간 수행, 성능 영향 최소화.
6. 해시 기반 무결성 검증(Hash-Based Integrity Checking)
원리:

파일, 데이터 블록, 시스템 바이너리 등에 대해 SHA-256, SHA-3 등의 해시를 계산하고 저장.
이후 동일한 방식으로 해시값을 재계산해 비교함으로써 무결성 여부 판단.
활용 사례:

운영체제 부팅 시 커널 검증 (Secure Boot, Measured Boot).
파일 시스템 무결성 검사 (AIDE, Tripwire).
패키지 설치 시 해시 체크 (apt, yum 등 패키지 관리자).

- 서버 CPU에서 Secure Boot의 원리와 필요성
개념:

Secure Boot은 부팅 시점에 실행되는 모든 코드(펌웨어, 부트로더, OS 등)가 서명된 정품 코드인지 확인하는 보안 기능이다.
원리:

CPU는 UEFI 펌웨어에 내장된 신뢰할 수 있는 공개 키(Root of Trust)를 기반으로,
부팅에 필요한 모든 구성 요소의 디지털 서명 검증을 단계적으로 수행하며,
검증 실패 시 부팅 중단 또는 사용자 경고.
필요성:

루트킷, 부트킷 등의 시스템 레벨 악성코드 차단.
OS 시작 전 위협을 막는 프리-OS 보안 강화 핵심 수단.
서버 무결성과 신뢰성 확보.
2. 하드웨어 백도어(Hardware Backdoor)의 개념과 탐지 방법
개념:

하드웨어 설계 단계 또는 생산 과정에서 의도적으로 삽입된 악성 회로, 명령어, 기능으로,
외부 제어 없이도 시스템 탈취, 정보 유출, 비인가 명령 실행 등이 가능.
탐지 방법:

하드웨어 리버스 엔지니어링: 칩의 레이아웃을 해체하여 의심 회로 식별.
정적 분석 및 시뮬레이션: 회로 설계도(HDL, Netlist)를 분석하여 비정상 동작 여부 탐지.
신호 분석: 전력 소비, 타이밍, 전자파 신호 등 사이드 채널 분석을 통한 이상 탐지.
신뢰할 수 있는 설계 및 공급망 관리가 가장 효과적인 예방책.
3. 클라우드 컴퓨팅 환경에서 VM 보안 강화를 위한 하드웨어 기술
TPM (Trusted Platform Module):

암호화 키와 부팅 무결성을 하드웨어 기반으로 보장.
VM과 호스트 간의 신뢰 체인 검증.
vTPM (Virtual TPM):

VM 전용 TPM 인스턴스를 제공하여 VM 간 보안 격리 및 무결성 보장.
AMD SEV (Secure Encrypted Virtualization):

VM 메모리를 암호화하여 하이퍼바이저나 다른 VM이 접근 불가하도록 보호.
Intel TDX (Trust Domain Extensions):

하이퍼바이저를 포함한 OS 외부 공격으로부터 VM 격리.
4. 양자 컴퓨터에서 큐비트(Qubit)의 개념과 전통적인 비트(Bit)와의 차이점
비트 (Bit):

전통적인 컴퓨터에서 정보의 최소 단위이며, 값은 0 또는 1 중 하나로 고정됨.
큐비트 (Qubit):

양자 중첩(Superposition)을 통해 0과 1을 동시에 표현 가능.
얽힘(Entanglement)과 측정(Measurement) 개념을 통해 병렬 연산 능력 보유.
차이점:

큐비트는 동시에 여러 상태를 표현해 복잡한 문제를 더 빠르게 해결할 수 있음.
전통 컴퓨터는 순차적 처리, 양자 컴퓨터는 동시성 기반 병렬 처리에 강함.
5. 뉴로모픽 컴퓨팅(Neuromorphic Computing)의 원리와 활용 가능성
개념:

인간 뇌의 뉴런과 시냅스의 구조와 동작 원리를 모방한 컴퓨팅 구조.
비동기적 이벤트 기반 처리, 병렬성, 자가 학습 등의 특성.
원리:

전통 CPU 대신 뉴런 회로와 시냅스 트랜지스터로 구성된 전용 칩(예: Intel Loihi)을 사용.
연산은 이벤트 발생 시에만 수행되어 초저전력, 초고속 특성.
활용 가능성:

엣지 AI, 실시간 패턴 인식, 뇌과학 연구, 초저전력 IoT 디바이스 등에 적합.
6. 광컴퓨터(Photonic Computing)의 개념과 기존 반도체 기반 컴퓨터와의 차이점
개념:

전기 대신 빛(광자)을 사용하여 정보를 처리하고 전달하는 컴퓨팅 기술.
레이저, 광 파장, 광 스위치, 광 메모리 등을 활용.
차이점:

광자는 전자보다 빠르며 열 발생이 적고, 신호 간섭이 적음.
병렬 데이터 처리 및 고속 전송에 유리하지만, 집적 회로화의 난이도와 제조 기술의 한계로 상용화가 느림.
전통 컴퓨터 대비 특징:

고속 전송, 전력 효율, 병렬성에 뛰어나며,
데이터센터, 고속 네트워크, AI 연산 등 차세대 초고성능 연산용 기술로 연구 중.

- PIM(Processing-In-Memory) 기술의 개념과 장점
개념:
PIM은 메모리 내에서 데이터를 처리하는 구조로, 기존의 CPU 중심 처리 방식과 달리 데이터 이동 없이 연산을 메모리 내부에서 직접 수행함.

장점:

데이터 이동 최소화: CPU-메모리 간 대역폭 병목 해소.
에너지 효율성: 불필요한 데이터 전송 제거로 전력 소비 감소.
지연 시간 감소: 메모리 내부 연산으로 레이턴시 절감.
AI/빅데이터 분석 등에 적합: 대용량 병렬 데이터 처리에 유리.
2. RISC-V가 기존 RISC/CISC 아키텍처와 비교하여 갖는 강점
개방형 ISA:

RISC-V는 오픈소스 명령어 집합 구조(ISA)로 누구나 자유롭게 설계, 수정, 구현 가능.
ARM이나 x86과 달리 라이선스 비용 없음.
모듈성:

기본 명령어 집합이 작고 단순하며, 필요에 따라 확장 모듈을 선택적으로 추가 가능.
예: 압축 명령어(C), 멀티플라이(M), 벡터(V) 확장 등.
유연성 및 이식성:

임베디드 시스템, 서버, AI 칩 등 다양한 플랫폼에 맞춤형 설계 가능.
교육, 연구, 상용화 모두에 적합.
3. AI 전용 칩(ASIC, FPGA, TPU)의 차이점과 활용 사례
ASIC (Application-Specific Integrated Circuit)

특정 연산에 최적화된 고정 하드웨어.
성능과 전력 효율 뛰어나지만 개발 비용과 시간 큼.
Google TPU, Tesla FSD 칩 등이 대표 사례.
FPGA (Field Programmable Gate Array)

프로그래밍 가능한 하드웨어.
유연성과 실시간 제어에 적합하며, 하드웨어 수준의 병렬 처리 가능.
에지 디바이스, 통신 장비, 실시간 AI 추론에 활용.
TPU (Tensor Processing Unit)

Google이 AI용으로 설계한 행렬 연산 특화 칩.
대규모 머신러닝 학습 및 추론에 최적화됨.
Google Cloud에서 제공되는 AI 서비스에 주로 사용.
4. 스마트NIC(SmartNIC)의 개념과 활용 사례
개념:

일반적인 NIC(Network Interface Card)에 CPU, FPGA, 메모리 등 연산 능력을 탑재한 지능형 네트워크 인터페이스.
기능:

패킷 필터링, 암호화, 로드밸런싱, 보안 처리 등 네트워크 관련 작업을 오프로드.
호스트 CPU 자원 절약 및 네트워크 성능 향상 가능.
활용 사례:

대형 클라우드 데이터센터의 보안 처리 및 네트워크 가속.
고성능 컴퓨팅(HPC), 5G MEC, SDN/NFV 환경.
5. Edge AI 컴퓨팅의 개념과 기존 AI 모델과의 차이점
개념:

클라우드가 아닌 엣지(단말/로컬 장비)에서 AI 모델을 실행하는 방식.
지연 최소화, 실시간 처리, 오프라인 동작 등이 장점.
차이점:

클라우드 AI: 중앙 서버에서 모델 학습 및 추론 수행.
엣지 AI: IoT 디바이스, 카메라, 차량 등 로컬에서 경량화된 모델로 즉시 판단 수행.
적용 분야:

자율주행차, 스마트 시티, 산업용 IoT, 헬스케어 등.
6. 소프트웨어 정의 하드웨어(Software-Defined Hardware)의 개념과 활용 가능성
개념:

하드웨어의 동작이나 구조를 소프트웨어적으로 프로그래밍/구성할 수 있도록 설계된 플랫폼.
주로 FPGA, eFPGA, Reconfigurable Computing 기반.
장점 및 활용 가능성:

유연한 기능 변경: 용도에 따라 실시간 하드웨어 구조 변경 가능.
신속한 AI 모델 업데이트, 보안 정책 변경, 통신 프로토콜 수정 가능.
클라우드, 엣지, 5G 네트워크 장비, 국방, 우주 항공 등에서 활발히 연구 및 적용 중.

- 차세대 메모리 기술과 기존 DRAM/NAND 플래시와의 차이점
Optane (3D XPoint)

인텔/마이크론 공동 개발. DRAM보다 느리지만 NAND보다는 빠르며, 비휘발성.
DRAM의 속도 + NAND의 용량을 추구. 캐시, 스토리지 가속 용도에 활용.
MRAM (Magnetoresistive RAM)

자기 저항 효과 기반. DRAM 속도에 비슷하면서 비휘발성.
전력 소모 적고 수명이 길며, 웨어러블/임베디드에 적합.
ReRAM (Resistive RAM)

저항 변화로 데이터를 저장. 빠르고 내구성 높음.
비휘발성이며 전력 효율 우수. AI 엣지 디바이스 등에 활용.
PCM (Phase Change Memory)

물질의 상태 변화를 통해 0/1을 저장. DRAM보다 느리고 NAND보다 빠름.
비휘발성이며 재기록 수명도 우수.
기존 DRAM/NAND와의 차이점

DRAM은 휘발성이며 빠르나 고가.
NAND는 비휘발성이나 속도가 느리고 수명 제한 있음.
차세대 메모리는 비휘발성과 속도/수명을 개선한 중간 단계 메모리로 스토리지 계층 구조의 다양성을 제공.
2. 명령어 스케줄링(Instruction Scheduling)의 역할과 종류
역할:

명령어 간의 의존성, 리소스 제약 등을 고려해 실행 순서를 최적화하여 파이프라인 효율 극대화.
종류:

정적 스케줄링 (Static Scheduling): 컴파일러가 결정 (VLIW에서 주로 사용).
동적 스케줄링 (Dynamic Scheduling): 하드웨어가 실행 시점에서 결정 (Tomasulo’s Algorithm, Scoreboarding 등).
3. Register Renaming의 개념과 역할
개념:

프로그램에서 동일한 논리 레지스터 이름을 사용하더라도, 하드웨어에서 실제 물리 레지스터로 매핑을 다르게 하여 충돌(의존성)을 회피하는 기법.
역할:

Out-of-Order Execution에서 WAW(Write After Write), WAR(Write After Read)와 같은 가짜 의존성(false dependency) 제거.
병렬 실행 가능한 명령어를 늘려 성능 향상.
4. 동적 명령어 스케줄링(Dynamic Instruction Scheduling)의 개념과 장점
개념:

실행 시점에서 명령어 간 의존성을 분석하여 가능한 명령어를 순서와 상관없이 실행하는 기법.
대표 예시: Tomasulo 알고리즘, Scoreboarding

장점:

프로그램의 제약에 덜 구속되어 하드웨어 수준에서 최대한 병렬성 확보 가능.
파이프라인 효율 증가, 자원 활용도 극대화.
5. VLIW에서 명령어 병렬성 최대 활용 방법
VLIW 구조:

여러 개의 연산 명령어를 하나의 긴 명령어(word)에 포함시켜 동시에 실행.
병렬성 활용 방법:

컴파일러가 명령어 스케줄링을 정적으로 수행, 실행 가능한 명령어를 병렬 배치.
명령어 간 데이터 의존성이 없도록 최적화 필요.
장점:

제어가 단순하여 하드웨어 복잡도 낮고, 에너지 효율성 높음.
컴파일러 의존도가 높고, 코드 크기가 커지는 단점도 존재.
6. 멀티코어 시스템에서 False Sharing의 개념과 영향
개념:

서로 다른 코어의 스레드가 서로 관련 없는 데이터를 같은 캐시 라인에 저장한 경우, 캐시 일관성을 유지하기 위해 불필요한 병목 발생.
영향:

성능 저하 및 CPU 캐시 트래픽 증가.
멀티스레딩 병렬 처리의 효율을 크게 낮춤.
해결 방법:

데이터 구조 정렬(Padding), false sharing 발생 위치를 회피하는 메모리 구조 설계.
7. CPU의 마이크로코드(Microcode)의 개념과 역할
개념:

CISC 명령어를 실제로 실행하기 위한 보다 세분화된 제어 신호의 시퀀스를 나타낸 내부 명령어 집합.
역할:

복잡한 명령어를 단순한 마이크로연산들로 해석하여 제어 유닛이 동작하게 함.
CPU 설계 유연성 증가 및 명령어 추가, 수정 가능.


- 실행 흐름 예측 (Execution Flow Prediction)
개념:
프로그램 실행 시 분기(branch), 루프, 함수 호출/복귀 등 제어 흐름의 전환 지점을 예측하여 파이프라인의 끊김을 최소화하는 기법.

방식:

정적 예측: 항상 분기를 취한다/취하지 않는다로 고정.
동적 예측: 히스토리 기반 분기 예측(예: 2-bit predictor, GShare, Tournament).
Return Address Stack (RAS): 함수 호출/복귀 흐름 예측.
2. Instruction Window의 개념과 크기 조절 요소
개념:
Out-of-Order Execution에서 명령어를 보류하거나 재배치할 수 있는 버퍼 영역으로, 명령어 병렬 실행을 위한 대기열.

크기 조절 고려사항:

파이프라인 병렬성 활용도: 클수록 명령어 병렬성 극대화 가능.
하드웨어 복잡성: 클수록 리소스, 전력 소비 증가.
레지스터 및 버스 충돌 가능성, Out-of-Order 처리량.
3. Data Forwarding (데이터 포워딩)
개념:
파이프라인 구조에서 연산 결과가 레지스터에 기록되기 전에 다음 명령어가 이를 바로 사용할 수 있도록 전달하는 기술.

활용 방식:

예: ALU에서 계산된 값을 바로 다음 명령어의 피연산자로 제공.
파이프라인의 데이터 의존성으로 인한 stall을 줄이는 데 효과적.
4. Reorder Buffer (ROB, 재정렬 버퍼)
역할:

명령어가 Out-of-Order로 실행되더라도 결과를 In-Order로 커밋(commit) 할 수 있게 관리.
필요성:

예외 상황 처리의 정합성 보장.
레지스터 상태 보호.
정적 프로그램 순서 유지.
5. Hybrid 메모리 관리 (Paging + Segmentation)
개념:

세그먼트 기반 주소 공간을 페이지 단위로 관리하는 방식.
세그먼트는 논리적 의미(코드, 데이터, 스택) 제공 + 각 세그먼트는 다시 페이지 단위로 나뉨.
장점:

유연한 메모리 보호 + 외부 단편화 방지.
주로 x86에서 사용됨.
6. Row Hammer 공격
개념:
DRAM에서 특정 메모리 셀(row)을 빠르게 반복적으로 액세스하면 인접 셀의 전하가 누출되어 비의도적인 비트 반전이 발생하는 하드웨어 취약점.

방어 기술:

TRR (Target Row Refresh): 공격 감지 시 주변 row 자동 새로고침.
ECC 메모리: 오류 탐지/수정.
Row Hammer Guarding 회로 탑재한 DRAM 설계.
7. TLB Shootdown
개념:
멀티코어 환경에서 하나의 코어가 페이지 테이블을 변경하면, 다른 코어들의 TLB에 남아있는 해당 페이지 정보를 무효화해야 하는 현상.

효율적 관리 방법:

IPI (Inter-Processor Interrupt)를 통한 TLB flush.
최근 아키텍처에서는 Selective TLB shootdown을 통한 범위 제한, 최적화된 sync 기법 사용.
8. Memory Fence (메모리 펜스)
개념:
CPU가 메모리 접근 순서를 자동으로 재정렬하는 것을 제어하기 위한 명령어로, 명령어 실행 순서를 보장해주는 동기화 도구.

동기화 역할:

스레드 간 공유 자원 접근 시 읽기/쓰기 순서를 명확하게 보장.
멀티코어 시스템에서 명령어 및 데이터 일관성 유지에 핵심적.


- Write Amplification(쓰기 증폭)이란 무엇이며, 이를 최소화하기 위한 SSD 최적화 방법은?
- Near Memory Computing(NMC)의 개념과 기존 메모리 구조와의 차이점은?
- Load/Store Queue의 개념과 역할은?
- 메모리 주소 공간에서 Direct Mapped Cache와 Set-Associative Cache의 차이점은?
- 메모리 컨트롤러의 역할과 성능 최적화 방법은?
- Prefetch Buffer(프리페치 버퍼)의 역할과 성능 향상 효과는?
- CPU와 FPGA의 차이점과 FPGA의 장점은?
    - 1. Write Amplification (쓰기 증폭)
개념:
SSD에서 사용자 요청보다 더 많은 데이터가 실제로 플래시에 쓰이는 현상으로, 성능 저하와 수명 단축의 원인.

발생 원인:

SSD는 페이지 단위로 쓰고 블록 단위로 삭제.
작은 변경에도 전체 블록 복사 및 갱신 필요.
최소화 방법:

Over-Provisioning: 여유 공간 확보로 병목 완화.
TRIM 명령어 지원: 불필요한 블록 사전 정리.
Garbage Collection 최적화: 쓰기 병합 최소화.
Write Coalescing: 연속 쓰기로 병합 처리.
2. Near Memory Computing (NMC)
개념:
데이터를 메모리 근처(또는 메모리 내부)에서 직접 처리하는 컴퓨팅 방식으로, 데이터 이동을 최소화.

기존 구조와 차이점:

기존은 프로세서 ↔ 메모리 간 왕복 전송 필요.
NMC는 연산을 메모리 컨트롤러나 메모리 칩 내부에서 직접 처리.
장점:

대역폭 병목 완화.
에너지 효율 향상.
AI/빅데이터 처리에 적합.
3. Load/Store Queue
개념:
Out-of-Order 프로세서에서 메모리 명령어의 순서 및 의존성 관리를 위한 대기 구조.

역할:

Load Queue: 이전 store가 있는 경우 데이터 Forwarding.
Store Queue: Store 명령어 지연 시 결과 보관 및 순서 보장.
메모리 접근 순서, 의존성 검사, 예외 처리 등에 관여.
4. Direct Mapped Cache vs. Set-Associative Cache
Direct Mapped Cache:

각 블록이 고정된 하나의 캐시 라인에만 저장 가능.
단순하고 빠르지만 충돌(Collision) 빈번.
Set-Associative Cache:

각 블록이 n개의 라인 중 한 곳에 저장 가능 (예: 4-way).
유연성은 높지만, 복잡도와 접근 시간 약간 증가.
5. 메모리 컨트롤러
역할:

CPU와 메모리 간 명령 스케줄링, 타이밍 조율, 버퍼 관리 등을 수행.
주소 변환, ECC 처리 등도 포함.
성능 최적화 방법:

Bank Interleaving, 명령 재정렬.
QoS 기반 스케줄링.
DRAM 타이밍 파라미터 최적화.
6. Prefetch Buffer (프리페치 버퍼)
개념:
CPU가 곧 필요할 것으로 예측되는 데이터를 미리 로드하여 대기 상태로 두는 버퍼.

성능 향상 효과:

메모리 접근 지연 최소화.
캐시 미스를 줄이고 캐시 히트율 향상.
순차 접근이 많은 워크로드에 효과적.
7. CPU vs. FPGA
CPU (Central Processing Unit):

범용 계산 장치, 소프트웨어 기반 명령어 실행.
적은 병렬성, 높은 유연성.
FPGA (Field Programmable Gate Array):

하드웨어 회로를 프로그래머블하게 구성.
병렬 처리 성능 우수, 고속 처리 가능.
지연 시간 최소화, 저전력 AI 가속기/통신 처리에 적합.
FPGA 장점:

병렬성 극대화.
실시간 처리 성능.
하드웨어 수준의 보안/속도.

- 1. SIMD vs. SIMT
SIMD (Single Instruction Multiple Data):

하나의 명령어가 여러 데이터에 동일하게 적용됨.
CPU의 벡터 연산, GPU의 연산 유닛 등에서 활용됨.
예: Intel AVX, ARM NEON
SIMT (Single Instruction Multiple Threads):

GPU에서 사용되는 모델로, 여러 스레드가 하나의 명령어 스트림을 공유하되, 각 스레드는 고유한 제어 흐름을 가짐.
실행 유닛은 SIMD지만, 추상화 계층은 멀티스레딩 형태.
NVIDIA CUDA 아키텍처가 대표적.
2. TPU vs. GPU
TPU (Tensor Processing Unit):

AI 특화 하드웨어. 텐서 연산을 고속 처리하는 매트릭스 곱셈 가속기 포함.
연산 효율성과 전력 효율에 초점.
주로 행렬 연산 중심의 딥러닝 추론 및 학습에 최적화.
GPU (Graphics Processing Unit):

범용 병렬 처리 유닛. SIMT 구조 기반.
연산 유연성 높고, AI 외에도 그래픽, 시뮬레이션 등에 사용.
차이점 요약:

TPU는 AI 전용, 고정 연산 패턴에 효율적.
GPU는 범용, 다양한 병렬 연산에 강점.
3. Pipelined vs. Non-Pipelined ALU
Pipelined ALU:

ALU 내부 연산 단계를 파이프라인화하여 동시에 여러 연산 처리 가능.
각 사이클마다 새로운 연산 입력 가능.
높은 처리량, 낮은 지연 시간.
Non-Pipelined ALU:

하나의 연산이 끝나야 다음 연산이 가능.
단순 구조, 낮은 설계 복잡성.
비교:

Pipelined ALU는 고속 처리, 고성능 CPU에서 주로 사용.
Non-Pipelined은 단순 제어기, 소형 임베디드 장치에 적합.
4. Coarse-Grained vs. Fine-Grained Multithreading
Fine-Grained Multithreading:

매 사이클마다 스레드를 교체하여 지연 시간 은폐.
컨텍스트 스위칭 오버헤드가 낮음, 파이프라인 스톨 방지.
예: 일부 GPU, 고성능 네트워크 장비
Coarse-Grained Multithreading:

캐시 미스 등 주요 이벤트 발생 시에만 스레드를 전환.
컨텍스트 스위칭 오버헤드는 크지만, 스레드당 처리 집중도 높음.
활용:

Fine-Grained: 지연 은폐 중심 설계에 효과적.
Coarse-Grained: 각 스레드 성능 중시 환경에 적합.
5. Shared Memory vs. Distributed Memory
Shared Memory 모델:

모든 프로세스/스레드가 동일한 메모리 공간을 공유.
동기화 필요 (Mutex, Lock 등).
예: 멀티코어 CPU 시스템, OpenMP
Distributed Memory 모델:

각 노드가 자신의 메모리를 가지고 통신(MPI 등)으로 데이터 교환.
동기화보다 데이터 전달 방식 설계 중요.
예: 클러스터, 슈퍼컴퓨터 환경
비교:

Shared: 빠른 접근, 설계 단순하나 동기화 문제 있음.
Distributed: 확장성 우수, 통신 지연 고려 필요.

- GPU에서 Thread Divergence란?
Thread Divergence:

GPU의 SIMT 구조에서는 한 워프(warp, 32 threads 등)가 동일한 명령어를 실행해야 최적 성능.
if/else 조건 분기로 워프 내 스레드가 다른 경로로 분기되면 실행이 순차화되어 성능 저하 발생.
방지 방법:

조건문 최소화 및 스레드 간 경로 통일화.
Warp Specialization을 통해 역할 구분.
분기 발생 시 조건별 워프 분리도 고려.
2. Heterogeneous Computing(이기종 컴퓨팅)
정의:

CPU, GPU, TPU, FPGA 등 서로 다른 연산 자원을 통합하여 특정 작업을 최적화하는 구조.
각 처리 장치의 장점을 살려 분산 작업 수행.
사용 환경:

AI 연산: GPU + TPU
고속 영상 처리: CPU + FPGA
임베디드 디바이스: CPU + NPU
고성능 서버: CPU + GPU 클러스터
3. Asynchronous vs. Synchronous Execution
동기 실행 (Synchronous):

명령어가 순차적으로 실행되며, 앞선 작업 완료 후 다음 단계 진행.
코드 이해는 쉬우나, I/O 블로킹 및 병렬성 낮음.
비동기 실행 (Asynchronous):

명령어 실행 후 완료를 기다리지 않고 다음 작업 진행.
병렬성 향상, I/O 작업 최적화에 유리.
예: GPU 커널 실행, 비동기 네트워크 전송
4. Persistent Memory(지속성 메모리)
정의:

전원이 꺼져도 데이터가 유지되는 비휘발성 메모리(예: Intel Optane DC PM).
DRAM처럼 빠르면서 SSD처럼 영구 저장 가능.
활용 사례:

대용량 인메모리 데이터베이스
저지연 로그 저장
재시작 복원 성능 향상 (비휘발성 체크포인트)
5. PCIe Lane과 Bandwidth의 관계
Lane:

데이터 전송을 위한 직렬 통신 채널의 쌍(송신/수신).
PCIe x1, x4, x8, x16 등으로 구성되며, Lane 수가 많을수록 대역폭 증가.
Bandwidth 관계:

PCIe Gen별로 Lane 당 속도는 정해져 있으며, 예: PCIe Gen 4 기준 Lane당 약 2GB/s.
x16 사용 시 2GB/s × 16 = 32GB/s 대역폭 확보 가능.
6. Direct I/O vs. Memory-Mapped I/O
Direct I/O:

장치 드라이버를 통해 명령어 기반 데이터 입출력.
CPU가 명령어로 I/O 요청 → OS가 제어.
I/O 제어는 명확하지만 성능 한계 존재.
Memory-Mapped I/O:

I/O 장치를 메모리 주소 공간에 매핑.
장치 레지스터나 버퍼를 메모리처럼 접근 가능.
캐시 제어 유의해야 하지만 성능 우수.
7. RDMA(Remote Direct Memory Access)
정의:

네트워크를 통해 상대방 메모리에 직접 읽기/쓰기할 수 있는 기술.
CPU 개입 없이 데이터 전송 → 낮은 지연, 높은 처리량.
TCP/IP와의 차이:

일반 TCP는 커널 스택, 복사 과정을 거치므로 지연 큼.
RDMA는 Zero Copy, Kernel Bypass가 가능.
데이터 센터 고성능 컴퓨팅, 스토리지 전송, AI 클러스터에서 필수 기술.

- Zero-Copy I/O 기술의 개념과 성능 향상 효과
개념:

전통적인 I/O는 사용자 공간 ↔ 커널 공간 간에 데이터 복사가 반복됨.
Zero-Copy는 복사 없이 직접 송수신 버퍼에 접근함으로써 데이터 이동을 최소화하는 기술.
DMA(Direct Memory Access), mmap(), sendfile() 등이 활용됨.
성능 효과:

CPU 오버헤드 감소: 복사 과정 생략으로 불필요한 연산 감소.
메모리 대역폭 절약: 메모리 간 복사 최소화.
지연 시간 감소: 대용량 파일 처리나 네트워크 I/O에서 성능 향상.
2. 인터럽트의 벡터 처리 방식 (Vector Interrupt Processing)
개념:

인터럽트 요청이 발생했을 때 벡터 번호(Vector Number)를 기반으로 특정 인터럽트 처리 루틴의 주소로 직접 점프하는 방식.
인터럽트 우선순위와 식별이 정적으로 매핑되어 있음 (예: x86의 IDT 구조).
특징:

빠른 응답성 확보.
다중 장치 환경에서 정확한 인터럽트 소스 식별 가능.
대부분의 현대 CPU 아키텍처가 채택.
3. NVMe Direct Storage의 개념과 기존 스토리지 방식과의 차이점
NVMe Direct Storage:

GPU, CPU가 OS 커널이나 파일 시스템을 거치지 않고 NVMe SSD에 직접 접근하는 방식.
Microsoft의 DirectStorage API나 GPU DSA(Direct Storage Access) 같은 기술에서 구현됨.
기존 방식과 차이:

기존: CPU → OS I/O 스택 → SSD
Direct: GPU/CPU → NVMe SSD (직접 I/O)
장점:

레이턴시 감소.
게임, AI, 대용량 데이터 로딩 속도 향상.
4. Adaptive Interrupt Moderation (AIM, 적응형 인터럽트 조절)
개념:

네트워크 인터페이스 카드(NIC)에서 인터럽트를 패킷 수나 부하에 따라 동적으로 조절하는 기법.
수신 속도가 높을 경우 인터럽트를 모아서 처리하고, 낮을 땐 실시간에 가깝게 처리.
활용 사례:

고속 이더넷 환경에서 인터럽트 폭주 방지.
클라우드 서버, 고성능 라우터, 가상화 환경에서 네트워크 처리 최적화.
5. Infiniband 네트워크의 개념과 고성능 컴퓨팅에서의 활용 사례
개념:

초고속, 저지연 네트워크 아키텍처. 주로 RDMA를 지원하고, 100Gbps 이상의 대역폭 제공.
CPU와 메모리 간 통신처럼 서버 간 메모리 직접 접근이 가능.
활용 사례:

HPC(High-Performance Computing) 클러스터
AI 모델 학습용 데이터센터
대용량 병렬 연산을 수행하는 슈퍼컴퓨터
6. Host Channel Adapter (HCA)의 개념과 역할
개념:

Infiniband 네트워크에서 서버와 네트워크 스위치를 연결하는 인터페이스 장치.
NIC와 유사하나, Infiniband 전용 기능 포함 (RDMA, Zero-Copy, Kernel Bypass 등).
역할:

서버 메모리와 리모트 노드 간의 고속 메모리 접근 지원.
대역폭 제어 및 패킷 처리 등 고속 네트워크 환경을 위한 핵심 구성요소.
7. Jumbo Frame(점보 프레임)의 개념과 장단점
개념:

이더넷 표준 프레임 크기(1500 바이트)를 넘어 9000 바이트 이상의 MTU를 갖는 프레임.
네트워크 장치가 이를 모두 지원해야 사용 가능.
장점:

패킷 수 감소 → CPU 오버헤드 감소
처리 효율 증가: 대량 데이터 전송에 유리 (예: 빅데이터, 클러스터 통신)
단점:

장치 간 호환성 문제 발생 가능
에러 발생 시 재전송 부담 커짐
네트워크 전 구간에서 일관된 설정 필요

- CPI(Cycles Per Instruction)와 IPC(Instructions Per Cycle)의 차이점
CPI (Cycles Per Instruction):

한 명령어를 실행하는 데 걸리는 평균 클럭 사이클 수.
낮을수록 성능이 좋음.
계산식: CPI = Total Cycles / Instruction Count
IPC (Instructions Per Cycle):

클럭 사이클당 실행할 수 있는 명령어 수.
높을수록 병렬 실행 능력이 뛰어남.
계산식: IPC = Instruction Count / Total Cycles
차이점:

CPI는 지연 시간 중심 지표, IPC는 처리량 중심 지표.
서로 역수 관계: IPC = 1 / CPI (단, 단일 쓰레드 기준)
2. Benchmarks의 종류와 사용 목적
SPEC (Standard Performance Evaluation Corporation):

CPU와 메모리 성능 측정 중심.
예: SPECint, SPECfp, SPECjbb.
TPC (Transaction Processing Performance Council):

데이터베이스 및 트랜잭션 처리 성능 측정.
예: TPC-C, TPC-H.
LINPACK:

과학 계산 성능, 주로 HPC 성능 (FLOPS) 평가.
슈퍼컴퓨터 랭킹 (Top500)에 사용.
사용 목적:

시스템 간 성능 비교.
병렬 처리/데이터베이스/과학연산 등 분야별 최적 성능 평가.
3. Amdahl’s Law를 적용할 때 병렬 처리 속도 향상 한계
개념:

전체 작업 중 병렬화 가능한 비율이 아무리 높더라도 직렬 처리 부분이 존재하면 전체 속도 향상에는 한계가 있음.
공식:

Speedup ≤ 1 / (S + (1 - S) / N)
S: 직렬 처리 비율, N: 프로세서 수
속도 향상 한계:

직렬 구간이 10%만 되어도 무한히 많은 프로세서를 투입해도 최대 10배 이상 성능 개선 불가.
병렬성 향상보다 직렬 구간 최소화가 중요함을 강조.
4. Roofline Model의 개념과 활용
개념:

CPU 또는 GPU의 성능 한계와 병목 요인을 시각화한 모델.
X축: 연산 강도 (FLOPS/byte), Y축: 성능(FLOPS)
두 가지 성능 경계: Memory Bandwidth Ceiling과 Compute Peak Ceiling
활용법:

연산이 메모리 병목인지 연산 병목인지 판단 가능.
최적화 방향 결정에 도움: 메모리 최적화 vs 계산 최적화.
5. Hot Spot Analysis(핫스팟 분석)의 개념과 최적화 기법
개념:

실행 시간의 상당 부분을 소비하는 코드 영역을 식별하는 작업.
성능 튜닝 시 가장 먼저 집중해야 할 타겟.
기법:

프로파일링 도구 사용: perf, gprof, Intel VTune, Flame Graph.
핫스팟 코드에 대해 루프 최적화, 병렬화, 알고리즘 개선 적용.
6. CPU 사용률(Usage)과 CPU Stall(스톨) 간의 관계
CPU Usage:

CPU가 실제로 일을 수행한 시간 비율.
높다고 해서 무조건 효율적인 것은 아님.
CPU Stall:

CPU가 명령 실행을 기다리며 놀고 있는 시간 (예: 메모리 지연, 파이프라인 정지).
Stall이 많으면 Usage는 높아도 실제 처리량이 낮아질 수 있음.
관계:

사용률이 높더라도 Stall 비율이 높으면 성능 저하 발생.
CPU 성능 향상을 위해선 Stall을 줄이는 것이 핵심 (예: 프리페치, 파이프라인 최적화).

- Performance Counter(성능 카운터)란 무엇이며, 이를 활용한 시스템 성능 분석 방법은?
개념:
CPU 내부에서 하드웨어적으로 제공되는 이벤트 계측 장치로, 명령어 수, 캐시 미스, 분기 실패, 사이클 수 등의 성능 이벤트를 계측함.

활용 방법:
성능 튜닝 시 사용자가 특정 코드 블록의 실행 동안 몇 번의 캐시 미스가 있었는지, 분기 예측 실패가 발생했는지 등을 추적하여 병목 지점 분석에 활용됨.

대표 도구:
perf(리눅스), Intel VTune, AMD uProf, PAPI, oprofile.

2. Instruction Fusion이란 무엇이며, 어떻게 성능을 최적화하는가?
개념:
CPU가 두 개 이상의 명령어를 하나의 마이크로-옵(micro-op)으로 결합하여 실행하는 최적화 기법.

예시:
cmp + jmp 명령이 결합되어 분기 비교와 실행을 하나의 마이크로-옵으로 수행.

효과:
명령어 디코딩 수를 줄이고, 파이프라인 효율을 높여 IPC 향상 및 전력 절감 가능.

3. Latency와 Throughput의 차이점과 성능 측정 시 고려해야 할 요소는?
Latency (지연 시간):

하나의 작업이 완료되기까지 걸리는 시간 (예: 한 패킷이 수신되는 데 걸리는 시간).
단일 작업 처리 성능을 평가할 때 중요.
Throughput (처리량):

단위 시간당 처리 가능한 작업 수 (예: 초당 1000건 처리).
시스템의 전반적인 처리 효율성을 평가.
성능 측정 시 고려:

지연 시간은 실시간성 요구 시스템에서 중요.
처리량은 배치 처리, 서버 부하 처리 능력에서 중요.
두 지표는 상호 보완적으로 사용해야 함.
4. Processor Affinity(프로세서 친화도)란 무엇이며, 성능 최적화에 어떤 영향을 미치는가?
개념:
특정 스레드를 특정 CPU 코어에 고정해서 실행하도록 설정하는 방식.

효과:

캐시 지역성(Locality) 유지: 동일한 코어에서 반복 실행되면 캐시 재사용 가능.
컨텍스트 스위치 비용 감소: 코어 간 이동 시 발생하는 오버헤드 방지.
활용 예시:

고성능 서버에서 네트워크 인터럽트를 특정 코어에 바인딩.
고부하 스레드를 별도 코어에 고정하여 자원 충돌 방지.
5. Quantum Supremacy(양자 우위)의 개념과 현재 연구 동향은?
개념:
양자 컴퓨터가 고전 컴퓨터로는 실질적으로 불가능한 문제를 일정 시간 내 해결할 수 있음을 입증한 상태.

주요 사례:

2019년 구글이 ‘Sycamore’ 양자 프로세서로 특정 무작위 회로 문제를 수초 내 해결한 사례 발표.
IBM은 동일 문제를 고전 컴퓨터로도 해결할 수 있다고 반박.
연구 동향:

전통적인 양자 알고리즘(Shor, Grover) 외에 NISQ(Noisy Intermediate-Scale Quantum) 시스템을 활용한 특화된 문제 영역에서 우위 입증 시도 중.
응용 분야: 암호 해독, 최적화, 머신러닝 등.
6. Memristor의 개념과 기존 트랜지스터 기반 메모리와의 차이점은?
개념:
전하가 흐른 기록 이력을 기억하는 비휘발성 메모리 소자. 전류 방향과 크기에 따라 저항 값이 변화하며, 전원이 꺼져도 그 상태를 유지함.

기존 메모리와의 차이점:

기억 소자와 연산 소자를 통합할 수 있어 PIM(Processing-In-Memory)에 적합.
DRAM/NAND보다 빠르고 전력 소모가 낮음.
트랜지스터 필요 없이 단층 구조 구현 가능.
활용 가능성:

뉴로모픽 컴퓨팅, 차세대 비휘발성 메모리, 초저전력 IoT 시스템.

- DNA Computing이란 무엇이며, 기존 컴퓨팅과의 차이점은?
개념:
DNA의 염기서열을 활용하여 계산 문제를 생화학적으로 해결하는 컴퓨팅 방식. 물리적인 전자 소자 대신 생물학적 분자(DNA)를 연산 수단으로 사용함.

기존 컴퓨팅과의 차이점:

전기적 신호가 아닌 생화학 반응 기반 연산.
병렬 계산 능력이 탁월하여 NP-완전 문제 해결에 이론적으로 유리.
속도보다 병렬성과 에너지 효율성에 초점.
실용화는 아직 초기 단계이며, 바이오 실험 환경 필요.
2. RRAM(Resistive RAM)의 개념과 기존 NAND Flash와의 차이점은?
개념:
저항의 상태 변화를 통해 데이터를 저장하는 비휘발성 메모리. 전압을 가해 전도성 필라멘트를 형성하거나 끊어 저항 상태를 변화시킴.

차이점:

NAND Flash는 부도체-절연체의 전하 트래핑 구조, RRAM은 저항 변화 기반.
RRAM은 쓰기 속도 빠름, 낮은 전력 소비, 높은 내구성.
구조가 단순하여 3D 적층에 유리함.
아직 상용화 초기 단계이지만 IoT, AI 연산용 메모리로 주목받음.
3. Optical Computing(광 컴퓨팅)의 원리와 기존 전자 기반 컴퓨팅과의 차이점은?
개념:
전자 대신 빛(광자, Photon)을 활용하여 데이터 전송과 처리를 수행하는 컴퓨팅 기술.

차이점:

속도: 전자보다 광자가 빠르게 전달됨.
발열: 전자 회로보다 발열이 적고 에너지 효율이 높음.
병렬성: 여러 파장의 빛을 동시에 다룰 수 있어 병렬 처리에 유리.
그러나 소형화 및 광 집적 기술은 아직 개발 진행 중이며, 실용화는 제한적.
4. Edge AI 프로세서의 개념과 기존 클라우드 AI와의 차이점은?
개념:
디바이스(스마트폰, 카메라 등) 자체에서 AI 연산을 실시간 수행할 수 있도록 설계된 경량 AI 전용 칩 또는 SoC(System on Chip).

차이점:

클라우드 AI는 연산을 중앙 서버에서 처리하지만, Edge AI는 로컬 장치에서 직접 처리함.
실시간 응답성과 데이터 프라이버시 보호, 네트워크 의존도 감소가 장점.
예시: Apple Neural Engine, Google Edge TPU, NVIDIA Jetson.
5. 3D TSV(Through-Silicon Via) 기술이란 무엇이며, 기존 반도체 설계와의 차이점은?
개념:
실리콘 웨이퍼를 수직 방향으로 관통하는 전극(비아)을 통해 칩 간 고속 연결을 가능하게 하는 3D 집적 기술.

기존 설계와 차이점:

기존은 2D 평면적 설계(칩 옆으로 배치), TSV는 칩을 수직으로 적층.
전송 거리 단축, 전력 소비 감소, 대역폭 증가 효과.
고속 메모리(HBM), AI 가속기 등 고성능 칩에 활용.
6. Nanosheet Transistor(나노시트 트랜지스터)란 무엇이며, 기존 FinFET과의 차이점은?
개념:
채널을 시트(sheet) 형태로 제작하고, 게이트가 이를 완전히 감싸는 구조의 차세대 트랜지스터. 3nm 이하 공정에 사용됨.

FinFET과의 차이점:

FinFET은 돌출된 날(fin) 형태, 나노시트는 널찍한 판 모양의 시트를 수직으로 쌓음.
나노시트는 채널 폭을 조절 가능, 더 세밀한 성능/전력 제어.
향후 GAA(Gate-All-Around) 구조로 진화 예정.

- 탄소 나노튜브 기반 트랜지스터의 개념과 기존 실리콘 반도체와의 차이점은?
개념:
탄소 나노튜브(Carbon Nanotube, CNT)는 원자 한 층 두께의 그래핀을 말아 만든 나노 구조로, 이를 이용한 트랜지스터는 전류 운반 효율이 매우 높고, 누설 전류가 적음.

기존 실리콘 대비 차이점:

전하 이동도(Electron Mobility)가 실리콘보다 수십 배 뛰어남.
더 낮은 전압으로도 스위칭이 가능하여 전력 소비가 매우 낮음.
공정 난이도가 높고, 정렬·순도 문제 등 상용화에는 기술적 도전이 있음.
2. Photonic Neural Networks(광 뉴럴 네트워크)의 개념과 활용 가능성은?
개념:
빛(광자)을 이용해 인공신경망의 연산을 수행하는 기술. 전기 신호 대신 광 신호를 활용하여 행렬 연산을 병렬로 처리함.

활용 가능성:

대규모 행렬 곱셈 연산(MAC)에 적합하며, 전력 효율과 연산 속도가 탁월함.
AI 엣지 디바이스, 초고속 inference 등에 유망.
아직은 회로 소형화, 집적도 문제, 오류율 개선 등의 과제가 존재.
3. CPU에서 Instruction Fusion이란 무엇이며, 성능 최적화에 어떤 영향을 미치는가?
개념:
두 개 이상의 명령어를 하나의 마이크로-연산(Micro-op)으로 결합하여 처리하는 최적화 기술. 예: 비교 후 분기 → 하나의 조건 분기 명령어로 결합.

성능 영향:

디코딩 비용 감소, 마이크로 연산 수 감소 → 파이프라인 혼잡도 완화.
인스트럭션 캐시 부담 완화 및 명령어 처리 속도 향상.
현대 x86 CPU에서 폭넓게 사용됨 (예: Intel의 Macro-op Fusion).
4. Tomasulo’s Algorithm이란 무엇이며, 명령어 스케줄링에서 어떤 역할을 하는가?
개념:
동적 명령어 스케줄링 알고리즘으로, 명령 간 데이터 의존성 해소, 레지스터 충돌 회피, Out-of-Order 실행을 가능하게 함.

주요 역할:

Reservation Station을 활용하여 명령어를 동적으로 발행.
레지스터 대신 태그 기반 데이터 추적.
Reorder Buffer와 함께 동적 명령어 실행 엔진의 핵심 요소.
현대 CPU에서 성능 향상에 기여하는 중추적 기술.
5. Value Prediction(값 예측)의 개념과 성능 최적화 기법은?
개념:
연산 결과 값을 예측하여 명령어 실행을 미리 수행하는 기법. 예측이 정확할 경우, 레지스터 의존성을 회피하고 병렬성을 극대화할 수 있음.

성능 최적화 기법:

Constant Value Prediction: 반복적으로 같은 값이 나올 경우 활용.
Last Value Prediction: 직전 결과를 예측 값으로 사용.
Stride Prediction: 값의 증가 폭을 기반으로 예측.
예측 실패 시 롤백 오버헤드 존재하므로 정확도가 핵심.
6. Micro-Op Cache(마이크로 연산 캐시)란 무엇이며, 성능 향상에 어떤 역할을 하는가?
개념:
디코딩된 마이크로 연산(Micro-Op)을 캐시에 저장해, 동일한 명령어가 반복될 경우 디코딩 과정을 생략하는 CPU 캐시 구조.

성능 역할:

디코더 부하 감소 → 파워 효율 향상.
반복되는 루프 코드에서 명령어 처리 속도 향상.
최신 x86 아키텍처(Intel, AMD)에서 적극 사용되고 있음 (e.g., Intel의 μOP Cache).

- Speculative Execution에서 Spectre와 Meltdown 공격이 발생하는 원리는?
Speculative Execution 개요:
CPU가 분기 명령의 결과를 예측하여, 예측된 경로의 명령어를 미리 실행하는 기법. 성능을 높이기 위해 광범위하게 활용됨.

Meltdown:

발생 원인: 사용자 모드에서 커널 메모리에 접근이 허용되지 않지만, Speculative Execution 중 일시적으로 접근이 가능한 상태가 생김.
효과: 캐시에 로드된 정보를 사이드 채널을 통해 추출할 수 있음.
Spectre:

발생 원인: 브랜치 예측을 악의적으로 조작하여 허용되지 않은 메모리 접근을 유도.
효과: 타 스레드나 프로세스의 메모리 내용 노출 가능.
공통점: 모두 CPU의 고성능 기술인 Speculative Execution을 악용하여 캐시 측면 채널을 통해 비인가 데이터를 추출함.

2. Loop Unrolling(루프 언롤링)이란 무엇이며, CPU 성능 최적화에서 어떤 역할을 하는가?
개념:
반복문 내의 명령어를 여러 번 미리 전개하여 루프 횟수를 줄이는 최적화 기법.

성능 최적화 효과:

루프 제어 오버헤드(조건 검사, 점프 등) 감소.
명령어 병렬 실행 가능성 증가 → ILP(Instruction Level Parallelism) 향상.
캐시 및 파이프라인 효율 향상.
주의사항: 코드 크기 증가로 인해 I-Cache 미스가 늘어날 수 있음.

3. Operand Forwarding(피연산자 포워딩)이란 무엇이며, 파이프라인 성능에 어떤 영향을 미치는가?
개념:
파이프라인에서 앞선 명령어의 실행 결과가 아직 레지스터에 저장되기 전에, 이를 뒤따르는 명령어에 직접 전달하여 사용하는 기법.

성능 효과:

데이터 의존으로 인한 파이프라인 스톨(대기) 방지.
파이프라인의 연속성과 처리율 향상에 기여.
4. Register Bypassing(레지스터 바이패싱)이란 무엇이며, 연산 속도를 높이기 위한 전략은?
개념:
이전 연산 결과를 레지스터 파일에 기록하지 않고, 바로 다음 연산에 직접 연결하여 전달하는 방식.

전략 및 장점:

Operand Forwarding의 구현 형태 중 하나.
스톨 없이 파이프라인 진행 가능, 특히 RISC 아키텍처에서 중요한 최적화.
5. Shadow Register(섀도 레지스터)의 개념과 활용 사례는?
개념:
기존 레지스터와 동일한 역할을 하는 백업 레지스터 집합으로, 컨텍스트 스위칭 시 레지스터 값을 빠르게 저장하거나 복원할 수 있게 함.

활용 사례:

인터럽트 처리 시 빠른 상태 전환.
스레드/태스크 간 컨텍스트 전환 속도 향상.
실시간 시스템, 임베디드 시스템 등에서 자주 사용됨.
6. Stack Machine과 Register Machine의 차이점은?
Stack Machine:

연산자가 스택에 쌓이고, 스택의 top 두 개 값을 사용하여 연산 수행.
명령어가 간단하고, 코드 크기가 작음.
예: Java Virtual Machine(JVM).
Register Machine:

명령어가 명시적으로 레지스터를 지정하여 연산 수행.
명령어 복잡성은 있지만, 더 빠르고 유연한 최적화 가능.
예: 대부분의 현대 CPU(ARM, x86 등).
차이점 요약:

접근성: Stack은 암시적, Register는 명시적.
성능: Register가 일반적으로 빠르고 효율적.
컴파일러 최적화: Register 기반 구조가 유리함.


- DRAM에서 Bank Interleaving(뱅크 인터리빙)이란 무엇이며, 성능 향상 효과는?
개념:
DRAM 내부에는 여러 개의 뱅크(Bank)가 존재하며, Bank Interleaving은 여러 뱅크에 데이터를 분산 배치하고 병렬 접근을 가능하게 하는 기술입니다.

성능 향상 효과:

명령어나 데이터 접근이 동시에 여러 뱅크에서 이루어지므로 메모리 접근 지연(latency) 감소.
메모리 대역폭 증가, 병렬 처리 성능 개선.
2. DDR3, DDR4, DDR5의 차이점과 성능 개선 요소는?
DDR3:

데이터 전송 속도: 최대 2133 MT/s
전압: 1.5V
기본 프리페치: 8n
DDR4:

속도 증가 (최대 3200 MT/s)
전압 감소 (1.2V)
Bank Group 구조 도입 → 병렬성 향상
Command Bus 복잡도 향상
DDR5:

속도 대폭 향상 (최대 8400 MT/s 이상)
전압 감소 (1.1V)
Doubled Bank Group, Dual-Channel DIMM
Power Management IC(PMIC) 모듈 통합
주요 차이점 요약: 전송 속도, 전력 효율, 병렬 접근 구조 개선 등으로 고성능 및 고효율 달성.

3. Persistent Memory(지속성 메모리)와 기존 DRAM/NAND Flash의 차이점은?
Persistent Memory (예: Intel Optane DC PMem):

DRAM처럼 빠르면서도 비휘발성.
전원이 꺼져도 데이터 보존 가능.
차이점 정리:

DRAM: 빠르고 휘발성
NAND Flash: 느리지만 비휘발성
Persistent Memory: 속도와 비휘발성을 절충한 메모리 계층
활용 사례:

인메모리 데이터베이스, Checkpointing, 빠른 재시작 등.
4. Hybrid Memory Cube(HMC)와 High Bandwidth Memory(HBM)의 차이점과 장점은?
HMC (Hybrid Memory Cube):

TSV(Through-Silicon Via) 기반 3D 구조로 DRAM 다이들을 스택.
별도 논리 컨트롤러 포함 → 고속 직렬 인터페이스 제공.
높은 병렬성과 속도.
HBM (High Bandwidth Memory):

GPU/AI용으로 특화된 메모리와 패키지 통합 설계.
낮은 전력 소비와 높은 대역폭 제공.
차이점 요약:

HMC는 DRAM + 로직 컨트롤러 통합.
HBM은 메모리 패키지 통합 중심.
HMC는 범용성/속도에, HBM은 GPU/AI/그래픽 성능에 초점.
5. DRAM의 Row Buffer Hit과 Miss가 성능에 미치는 영향은?
Row Buffer Hit:

현재 액세스하려는 데이터가 이미 열린 행(Row)에 존재.
빠른 접근(낮은 latency) 가능.
Row Buffer Miss:

새로운 행을 열기 위해 기존 행을 닫고 새로 활성화해야 함.
Latency 증가, 전력 소모 증가.
성능 영향:

Row Hit 비율이 높을수록 DRAM 액세스 성능이 향상됨.
메모리 컨트롤러 최적화로 Hit률을 높이기도 함.
6. Virtual Memory에서 Inverted Page Table 구조의 장점과 단점은?
개념:
기존 Page Table이 각 프로세스의 가상 페이지마다 존재하는 데 반해, Inverted Page Table은 물리 메모리의 각 프레임마다 하나의 엔트리만 유지하는 방식.

장점:

메모리 절약, 테이블 크기 작음.
시스템 전반의 관리가 효율적.
단점:

역방향 매핑이므로 가상 주소 → 물리 주소 검색이 느림.
해시 테이블 사용 필요 → 충돌 문제.
7. 메모리 압축 기술(Memory Compression)의 원리와 성능 향상 효과는?
원리:

DRAM에 데이터를 저장할 때, 데이터 패턴을 압축하여 더 많은 데이터를 저장함.
메모리 용량을 논리적으로 확장하는 효과.
성능 효과:

페이지 교체 감소 → Page Fault 감소.
물리 메모리 부족 상황에서 응답 속도 향상.
가상 메모리 효율 증가.
활용 사례:

운영체제 수준의 압축(ZRAM, ZSwap), 하드웨어 메모리 컨트롤러 압축 등.

- Software-Managed Cache와 Hardware-Managed Cache의 차이점
Software-Managed Cache:

캐시 데이터의 관리(할당/교체 등)를 소프트웨어(컴파일러, OS)가 직접 수행.
임베디드 시스템, GPU에서 사용.
개발자 제어 가능하지만 복잡함.
Hardware-Managed Cache:

하드웨어가 자동으로 캐시를 제어.
대부분의 CPU에서 사용됨.
간편하지만, 제어 유연성은 떨어짐.
2. Address Alias(주소 중복) 문제와 해결 방법
개념:

서로 다른 가상 주소가 동일한 물리 주소를 참조할 때 발생하는 문제.
특히 캐시에서 중복 데이터로 인해 캐시 일관성 문제, 불필요한 무효화 등이 발생.
해결 방법:

페이지 색상(Page Coloring) 기법으로 물리 주소를 제어.
메모리 접근 시 명시적 flush 또는 invalidate 사용.
캐시 인덱싱 방식 개선 (물리 주소 기반 인덱싱 등).
3. Memory Consistency Model(메모리 일관성 모델)
개념:

멀티프로세서 시스템에서 메모리 읽기/쓰기 순서에 대한 규칙을 정의.
프로세서 간 메모리 접근 순서를 어떻게 보장할지 결정.
대표적인 모델:

Sequential Consistency: 모든 프로세스가 같은 순서로 메모리 접근을 관찰.
Total Store Order (TSO): 대부분의 x86 CPU에서 사용, Load-Store 순서만 유지.
Relaxed Consistency: ARM, POWER 아키텍처에서 사용, 성능 최적화를 위해 순서 유연성 보장.
Release Consistency, Acquire Consistency 등도 존재.
4. Spinlock과 Mutex의 차이점과 활용 사례
Spinlock:

Lock이 해제될 때까지 계속 반복 루프(바쁘게 대기).
짧은 임계 구역, 컨텍스트 스위칭이 비용이 클 때 적합.
커널 또는 실시간 시스템에서 자주 사용.
Mutex:

Lock이 해제될 때까지 스레드를 대기열에 넣고 차단(Blocking).
긴 임계 구역에 적합, CPU 낭비 줄임.
사용자 공간 프로그램이나 POSIX 스레드 등에서 일반적으로 사용.
5. Read-Copy-Update(RCU) 기법
개념:

읽기 성능을 극대화하기 위해 읽기-쓰기 충돌 없이 동시 접근을 가능하게 하는 동기화 기법.
동작 방식:

쓰기 시 기존 데이터를 복사하여 수정 → 완료 후 포인터 전환.
읽기 스레드는 락 없이 읽기 가능.
쓰기 후 지연 해제(GP: Grace Period)를 통해 안전하게 메모리 해제.
활용 사례:

리눅스 커널, 트리 구조 탐색, 읽기 많은 공유 자원 관리.
6. Cache Coherence Protocol에서 MOESI와 MESI의 차이점
MESI (Modified, Exclusive, Shared, Invalid):

각 캐시 라인의 상태를 4단계로 구분하여 일관성 유지.
CPU 간 읽기/쓰기 충돌 시 캐시 상태 전환으로 조정.
MOESI (Modified, Owned, Exclusive, Shared, Invalid):

MESI에 Owned 상태 추가.
공유 중인 데이터를 한 CPU가 소유(Owned)하고, 다른 CPU는 읽기만 가능.
불필요한 메모리 쓰기 감소 → 버스 사용량 감소, 성능 향상.

- Directory-Based Cache Coherence의 개념과 장점
개념:

각 메모리 블록마다 디렉토리(Directory)를 유지하며, 그 블록을 캐시에 보유 중인 프로세서 목록을 관리.
요청이 있을 때 디렉토리에서 다른 캐시에 알려 일관성을 유지.
장점:

스케일 아웃 가능: 프로세서 수가 많아도 브로드캐스트 필요 없음.
네트워크 기반 멀티프로세서 시스템에서 유리.
트래픽 절감: 버스가 아닌 포인트-투-포인트 통신 구조를 활용.
2. Thread-Level Speculation (TLS)
개념:

병렬 처리 불가능해 보이는 코드(예: 루프 등)를 스레드 단위로 추측 실행한 후, 의존성 충돌이 없으면 결과 채택, 충돌 시 롤백.
활용 예:

컴파일러 또는 런타임 시스템이 제어 흐름이나 데이터 의존성 예측을 통해 speculative thread 생성.
예측 성공 시 병렬성 극대화 → 성능 향상.
장점:

프로그래머가 병렬성 명시하지 않아도 자동 병렬화 가능.
기존 코드의 성능 향상 가능성.
3. Data Prefetching vs Instruction Prefetching
Data Prefetching:

다음에 사용할 데이터를 메모리에서 캐시로 미리 불러오기.
메모리 접근 지연을 줄여 데이터 병목 완화.
Instruction Prefetching:

다음에 실행될 명령어를 캐시에 미리 적재.
분기 예측과 함께 사용되며 파이프라인 효율 향상.
성능 효과:

둘 다 캐시 미스 감소, 메모리 지연 최소화, CPI 개선에 기여.
4. NUMA-aware Memory Allocation
개념:

NUMA(Non-Uniform Memory Access) 구조에서 프로세서가 자신에 가까운 메모리 뱅크에 데이터 할당하도록 최적화하는 전략.
방법:

OS나 프로그래머가 메모리 바인딩(affinity) 또는 numactl 등 정책 활용.
스레드-메모리 바인딩으로 메모리 접근 레이턴시 최소화.
장점:

메모리 병목 해소, 캐시 지역성 향상, 성능 예측성 증가.
5. Weak Consistency vs Sequential Consistency
Sequential Consistency:

모든 연산이 프로그램 순서대로 실행되고, 모든 프로세스가 동일한 순서로 관찰.
Weak Consistency:

명시적 동기화 시점만 일관성 보장.
성능 향상을 위해 일부 순서 무시 가능.
비교:

Sequential은 간단하지만 느림.
Weak은 복잡하지만 성능 좋음, 예: ARM, POWER 아키텍처.
6. Write Combining 기술
개념:

여러 개의 작은 쓰기 연산을 하나의 큰 블록으로 결합하여 메모리에 쓰는 방식.
동작 원리:

CPU가 쓰기 버퍼에 데이터를 모았다가, 일정 기준 충족 시 한 번에 쓰기.
효과:

버스 트래픽 감소, 쓰기 병렬성 증가, 메모리 쓰기 성능 향상.
GPU 또는 I/O 연산에서 특히 효과적.

- Hardware Transactional Memory (HTM)
개념:
HTM은 CPU가 지원하는 하드웨어 수준의 트랜잭션 기반 병렬 처리 기법으로, 임계 구역(Critical Section)을 명시적 락 없이 실행하며 충돌 시 자동 롤백함.

역할:

락을 사용하지 않고 동기화를 처리하므로 데드락 회피, 락 경합 최소화, 성능 향상에 유리.
병렬 알고리즘 구현의 복잡성을 줄이고, 다중 스레드 환경에서의 동시성 제어 간소화.
적용 예시:
Intel의 TSX(Transactional Synchronization Extensions), IBM POWER의 HTM 기능 등.

2. DMA vs RDMA
DMA (Direct Memory Access):

CPU의 개입 없이 디바이스가 메모리와 직접 데이터 송수신을 수행.
단일 시스템 내에서 I/O 처리 속도 향상.
RDMA (Remote Direct Memory Access):

네트워크 상에서 원격 시스템의 메모리에 직접 접근하는 방식.
CPU 개입 없이 원격 메모리에 읽기/쓰기가 가능하며, 초저지연 고속 네트워킹 (예: InfiniBand)에서 주로 사용됨.
차이점:

DMA는 로컬, RDMA는 원격 메모리 접근.
RDMA는 네트워크 오버헤드 감소와 고속 데이터 이동에 효과적.
3. PCIe Link Width(x1, x4, x8, x16)의 성능 영향
개념:
PCIe는 레인(Lane) 단위로 데이터 전송이 이루어지며, x1은 1레인, x16은 16레인 사용을 의미.

성능 영향:

각 레인은 독립적인 직렬 통신 채널로, 레인 수가 많을수록 대역폭 증가.
예: PCIe 4.0 기준으로 x1은 약 2GB/s, x16은 32GB/s 이상 지원.
GPU, 고속 SSD, NIC 등은 x8 또는 x16 슬롯이 요구됨.
4. NUMA 환경에서 I/O 디바이스 배치의 영향
개념:
NUMA 시스템에서는 CPU마다 메모리 뱅크와 I/O 버스 연결이 비균일하게 구성되어 있음.

영향:

I/O 디바이스가 CPU와 가까운 노드에 연결될수록 레이턴시 감소.
잘못된 디바이스 배치는 원격 메모리 접근 유발 → 성능 저하.
OS 또는 드라이버 수준에서의 I/O affinity 설정 필요.
5. Overlapping I/O 기법
개념:
I/O 작업과 계산(연산)을 동시에 겹쳐서 수행하여 시스템 전체 처리량을 향상시키는 방식.

기법:

비동기 I/O (Async I/O) 사용으로 CPU는 I/O를 기다리지 않고 다음 작업 수행.
버퍼링 및 파이프라인 처리로 연산과 I/O가 병렬화됨.
DMA 기반 I/O와 잘 결합되어 고성능 시스템에서 필수 기법.
효과:

CPU 유휴 시간 최소화, I/O 병목 완화, 처리율 향상.
6. Thunderbolt vs USB
USB (Universal Serial Bus):

범용 I/O 인터페이스로, 호환성 우수.
USB 3.2 Gen 2x2 기준 최대 20Gbps, USB4에서는 40Gbps 지원.
Thunderbolt:

PCIe + DisplayPort + 전원 전송을 하나로 통합한 고속 인터페이스.
Thunderbolt 4는 최대 40Gbps, 다중 모니터 및 고속 저장장치 연결에 최적.
차이점:

Thunderbolt는 직접 PCIe 통신, 데이지 체인 지원, 전력 공급 능력 우수.
USB는 비용 효율적이고 광범위한 장치 지원.
Thunderbolt는 주로 고성능 워크스테이션, 전문가용 기기에서 사용.

- Polling 기반 I/O vs Interrupt 기반 I/O
Polling I/O:

CPU가 I/O 장치 상태를 반복적으로 확인하는 방식.
구현이 간단하지만, CPU 자원을 비효율적으로 소모함.
고속 장치보다 저속 주변기기에서 주로 사용.
Interrupt I/O:

장치가 준비되면 인터럽트를 발생시켜 CPU에 알림.
CPU는 다른 작업 수행 중에도 효율적 반응 가능.
고성능 시스템에서 선호, 처리량과 응답성이 뛰어남.
비교 요약:

Polling: CPU 낭비 ↑, 구현 간단.
Interrupt: 응답성 ↑, CPU 효율 ↑, 복잡도 ↑.
2. Write Cliff 현상 (Persistent Storage)
개념:
SSD 등의 플래시 기반 저장장치에서 쓰기 성능이 갑자기 급격히 저하되는 현상.

원인:

SLC 캐시 포화 후, TLC/QLC 등 느린 셀에 쓰기 전환.
Garbage Collection, Wear Leveling에 의한 I/O 지연.
해결 방법:

지속적인 SLC 캐시 비우기, Over-Provisioning 증가.
호스트 기반 쓰기 패턴 최적화.
SSD 펌웨어 최적화로 GC 간섭 최소화.
3. Hot Plugging 고려사항 (USB, PCIe 등)
개념:
시스템을 끄지 않고도 장치를 연결하거나 제거할 수 있는 기능.

기술적 고려 요소:

전력 공급의 안정성 확보 (전류 서지 방지).
신호 무결성 유지, ESD 보호 회로 필요.
OS와 드라이버의 Hot-Plug 이벤트 처리 설계.
PCIe의 경우 Hot-Plug 컨트롤러와 BIOS/UEFI 지원 필요.
4. Network-on-Chip (NoC)의 개념과 기존 버스 아키텍처와 차이점
NoC (Network-on-Chip):

칩 내 컴포넌트 간 네트워크 기반 통신 아키텍처.
각 모듈이 라우터를 통해 메시지 교환.
버스 아키텍처와의 차이점:

버스는 공유 버스 방식 → 충돌 및 병목 발생.
NoC는 병렬 데이터 전송 가능, 스케일 확장성 우수.
NoC는 복잡한 멀티코어 및 AI 칩 설계에 필수.
5. Direct I/O vs Memory-Mapped I/O
Direct I/O (Port-Mapped I/O):

CPU가 I/O 전용 명령어 (e.g., IN, OUT) 사용.
I/O 공간과 메모리 공간이 분리되어 있음.
Memory-Mapped I/O:

I/O 장치를 메모리 공간에 매핑, LOAD/STORE로 접근.
캐시 사용 불가 설정 필요.
범용성, 코드 단순화에 유리, 고성능 시스템에서 선호.
성능 영향:

Memory-Mapped는 더 높은 유연성과 범용 CPU 명령어 사용 가능.
Direct I/O는 하드웨어가 단순하며 저성능 임베디드에 적합.
6. Superpipeline(슈퍼파이프라이닝)의 개념과 차이점
기본 개념:

파이프라인 단계를 더 잘게 나누어, 클럭 사이클을 줄이고 처리량 증가.
차이점:

일반 파이프라인은 5~6단계, 슈퍼파이프라인은 10단계 이상.
각 단계가 짧아져서 클럭 주파수 ↑ → 더 높은 CPI 성능 가능.
단점: 해저드 민감도 증가, 분기 예측 실패 시 손실 증가.

- Data Dependency Hazard(데이터 종속 해저드)의 종류와 해결 방법
종류:

RAW (Read After Write): 읽기 명령이 이전 쓰기 명령의 결과를 기다려야 함 → 진짜 의존성.
WAR (Write After Read): 쓰기 명령이 이전 읽기 명령보다 먼저 실행될 수 없음.
WAW (Write After Write): 두 쓰기 명령의 순서가 바뀌면 데이터 불일치 발생.
해결 방법:

파이프라인 스톨(Stall) 또는 NOP 삽입.
레지스터 리네이밍(Register Renaming).
데이터 포워딩(Forwarding, Bypassing).
컴파일러 최적화 (Instruction Scheduling).
2. Loop-Carried Dependency(루프 의존성)의 개념과 해결 기법
개념:

반복문 내에서 현재 반복의 결과가 다음 반복의 입력에 영향을 주는 의존성.
예: A[i] = A[i-1] + 1은 병렬 실행 불가.
해결 기법:

루프 변환 (Loop Transformation):
Loop Unrolling, Loop Fission, Loop Interchange 등.
프라이빗 변수 활용 (Privatization).
스칼라 확장 (Scalar Expansion)으로 종속 제거.
데이터 흐름 분석 기반 병렬화 (자동화 툴이나 OpenMP 사용).
3. Register Windowing(레지스터 윈도잉) 기법과 RISC에서의 활용
개념:

함수 호출 시, 새로운 레지스터 집합(Window)을 할당하여 스택 접근 최소화.
대표 사례: SPARC 아키텍처의 윈도우 기반 레지스터 파일.
RISC 활용 방식:

각 함수 호출마다 입력/출력/지역 레지스터 집합 분리.
컨텍스트 스위칭 비용 절감.
레지스터 이름 충돌 방지 및 컴파일러 설계 최적화.
4. Fetch-Decode-Execute 사이클과 병목 줄이기 위한 최적화
단계별 역할:

Fetch: 명령어를 메모리에서 가져옴.
Decode: 명령어를 해석하고 오퍼랜드 추출.
Execute: 연산 수행 또는 메모리/분기 처리.
병목 최적화 방법:

Fetch 단계: 분기 예측, 인스트럭션 캐시, Prefetch Buffer 사용.
Decode 단계: 디코딩 병렬화, 명령어 인코딩 단순화(RISC 구조).
Execute 단계: 파이프라이닝, ALU 병렬 처리, 레지스터 리네이밍.
5. Control Flow Graph(CFG) 기반 최적화 기법
기본 개념:

프로그램 흐름을 노드(기본 블록)와 에지(분기)로 표현한 그래프.
주요 최적화 기법:

데드 코드 제거 (Dead Code Elimination).
공통 표현식 제거 (Common Subexpression Elimination).
루프 불변 코드 이동 (Loop Invariant Code Motion).
분기 병합/최소화 (Branch Folding, If Conversion).
도달 가능성 분석을 통한 코드 제거.
6. Hardware Prefetching vs Software Prefetching
Hardware Prefetching:

CPU가 메모리 접근 패턴을 감지해 자동으로 미리 데이터를 로드.
캐시 미스를 줄이고 투명한 성능 향상 제공.
제어 불가능, 잘못된 예측 시 캐시 오염 가능성 존재.
Software Prefetching:

컴파일러 또는 개발자가 명시적으로 미리 로드 명령어를 삽입.
더 정밀한 제어 가능, 특화된 루틴에 최적화 가능.
단점: 프로그래머가 명확한 메모리 패턴을 파악해야 함.

- Instruction-Level Parallelism (ILP) vs Data-Level Parallelism (DLP)
ILP (명령어 수준 병렬성):

하나의 스레드 내에서 상호 독립적인 명령어들을 동시에 실행하는 방식.
파이프라이닝, 슈퍼스칼라, VLIW, Out-of-Order Execution 등이 ILP 구현 방식.
제어 흐름 기반 성능 향상에 중점.
DLP (데이터 수준 병렬성):

하나의 명령어가 다수의 데이터에 동시에 적용되는 병렬 처리 방식.
SIMD(Single Instruction Multiple Data), GPU 기반 연산이 대표적.
벡터 연산이나 행렬 연산에서 강력한 성능 발휘.
2. Out-of-Order Execution vs Speculative Execution
Out-of-Order Execution:

명령어들이 데이터 의존성에 따라 순서를 바꿔서 실행되지만, 결과는 순서대로 커밋.
성능 병목 해소를 위한 동적 명령어 스케줄링 기술.
Speculative Execution:

프로그램 흐름상 결과가 확정되지 않았지만 예상(예측)을 기반으로 실행.
대표적으로 분기 예측 후의 명령어들을 미리 실행.
잘못된 예측 시 롤백 비용 존재.
보완 관계:

둘 다 ILP 향상을 위한 기술이며, 함께 사용되어 CPU 유휴 자원 활용 극대화.
Out-of-Order는 명령 간 의존성 해소, Speculative은 제어 흐름 병목 해소.
3. Branch Target Buffer (BTB) 개념과 Branch Prediction에 미치는 영향
BTB(분기 대상 버퍼):

조건 분기 명령의 주소와 목표 주소를 저장하는 캐시형 테이블.
명령어 Fetch 단계에서 해당 주소의 분기가 이전에 어디로 이동했는지 빠르게 판단.
Branch Prediction에 미치는 영향:

BTB가 정확하면 분기 주소를 미리 알고 빠르게 Fetch 가능.
예측 실패율 감소 및 파이프라인 플러시 최소화.
고성능 CPU의 분기 예측 정확도를 결정짓는 핵심 요소 중 하나.
4. Temporal Locality vs Spatial Locality
Temporal Locality(시간 지역성):

최근에 접근한 데이터가 가까운 미래에 다시 접근될 확률이 높음.
예: 루프 내의 변수 참조, 함수 호출 스택.
Spatial Locality(공간 지역성):

특정 주소가 접근되면, 그 주변 주소들도 곧 접근될 가능성이 높음.
예: 배열 순차 접근, 구조체 필드 접근.
차이점:

시간 vs 공간 개념으로 접근하며, 캐시 설계에서 다양한 블록 크기와 유지 정책을 결정하는 기준이 됨.
5. ARC(Adaptive Replacement Cache) vs LRU(Least Recently Used)
LRU (최소 최근 사용):

가장 오래전에 사용된 캐시 블록을 제거.
단순하고 널리 쓰이지만, 특정 워크로드에선 성능 저하 가능.
ARC:

LRU와 LFU(Least Frequently Used)를 동적으로 조합하여 캐시 적중률 향상.
워킹 셋 변화가 심한 환경에서도 높은 적중률 유지.
더 복잡한 알고리즘이지만 메모리 액세스 패턴 변화에 유연하게 대응.
6. Cache Associativity와 Conflict Miss의 관계
Cache Associativity(연관도):

하나의 메모리 블록이 캐시 내 몇 개의 슬롯에 들어갈 수 있는지를 정의.
직접 사상(1-way), 집합 연관(2~8-way), 완전 연관(full associative) 등이 있음.
Conflict Miss:

서로 다른 메모리 주소들이 같은 캐시 슬롯(라인)에 반복해서 맵핑되어 발생하는 캐시 미스.
연관도가 낮을수록 충돌 미스 빈도가 증가.
연관도를 높이면 conflict miss를 줄일 수 있지만, 하드웨어 복잡도와 접근 시간이 증가함.

- Cache Write Allocation Policy(쓰기 할당 정책)의 종류와 특징
Write-Allocate (Fetch-on-Write):

쓰기 시 캐시에 해당 블록이 없으면 메모리에서 불러와 캐시에 올린 후 쓰기를 수행.
Write-back 정책과 자주 함께 사용됨.
쓰기 후 재사용 가능성이 높은 경우에 효율적.
No-Write-Allocate (Write-No-Allocate):

쓰기 시 캐시에 해당 블록이 없으면 메모리 직접 쓰기만 수행하고 캐시는 무시.
Write-through 정책과 함께 자주 사용됨.
데이터 재사용 가능성이 낮은 경우에 적합.
2. Cache Partitioning(캐시 파티셔닝)의 개념과 멀티코어 활용
개념:

캐시 공간을 여러 코어 또는 프로세스에 논리적으로 분할하여 경쟁을 최소화하는 기술.
캐시 오염(Cache Pollution)과 자원 경합(Cache Contention)을 줄임.
활용 사례:

멀티코어 CPU에서 코어 간 캐시 간섭 방지.
클라우드 환경에서 보안 격리 강화.
하드웨어 기반(L2/L3 캐시 분할) 또는 소프트웨어 기반(페이지 매핑)으로 구현.
3. Translation Lookaside Buffer(TLB)의 Multi-Level 구조 필요성
이유:

가상 메모리에서 주소 변환 속도를 높이기 위해 캐시된 페이지 테이블.
TLB 크기를 너무 키우면 접근 속도 저하 → L1 TLB + L2 TLB 등 계층적 구조 필요.
장점:

L1 TLB는 소형이지만 빠른 접근.
L2 TLB는 대형이지만 상대적으로 느림.
TLB miss 확률 감소와 성능 향상을 동시에 달성.
4. ASLR(Address Space Layout Randomization)의 역할
개념:

실행 파일, 라이브러리, 스택, 힙 등의 메모리 주소를 무작위화하여 로딩.
버퍼 오버플로우 및 리턴 주소 조작 공격 방어에 효과적.
보안 효과:

공격자가 특정 위치의 코드를 예측하지 못하게 만들어 Exploit 난이도 상승.
ROP(Return Oriented Programming) 등 메모리 기반 공격 차단.
5. Memory Bandwidth vs Latency
Memory Bandwidth (대역폭):

단위 시간당 메모리로 전송 가능한 데이터 양 (GB/s).
처리량(Throughput)의 척도.
Memory Latency (지연시간):

메모리 요청 후 응답까지 걸리는 시간(ns).
응답 속도의 척도.
최적화 방법:

대역폭: 멀티채널 메모리, Prefetching, Burst Mode 활용.
지연시간: 캐시 활용, Bank Interleaving, 낮은 CAS Latency 메모리 도입.
6. ECC(Error Correcting Code) 메모리 개념과 오류 처리
ECC 메모리:

메모리에 저장된 데이터를 코드화된 방식으로 오류 감지 및 정정 가능.
Parity + Hamming Code 등을 사용.
오류 처리:

Single-bit Error: 자동 감지 및 수정 가능.
Double-bit Error: 감지 가능하지만 수정 불가 (시스템 알림 또는 멈춤).
중요 시스템(서버, 항공, 금융 등)에서 데이터 무결성 보장에 필수.
7. Memory Scrubbing이란?
개념:

주기적으로 메모리 전체 또는 특정 영역을 읽어오고 ECC로 오류 검사 및 수정하는 과정.
역할:

오래된 비트 오류(Silent Error)나 방사선에 의한 오류 발생을 조기에 수정.
장시간 가동되는 시스템의 데이터 신뢰성 유지에 중요.

- False Sharing의 원인과 방지 방법
원인:

서로 다른 스레드가 서로 다른 변수를 사용하지만, 이 변수들이 같은 캐시 라인에 존재할 때 발생.
하나의 스레드가 캐시 라인을 수정하면, 다른 스레드의 캐시 데이터도 무효화(Invalidation)됨 → 성능 저하.
방지 방법:

변수 간에 Padding을 넣어 캐시 라인 분리.
구조체 내 중요 필드에 alignment directive 적용.
스레드별 데이터 분리(Thread-local storage) 전략 활용.
2. Multi-Threading 환경에서 Load Balancing 기법
Static Load Balancing:

작업을 스레드에 사전 고정 배분.
예측 가능한 작업에 효과적, 오버헤드는 적음.
Dynamic Load Balancing:

실행 중에 작업량에 따라 작업 재배분.
대표 기법: Work Stealing, Task Queue 기반 분산 처리.
Hybrid Load Balancing:

정적 분배 후 실행 중 상태에 따라 일부 동적 조정.
Affinity-Aware Scheduling:

스레드가 특정 코어나 NUMA 노드에 고정되도록 하여 캐시 효율을 극대화.
3. Directory-Based Cache Coherence의 Full Map vs Limited Map
Full Map:

모든 캐시의 소유 정보를 디렉토리에서 완전하게 추적.
높은 정합성 보장, 하지만 디렉토리 크기 증가 문제가 있음 (확장성 제한).
Limited Map (Sparse Directory):

일부 캐시 상태만 추적하거나, 특정 수 이상의 캐시만 등록 가능.
확장성은 좋지만 정합성 충돌 시 추가 통신 필요.
4. Prefetching이 Cache Coherency에 미치는 영향
긍정적 효과:

적절한 Prefetching은 미리 데이터를 캐시에 로딩하여 접근 속도 향상.
부정적 영향:

공유 데이터에 대한 Prefetch가 캐시 일관성 프로토콜(MESI 등)에 의해 불필요한 invalidation을 유발할 수 있음.
쓰기 직전 읽기 Prefetch는 오히려 오버헤드 증가 가능성 존재.
대응 전략:

Prefetch는 비공유 데이터 또는 읽기 전용에 제한하거나, Prefetch와 Coherence 로직을 협조적으로 설계.
5. Lock-Free Data Structure의 개념과 구현 방법
개념:

스레드 간 동기화 시 Lock 없이도 동시 접근이 가능한 구조.
시스템의 데드락, 우선순위 반전, 컨텍스트 스위칭 문제 회피.
구현 방법:

CAS(Compare-And-Swap), LL/SC(Load-Link/Store-Conditional) 같은 원자적 명령어 활용.
예: Lock-free Queue (Michael-Scott Queue), Stack, Hash Table 등.
메모리 관리에서 ABA 문제 해결을 위한 Tagged Pointer 또는 Hazard Pointer 필요.
6. Parallel Reduction(병렬 축소)의 개념과 SIMD/SIMT 활용
개념:

합계, 최댓값, 곱 등의 연산을 병렬적으로 계산한 후, 트리 방식 또는 누적 방식으로 결과를 축소(연산)하는 방식.
병렬 알고리즘에서 집계 작업의 성능 병목을 완화.
SIMD/SIMT 활용:

SIMD: 동일 연산을 여러 데이터에 동시에 수행 → 벡터 단위로 부분 결과 생성.
SIMT(GPU): 쓰레드 그룹을 구성해 각 블록이 병렬로 계산하고 최종 결과를 shared memory 또는 global memory에 집계.
GPU 커널에서 shared 메모리 기반 Reduction 알고리즘은 매우 대표적인 활용 사례.

- HyperTransport vs QuickPath Interconnect(QPI)
HyperTransport (HT):

AMD에서 개발한 고속 포인트-투-포인트 연결 인터페이스.
각 장치 간 직접 연결을 제공하여 버스 병목을 제거.
패킷 기반 데이터 전송, 낮은 레이턴시와 높은 대역폭 제공.
QuickPath Interconnect (QPI):

인텔이 개발한 CPU-CPU 및 CPU-메모리 간 고속 인터커넥트.
메모리 컨트롤러가 CPU에 내장됨에 따라 등장.
캐시 일관성(Coherency) 유지를 위한 프로토콜 포함.
차이점:

HT는 모듈형 설계에 강점, QPI는 캐시 일관성 중심 설계.
QPI는 멀티프로세서 환경에서 NUMA 최적화에 유리.
2. Synchronization Barrier의 개념과 멀티스레딩 환경에서의 역할
개념:

모든 스레드가 특정 지점(Barrier)에 도달할 때까지 대기하고, 이후 동시에 다음 단계로 진행하도록 보장.
병렬 알고리즘에서 동기적 단계 수행을 보장하는 데 사용.
역할:

분산 계산, 병렬 시뮬레이션에서 각 단계마다 정합성을 유지.
예: 병렬 행렬 곱셈에서 각 타임스텝이 끝나기 전 다음 단계로 못 가도록 동기화.
구현 방식:

소프트웨어 기반(Pthread Barrier), 하드웨어 명령어 기반(전용 명령어) 존재.
3. Multi-Core 환경에서 Memory Contention(메모리 충돌)을 줄이는 방법
메모리 분산:

NUMA 노드 최적화, 메모리 접근이 로컬이 되도록 스레드 바인딩(Thread Affinity) 적용.
False Sharing 방지:

변수 간 캐시 라인 간섭 제거로 성능 저하 예방.
데이터 분할(Data Partitioning):

코어마다 별도의 데이터 블록 할당하여 충돌 최소화.
락 경쟁 완화:

락-프리(lock-free) 자료구조, 분산 큐 등 적용.
4. CC-NUMA(Cache-Coherent Non-Uniform Memory Access) 성능 최적화 기법
메모리 접근 로컬화(Locality-Aware Placement):

스레드가 사용하는 데이터는 같은 NUMA 노드 메모리에 할당.
Thread Affinity 설정:

스레드가 자주 이동하지 않도록 코어에 고정(핀ning).
메모리 할당 정책 조정:

First-touch, Interleaved 등 운영체제 정책에 따라 메모리 할당 제어.
NUMA-aware 프로그래밍:

메모리/스레드 배치를 고려한 프로그래밍 모델 적용 (예: OpenMP + NUMA).
5. PCIe Lane Width와 대역폭 관계
Lane의 의미:

PCIe는 직렬 링크 기반으로, 한 쌍의 송수신 라인이 x1, x4, x8, x16 등으로 확장 가능.
대역폭 관계:

대역폭은 Lane 수에 비례하여 증가.
예: PCIe 4.0 기준, x1은 약 2GB/s, x16은 약 32GB/s.
활용 사례:

GPU는 보통 x16, SSD는 x4, 사운드카드는 x1 슬롯을 활용함.
6. Direct I/O vs Memory-Mapped I/O
Direct I/O:

CPU가 명령어로 I/O 포트에 직접 접근 (x86의 IN/OUT 명령어).
보통 Port-mapped I/O라고도 하며, 특정 I/O 주소 공간 사용.
장점: I/O와 메모리 주소 분리.
단점: 접근 속도 낮고, 범용성 떨어짐.
Memory-Mapped I/O:

I/O 장치가 일반 메모리 주소 공간 내에 매핑되어, 메모리 접근처럼 처리.
장점: 통일된 접근 방식, 고속 처리 가능.
단점: 주소 공간 낭비 가능성.
성능 비교:

Memory-Mapped 방식이 고성능 시스템에서 일반적으로 우세.

- Latency-Hiding Techniques(지연시간 은폐 기법)의 개념과 예시
개념:
시스템 내에서 발생하는 지연(latency)을 완전히 줄이기 어렵기 때문에, 작업을 병렬적으로 수행하거나 기다리는 동안 다른 연산을 실행하여 지연을 숨기는 기법이다.
대표적인 기법들:
멀티스레딩: 한 스레드가 메모리 대기 중일 때 다른 스레드 실행.
비동기 처리(Async I/O): 입출력을 요청만 하고 CPU는 다른 작업 수행.
프리페칭(Prefetching): 미리 데이터를 가져와 캐시에 적재.
파이프라이닝: 각 단계가 독립적으로 수행되어 연속 처리 가능.
2. USB 3.0과 USB 4.0의 차이점과 성능 비교
USB 3.0:

이론적 속도: 5Gbps (SuperSpeed).
단방향 통신(Host-Device).
표준 Type-A 커넥터 중심.
USB 4.0:

최대 속도: 40Gbps (Thunderbolt 3 기반).
양방향 동시 통신, 디스플레이 및 데이터 동시 지원.
USB-C 커넥터 기반, 다중 프로토콜 통합(TB3, DisplayPort 등).
차이점 요약:

속도, 커넥터 종류, 프로토콜 지원 범위에서 USB 4.0이 압도적이다.
3. Storage Class Memory(SCM)의 개념과 DRAM/NAND와의 차이점
개념:

비휘발성 메모리와 DRAM의 특성을 혼합한 고속 스토리지 계층.
예: Intel Optane (3D XPoint)
기존 메모리와 차이점:

DRAM: 매우 빠르지만 휘발성.
NAND Flash: 비휘발성이나 느리고 수명이 제한됨.
SCM: DRAM보다 느리지만 비휘발성이며 NAND보다 훨씬 빠름.
활용:

메모리 계층 사이에 캐시 또는 확장 메모리로 사용.
4. I/O Bandwidth vs I/O Throughput
I/O Bandwidth:

단위 시간당 전송 가능한 최대 데이터 양.
하드웨어의 이론적 용량에 가까움.
I/O Throughput:

실제로 전송된 데이터 양.
소프트웨어, 병목, 지연 등에 영향을 받음.
차이:

Bandwidth는 용량, Throughput은 성능 지표.
Throughput이 항상 Bandwidth보다 작거나 같음.
5. Hot Plugging(핫 플러깅)에서 발생할 수 있는 기술적 문제와 해결 방법
문제:

전기 충격 또는 전압 불균형으로 인한 하드웨어 손상.
운영체제 또는 드라이버의 동적 인식 실패.
데이터 손실 가능성 (캐시 미작성 등).
해결 방법:

Hot Swap Controller 사용 (전기적 보호 회로 포함).
OS 레벨에서 동적 디바이스 인식 및 드라이버 관리.
스토리지 장치의 쓰기 캐시 플러시 설정 적용.
6. M.2 vs U.2 인터페이스의 차이점과 활용 사례
M.2:

소형 폼팩터, 직접 메인보드에 장착.
SATA 또는 NVMe 모두 가능.
주로 노트북, 소형 데스크탑에서 사용.
U.2:

2.5인치 SSD용 인터페이스, SAS/SATA와 호환 가능.
높은 내구성과 성능, 엔터프라이즈 서버에서 활용.
차이점:

M.2는 폼팩터가 작고, U.2는 케이블 연결 방식으로 교체 및 확장에 유리.
7. NVMe Queueing Mechanism의 장점 (vs SATA AHCI)
SATA AHCI:

큐 하나당 32개 명령만 처리 가능.
레거시 하드디스크용 설계로, SSD 병렬성 활용 불가능.
NVMe:

최대 64K 큐, 큐당 64K 명령어 처리 가능.
각 CPU 코어마다 전용 큐 배정으로 병목 최소화.
장점 요약:

낮은 레이턴시, 높은 병렬 처리량, 멀티코어 최적화.

- Memory-Mapped I/O의 장점과 단점 (고속 I/O 관점)
개념: I/O 장치를 메모리 주소 공간에 매핑하여 CPU가 메모리 접근 방식으로 장치를 제어하는 방식.

장점:

명령어 통합: 별도의 입출력 명령 없이 load/store 명령만으로 접근 가능.
고속 처리: 메모리 캐시, 버스, 파이프라인 등과의 연계로 데이터 접근 속도 향상.
프로그래밍 단순화: 포인터 연산만으로 장치 제어 가능.
단점:

메모리 주소 공간 소모: I/O가 차지하는 만큼 일반 메모리 영역이 줄어듦.
캐싱 문제: 일부 장치 접근 시 캐시 사용이 위험하여 비효율을 초래할 수 있음.
2. DNA Computing의 개념과 기존 컴퓨터 아키텍처와의 차이점
개념:

DNA 분자의 생화학적 반응을 이용해 계산을 수행하는 생물학적 컴퓨팅 방식.
대량의 데이터와 가능한 해를 동시에 처리할 수 있는 병렬 계산 구조.
기존 아키텍처와의 차이점:

기존 컴퓨터: 전자 회로 기반, 순차적 명령 실행.
DNA 컴퓨터: 동시 다중 계산, 물질적 계산, 속도보다 병렬성과 저장 능력에 강점.
활용 예: 조합 최적화 문제, 암호 해독 등에서 실험적 적용 중.

3. Optical Computing(광 컴퓨팅)의 개념과 기존 전자 기반 컴퓨팅과의 차이점
개념:

전자 대신 광자(Photon)를 사용하여 정보를 처리하는 방식.
빛의 간섭, 굴절, 반사 등을 계산 자원으로 사용.
차이점:

속도: 광신호는 전기 신호보다 빠르며 병렬 연산 가능성이 높음.
발열 적음: 전자보다 발열이 낮아 고집적도 가능.
상용화 제약: 광소자 제어 및 메모리 통합에서 기술적 난제 존재.
4. Zero Trust Architecture(제로 트러스트 아키텍처)와 하드웨어 보안
개념:

내부/외부를 구분하지 않고 모든 접근을 검증하고 최소 권한만 부여하는 보안 모델.
“Never Trust, Always Verify” 원칙에 기반.
하드웨어 보안에서의 중요성:

펌웨어, 칩셋, 부트로더 등 하드웨어 루트도 항상 검증 필요.
하드웨어 수준의 보안 부팅(Secure Boot), TPM, 암호화 가속기와 연계 필요.
내부 공격자나 하드웨어 백도어도 고려하는 구조.
5. Edge AI 프로세서와 클라우드 AI의 차이점
Edge AI 프로세서:

데이터 센터가 아닌 단말 또는 게이트웨이에서 AI 연산 수행.
전력 효율, 실시간성, 네트워크 종속성 최소화를 지향.
차이점 요약:

위치: Edge는 현장, Cloud는 중앙 서버.
지연 시간: Edge가 짧음.
모델 사이즈: Edge는 경량 모델 최적화 필요.
보안: Edge는 네트워크 노출이 적지만, 단말 보안은 강화 필요.
6. Near-Memory Computing(NMC)의 개념과 기존 메모리 계층 구조와의 차이점
개념:

메모리 칩 또는 그 근처에 연산 회로를 포함시켜, 데이터 이동 없이 연산을 수행.
주로 DRAM, HBM 근처에 연산 유닛을 배치.
기존 계층 구조와의 차이:

기존: CPU ↔ Cache ↔ DRAM 구조, 데이터 이동 비용 큼.
NMC: 데이터가 있는 위치에서 바로 연산, 메모리 병목 해결.
AI, 그래프 연산, DB 등 대용량 데이터 반복 접근 분야에 유리.


- Neuromorphic Computing(뉴로모픽 컴퓨팅)이란 무엇이며, AI 연산에서 어떻게 활용되는가?
- Quantum Error Correction(양자 오류 정정) 기술이란 무엇이며, 왜 필요한가?
- Nanosheet Transistor(나노시트 트랜지스터)란 무엇이며, 기존 FinFET과의 차이점은?
- 3D Heterogeneous Integration(3D 이기종 집적)의 개념과 활용 가능성은?
- Human Brain-Inspired Computing(인간 뇌 모방 컴퓨팅)이란 무엇이며, 기존 AI 아키텍처와의 차이점은?
- Instruction Window(명령어 윈도우) 크기가 성능에 미치는 영향은?
    - 1. Neuromorphic Computing(뉴로모픽 컴퓨팅)의 개념과 AI 활용
개념: 인간의 뇌 신경망을 하드웨어적으로 모사한 방식으로, 뉴런과 시냅스를 회로로 구현.
특징:
사건 기반(Event-driven) 연산
초저전력, 비동기적 신호 처리
병렬 구조 최적화
AI 활용:
센서 연동 실시간 인식 (예: 스마트 센서, 음성/이미지 분류)
학습보다 추론에 특화된 경량 AI 시스템
대표 사례: IBM TrueNorth, Intel Loihi
2. Quantum Error Correction(양자 오류 정정)의 개념과 필요성
개념: 양자 비트(큐비트)는 외부 간섭에 매우 민감하므로, 오류를 탐지하고 복구하기 위한 알고리즘/코딩 기법.
필요성:
큐비트는 decoherence, noise에 쉽게 노출
측정 불가능한 상태를 보호하기 위해 중첩 상태 유지가 필수
기술 예:
Shor 코드, Surface 코드, Bacon-Shor 등
논리 큐비트(Logical Qubit)와 물리 큐비트의 다대일 대응이 일반적
3. Nanosheet Transistor(나노시트 트랜지스터)와 FinFET의 차이점
Nanosheet Transistor:
GAA(Gate-All-Around) 구조의 일종, 게이트가 채널을 전방위로 감싸 제어.
3~5nm 이하 공정에서 적용되는 차세대 트랜지스터 구조.
FinFET과의 차이점:
FinFET: Fin 구조 위쪽과 양 옆에서 게이트 제어 (3면)
Nanosheet: Gate가 4면 모두에서 채널 제어, 누설전류 줄이고 성능 향상
설계 유연성 증가 (채널 폭 조정 가능)
4. 3D Heterogeneous Integration(3D 이기종 집적)의 개념과 활용
개념: 다양한 기능(로직, 메모리, 센서 등)을 가진 칩들을 수직 적층 방식으로 집적하는 기술.
활용 가능성:
AI, 5G, HPC에서 고속 데이터 처리와 소형화 실현
HBM(High Bandwidth Memory), SoIC(System on Integrated Chip) 적용 사례
다양한 공정 기술을 융합 가능 (e.g., CMOS + RF + Photonic)
5. Human Brain-Inspired Computing(뇌 모방 컴퓨팅)과 기존 AI 아키텍처의 차이점
개념: 생물학적 뇌의 신경 네트워크를 모방한 비전통적 컴퓨팅 모델 (단순한 인공신경망 이상의 접근).
차이점:
기존 AI: 순차적, 수학적 모델 기반(딥러닝, CNN 등)
뇌 모방 컴퓨팅: 비선형, 사건 기반, 비동기적, 더 적은 연산으로 복잡한 학습/추론
메모리와 연산의 통합 (인메모리 연산, 시냅틱 연산)
6. Instruction Window(명령어 윈도우) 크기와 성능의 관계
개념: 파이프라인에서 동시에 디코딩/실행 대기 중인 명령어 집합의 크기.
성능 영향:
크기가 클수록 명령어 간 독립성 탐지와 재정렬 기회 증가 → ILP 향상
단, 너무 크면 하드웨어 복잡도, 소비 전력, 레이턴시 증가
최적화 포인트:
워크로드 특성에 따라 동적 조정 또는 리오더링 기법과 연계 사용
OOO(Out-of-Order Execution) 프로세서에서 성능 향상의 핵심 요소

- Indirect Branch Prediction(간접 분기 예측)
개념: 간접 분기는 jump eax나 call [table + index]처럼 실행 시점에 타겟 주소가 레지스터나 메모리에 의해 결정되는 분기를 말합니다. 일반 분기보다 예측이 어렵습니다.
최적화 방법:
BTB(Branch Target Buffer) 확장 또는 다중 엔트리 방식
Target Address Cache(TAC) 또는 Virtual Call Target Cache 사용
다중 히스토리 기반 예측기(Multi-History Predictors)를 도입해 컨텍스트별 예측 성능 향상
2. Register File 크기와 CPU 성능의 관계
Register File: 명령어 실행 시 참조되는 레지스터들의 집합으로, 연산 중간 결과나 데이터 저장에 사용됩니다.
성능과의 관계:
레지스터 수가 많을수록 더 많은 ILP(Instruction Level Parallelism)를 활용 가능 (특히 OOO에서)
리네이밍(Register Renaming) 성능 향상에도 긍정적
반면 레지스터 수가 많아지면 액세스 레이턴시 및 전력 소비 증가 → 타협 필요
3. ALU Pipeline vs. FPU Pipeline
ALU Pipeline (산술 논리 유닛 파이프라인):
정수 기반 연산 중심 (덧셈, AND, OR 등)
파이프라인 깊이가 짧고, 연산이 간단하며 고속 처리에 최적화
FPU Pipeline (부동소수점 유닛 파이프라인):
부동소수점 연산 처리 (부동소수 덧셈, 곱셈 등)
파이프라인 단계가 깊고 레이턴시가 크지만 정밀한 과학/그래픽 계산에 필수
활용: HPC, 그래픽, 과학 시뮬레이션 등에서 FPU 사용률이 높음
4. Decoupled Access/Execute Architecture(분리형 접근/실행 아키텍처)
개념: 메모리 접근(Load/Store)과 연산(Execute) 단계를 독립적인 파이프라인으로 분리하여 병목을 완화하는 구조.
장점:
메모리 지연에 덜 민감
로드와 실행 사이에 큐를 두어 명령어 흐름을 비동기적으로 처리
활용 사례:
CDC 6600, IBM 360/91 등의 고전적인 슈퍼스칼라 설계
일부 임베디드/저전력 아키텍처에서도 변형 사용
5. Hardware Loop Buffer(하드웨어 루프 버퍼)
개념: 반복되는 루프 명령어들을 전용 버퍼에 저장해 재인출 없이 반복 실행할 수 있도록 지원하는 기능.
성능 향상 효과:
명령어 캐시 접근 및 디코딩 생략 → 전력 및 시간 절약
루프 카운터만으로 제어 가능 → 루프 실행 속도 향상
활용 예: DSP, RISC-V, ARM Cortex-M 시리즈 등에서 에너지 효율 향상을 위해 채택
6. SIMD Execution Unit vs 일반 ALU
SIMD Execution Unit:
하나의 명령어로 여러 데이터를 동시에 처리 (벡터 연산)
데이터 병렬성에 최적화 (멀티미디어, 이미지 처리, 행렬 연산 등)
예: SSE, AVX, NEON
일반 ALU:
정수 단일 연산 수행 (일반 산술/논리 연산)
컨트롤 흐름 기반 프로그램에 적합
장점 비교:
SIMD: 연산 처리량 높고 병렬 처리에 강함
ALU: 단순 구조로 빠른 반응, 제어 흐름에 유리

- Power Gating과 Clock Gating의 개념과 전력 절감 효과
Power Gating:

사용하지 않는 회로 블록의 전원 공급 자체를 차단하여 누설 전류(Leakage Power)를 줄이는 기법
주로 슬립 트랜지스터(sleep transistor)를 사용
효과: 대기 상태 전력 절감에 탁월
Clock Gating:

회로 블록의 클럭 신호를 차단하여 불필요한 스위칭 활동(dynamic power)을 줄이는 기법
RTL 및 컴파일러 수준에서 많이 사용
효과: 동작 중인 회로의 전력 절감에 효과적
차이점:

Power Gating은 완전한 꺼짐(off), Clock Gating은 일시 정지(pause)와 유사
Power Gating은 wake-up 시간이 오래 걸리지만 절감 효과는 큼
2. Register Scoreboarding의 개념과 Out-of-Order Execution에서의 역할
Register Scoreboarding:
레지스터의 사용 상태를 추적하여 데이터 종속성 여부를 확인하고, 명령어 발행 타이밍을 제어하는 기법
CDC 6600에서 최초 도입
역할:
명령어 간 RAW, WAR, WAW 종속성을 식별해 실행 순서를 동적으로 조정
레지스터 리네이밍이 없는 환경에서도 OOO 실행을 지원 가능
특징:
비교적 단순하지만 현대 아키텍처에서는 리네이밍 기법이 더 널리 사용됨
3. Hybrid Branch Prediction(하이브리드 분기 예측)의 개념과 성능 최적화 방법
개념:
여러 예측기를 결합하여 상황에 따라 최적의 예측기를 선택하는 기법
예: 글로벌 히스토리 기반 vs 로컬 히스토리 기반 결합
구성:
선택기(selector)를 사용해 각 분기마다 더 잘 맞는 예측기 선택
예측기의 결과를 바탕으로 선택기의 성능을 학습
최적화 효과:
다양한 유형의 분기(짧은 루프, 함수 호출 등)에 대해 높은 예측 정확도 제공
4. Page Coloring 기법의 개념과 캐시 활용도 향상 효과
개념:
가상 페이지와 물리 페이지를 캐시 색상(Color)에 맞게 매핑하여 캐시 충돌을 줄이는 기술
캐시 인덱스와 페이지 프레임 번호 간의 관계를 이용
효과:
같은 캐시 세트를 사용하는 페이지 충돌을 방지
캐시 사용률 향상 → 캐시 히트율 증가, 성능 개선
사용 환경:
OS 커널 수준에서 페이지 할당 시 제어
5. Direct Mapped Cache에서 Conflict Miss를 줄이는 방법
문제:
Direct Mapped Cache는 하나의 주소만 특정 캐시 라인에 매핑 → 동일 인덱스를 갖는 주소가 많으면 Conflict Miss 발생
해결 방안:
Page Coloring으로 서로 다른 페이지가 같은 캐시 인덱스에 매핑되지 않도록 조정
컴파일러 최적화 (배열 주소 간격 재조정 등)
캐시 크기 또는 라인 수 조정
Pseudo-Associative Cache와 같은 대안 아키텍처 도입
6. Sub-Block Placement Policy(서브 블록 배치 정책)의 개념과 성능 최적화
개념:
캐시 블록을 더 작은 서브 블록 단위로 분할 저장하여 미세 단위 접근을 지원하는 정책
장점:
전체 블록을 로딩하지 않고 필요한 서브 블록만 접근 가능 → 메모리 대역폭 절약
캐시 미스 감소, 데이터 활용도 향상
활용:
낮은 공간 지역성(Locality)을 갖는 워크로드에 유리
일부 임베디드 시스템 및 고속 캐시 설계에 적용

- Page Walk Overhead(페이지 탐색 오버헤드)을 줄이기 위한 최적화 기법
문제:

가상 주소 → 물리 주소 변환 시, 다단계 페이지 테이블 탐색이 필요하여 메모리 접근이 지연됨
특히 TLB 미스 시, CPU가 Page Walk를 수행해야 하며 이는 수십 사이클 소요
최적화 기법:

TLB 크기 증가 또는 멀티 레벨 TLB 구조 도입
Page Walk Cache: 최근 탐색한 테이블 경로를 캐시에 저장
Huge Page 사용: 2MB, 1GB 단위 페이지를 사용해 TLB 엔트리 수 감소
TLB Prefetching: 예측된 주소를 미리 TLB에 적재
2. Cache Write Throttling(캐시 쓰기 제어)의 개념과 성능 영향
개념:

쓰기 작업이 캐시 및 메모리 대역폭을 과도하게 소모하는 것을 방지하기 위해 쓰기 트래픽을 조절하는 정책
멀티코어 환경에서 공유 메모리 혼잡을 완화하는 데 유용
성능 영향:

장점: 메모리 트래픽 분산, 읽기 지연 최소화, 전력 절감
단점: 지나치게 제한하면 쓰기 지연 또는 데이터 반영 지연 초래
기법 예시:

비동기 쓰기 버퍼 제어, QoS 기반 쓰기 제한, Read-Preferred 스케줄링
3. Cache Reuse Distance(캐시 재사용 거리) 분석과 성능 최적화
개념:

동일한 데이터가 두 번 연속 참조되는 메모리 접근 간 거리 (Access 간의 주소 또는 시간 거리)
재사용 거리가 짧을수록 캐시에 오래 유지되기 쉬움
분석 목적:

메모리 접근 패턴을 분석하여 캐시 활용도 향상
코드 리팩토링 또는 배열 레이아웃 최적화 시 활용
최적화 방법:

데이터 접근 순서 변경 (Loop Interchange)
배열 Padding 또는 Loop Tiling을 통해 공간 지역성 확보
쓰레드 간 공유 변수 분리로 False Sharing 방지
4. Zero Page Optimization의 개념과 성능 향상 효과
개념:

초기화되지 않은 페이지를 모두 0으로 채우는 대신, 공유된 "zero page" 하나를 여러 가상 페이지가 참조하도록 구성
물리 메모리를 절약하며 성능도 향상
효과:

불필요한 페이지 할당/초기화 시간 감소
메모리 사용량 절감
OS 레벨에서 효율적인 메모리 관리 가능
적용 예시:

Linux 커널에서 zero_page 공유
Cloud 환경에서 VM 초기 메모리 최적화
5. Flash Memory의 Program/Erase Cycle이 성능과 내구성에 미치는 영향
특징:

Flash는 데이터를 덮어쓰기 전에 반드시 Erase 과정 필요
NAND Flash는 Block 단위로 Erase, Page 단위로 Program
영향:

제한된 수명의 P/E Cycle로 인해 반복 쓰기가 많을수록 내구성 저하
Write Amplification 발생 → 성능 저하
대응 방법:

Wear Leveling: 전체 셀에 고르게 쓰기 분배
Over-Provisioning: 여유 공간 확보로 성능 보조
TRIM 명령어 활용
Garbage Collection 최적화
6. Adaptive Page Replacement Algorithm(적응형 페이지 교체 알고리즘)이란?
개념:

페이지 참조 패턴을 동적으로 학습하여 상황에 따라 적절한 페이지 교체 전략(LRU, LFU 등)을 선택하는 알고리즘
예시: ARC (Adaptive Replacement Cache), CAR (Clock with Adaptive Replacement)
장점:

LRU와 LFU의 단점을 보완
다양한 워크로드에 대해 높은 히트율 유지
전략:

참조/미참조 페이지 리스트를 분리하고, 참조 패턴에 따라 가중치를 조절하여 교체 대상 선정

- Memory Prefetching이 성능에 미치는 영향을 측정하는 방법
기본 개념:
Memory Prefetching은 CPU가 필요한 데이터를 미리 메모리에서 불러오는 기법으로, 캐시 미스를 줄이는 데 목적이 있다.

성능 측정 방법:

Cache Miss Rate 변화: 프리페칭 전후의 L1/L2 Miss Rate 비교
Instructions Per Cycle (IPC): IPC가 증가하면 프리페칭 효과 있음
Memory Bandwidth 사용량: 과도한 프리페칭은 bandwidth 낭비를 유발하므로 함께 분석
프로파일링 툴 활용: perf, Intel VTune, AMD uProf 등으로 프리페치 통계 수집
Prefetch Accuracy와 Coverage: 불러온 데이터가 실제로 사용된 비율과 전체 접근 대비 프리페치 커버 범위 평가
2. Shared Memory System에서 False Sharing을 방지하는 방법
False Sharing 정의:
서로 다른 쓰레드가 동일한 캐시 라인 내의 다른 변수를 수정할 때 발생하는 불필요한 캐시 일관성 트래픽 현상

방지 방법:

변수 간 Padding 삽입: 한 캐시 라인에 여러 쓰레드의 데이터가 존재하지 않도록 공간 분리
구조체 정렬 최적화: 구조체 내 변수들을 캐시 라인 기준으로 재정렬
쓰기 집중 변수 분리: 각 쓰레드가 사용하는 변수는 별도 메모리 영역에 위치
컴파일러 프래그마 또는 align 명령어 활용: C/C++에서 alignas(64) 등의 키워드 사용
3. MESIF 프로토콜과 MESI 프로토콜의 차이점
MESI 프로토콜(M, E, S, I):

Modified: 이 캐시에만 있고 변경됨
Exclusive: 유일하며 변경되지 않음
Shared: 여러 캐시에 존재, 변경되지 않음
Invalid: 무효 상태
MESIF 프로토콜:

기존 MESI에 F (Forward) 상태를 추가
F 상태는 Shared 상태 중 하나지만, 다른 캐시에 데이터를 전달할 책임이 있음
S 상태는 전달 책임이 없으며, F 상태만 Data Forwarder 역할
장점:

데이터 전송 효율 향상
특정 캐시에만 전송 요청이 집중되는 병목 방지
Snooping 트래픽 감소
4. Distributed Shared Memory(DSM)의 개념과 활용 사례
개념:
물리적으로 분산된 시스템이지만, 논리적으로 하나의 공유 메모리처럼 접근할 수 있도록 구성된 메모리 시스템

특징:

클러스터 간 메모리 일관성을 소프트웨어 또는 하드웨어 수준에서 유지
일관성 모델은 Sequential, Release Consistency 등 선택 가능
활용 사례:

NUMA 기반 고성능 서버
MPI 기반 응용에서의 페이지 공유
Cloud 기반 병렬 프로그래밍 환경
OpenMP, Java RMI 등 일부 언어 및 프레임워크에서의 추상화된 공유 메모리 구현
5. Task-Level Parallelism(TLP)과 Data-Level Parallelism(DLP)의 차이점
TLP (작업 수준 병렬성):

서로 다른 작업(Task 또는 Thread)을 병렬로 수행
주로 멀티코어 또는 SMT 기반에서 활용됨
예: 웹 서버에서 요청을 쓰레드로 병렬 처리
DLP (데이터 수준 병렬성):

동일한 연산을 다수의 데이터에 동시에 수행
SIMD, GPU, 벡터 프로세서 등에서 활용
예: 이미지 필터링, 벡터 행렬 곱
차이점 요약:

TLP는 병렬 작업, DLP는 병렬 데이터
DLP는 같은 명령 반복, TLP는 서로 다른 명령
6. Multi-Chip Module(MCM)과 단일 다이 프로세서의 성능 및 설계 차이점
MCM(Multi-Chip Module):

여러 개의 칩(Chiplet)을 하나의 패키지로 집적한 구조
AMD Ryzen 시리즈가 대표적 사례
단일 다이 구조:

하나의 실리콘에 모든 코어, 캐시, I/O가 집적됨
인텔의 전통적 설계 방식
차이점:

MCM 장점: 수율 개선, 유연한 설계, 비용 절감
MCM 단점: 칩 간 통신 지연 증가 가능성 (Infinity Fabric 등 보완)
단일 다이 장점: 칩 내부 통신 빠름, 통합성 높음
단일 다이 단점: 다이 크기 증가로 수율 저하, 발열 문제

- Parallel Programming에서 Load Imbalance(부하 불균형)의 원인과 해결 방법
개념:
병렬 프로그래밍에서 일부 스레드나 프로세스가 다른 것보다 더 많은 작업을 수행하면서 전체 실행 시간이 늘어나는 문제를 의미함.

주요 원인:

작업 분할의 불균형
스레드 간 자원 경쟁
동적 입력 데이터의 분포 차이
데이터 접근 지연 또는 캐시 미스
해결 방법:

동적 작업 할당(Dynamic Scheduling)
작업 단위(Task Granularity) 조정
데이터 로컬리티 최적화
Work Stealing 사용
2. Work Stealing Algorithm(작업 도둑 알고리즘)의 개념과 성능 최적화 효과
개념:
스레드가 자신에게 할당된 작업을 모두 마치면 다른 스레드의 작업 큐에서 작업을 훔쳐와 실행하는 방식

특징:

각 스레드는 로컬 작업 큐를 가지고 있으며, 유휴 상태가 되면 다른 큐에서 작업을 훔친다.
예: Cilk, Java Fork/Join Framework, Go 런타임 등에서 사용됨
장점:

부하 균형을 자동으로 맞춤
유휴 리소스 활용률 극대화
병렬 처리 효율성 증가
3. Compute-Bound Task와 Memory-Bound Task의 차이점
Compute-Bound Task:

CPU 연산이 중심인 작업 (예: 행렬 곱, 암호화, AI 추론)
성능은 CPU 속도 및 병렬 처리 능력에 의존
Memory-Bound Task:

데이터 이동과 접근이 병목인 작업 (예: 대용량 배열 순회, 파일 I/O)
성능은 메모리 대역폭과 캐시 구조에 좌우
차이점 요약:

Compute-Bound는 CPU 중심, Memory-Bound는 데이터 접근 중심
병목 위치가 다르므로 최적화 방향도 달라짐
4. CC-NUMA와 SC-NUMA의 차이점과 성능 최적화 방법
CC-NUMA (Cache-Coherent NUMA):

노드 간 캐시 일관성 유지
프로그래머는 공유 메모리처럼 접근 가능
대표 예: AMD EPYC, Intel Xeon
SC-NUMA (Simple/Non Coherent NUMA):

캐시 일관성 유지하지 않음
명시적 메시지 패싱 또는 페이지 복제 필요
성능 최적화 방법:

NUMA-Aware Memory Allocation (numactl 등)
Thread Affinity 설정
메모리 접근 로컬리티 유지
캐시 일관성 유지 정책 고려한 데이터 배치
5. Parallel Reduction과 Scan Operation(스캔 연산)의 차이점
Parallel Reduction:

전체 데이터를 하나의 값으로 축소 (예: 합계, 최댓값)
연산은 연관적이어야 함 (Associative)
Scan Operation (Prefix Sum):

입력 배열에 대해 누적 연산을 수행하여 부분 합의 배열을 출력
예: [1,2,3,4] → [1,3,6,10]
차이점 요약:

Reduction은 단일 결과, Scan은 중간 결과 보존
Scan은 후속 연산 최적화(예: 히스토그램, 정렬)에 유리
6. NVMe Over Fabrics(NVMe-oF)의 개념과 기존 스토리지 인터페이스와의 차이점
NVMe-oF 개념:

고속 NVMe 명령어 세트를 RDMA, TCP, Fibre Channel 등 네트워크 상에서 확장한 프로토콜
스토리지를 물리적으로 분리된 환경에서도 직접 접근하듯 사용 가능
기존 인터페이스와의 차이점:

SATA/SAS: 레거시 프로토콜, 지연 시간 큼
NVMe: PCIe에 최적화된 고속 인터페이스
NVMe-oF: 네트워크 기반이지만 PCIe 수준 성능 유지
장점:

고성능 분산 스토리지 구성 가능
CPU 오버헤드 감소 (특히 RDMA 기반)
클라우드, 데이터센터에서 스토리지 자원 유연하게 확장

- Persistent Memory에서 Read Disturbance 현상이란 무엇이며, 이를 해결하는 방법은?
개념:
지속성 메모리(Persistent Memory)에서 특정 셀을 반복적으로 읽을 경우, 인접 셀의 전하 분포에 영향을 주어 비의도적인 데이터 오류를 유발하는 현상. 이는 전통적인 DRAM의 Row Hammer와 유사한 문제로, 읽기만으로 인접 셀의 상태가 바뀔 수 있음.

해결 방법:

셀 간 물리적 간격 증가 또는 이중 트랜지스터 설계
읽기 횟수 추적 후 리프레시하는 로직
Error Correcting Code (ECC) 도입
Wear-Leveling + Disturbance-Aware Mapping 기법 활용
2. DMA에서 Bounce Buffering이란 무엇이며, 성능에 미치는 영향은?
개념:
DMA가 직접적으로 접근할 수 없는 메모리(예: 고주소 영역)에 대해, 중간 버퍼(bounce buffer)를 통해 데이터를 복사한 후 DMA 전송을 수행하는 기법.

성능 영향:

장점: 시스템의 하드웨어 호환성과 데이터 안정성을 확보
단점: 추가 복사 과정으로 인해 전송 지연 증가, 메모리 낭비, CPU 사용률 증가
최적화 방법:

DMA가 가능한 연속 물리 메모리 확보 (dma_alloc_coherent)
IOMMU 기반 매핑 활용
3. PCIe Atomics(PCIe 원자적 연산)의 개념과 활용 사례는?
개념:
PCI Express 3.0 이후부터 지원되는 기능으로, 장치 간 또는 장치-호스트 간에 원자적(Read-Modify-Write) 연산을 PCIe 수준에서 직접 수행할 수 있게 함.

활용 사례:

GPU나 FPGA가 공유된 메모리 영역에 대해 Lock-Free 방식으로 업데이트
분산 데이터베이스에서 원자적 카운터 처리
캐시 일관성 없는 환경에서 동기화 수행
장점:

CPU介입 없이 원자성 보장
동기화 오버헤드 감소
확장성 있는 고성능 시스템 구축 가능
4. I/O Virtualization(IOV)이란 무엇이며, 가상 환경에서의 역할은?
개념:
단일 물리적 I/O 디바이스를 여러 가상 머신에 안전하게 분할하여 직접 접근 가능하게 해주는 기술. 대표적으로 SR-IOV(Single Root IOV)가 있음.

가상 환경에서의 역할:

VM마다 Virtual Function(VF) 할당
호스트 CPU 오버헤드 감소
성능 저하 없이 VM 간 네트워크 및 스토리지 공유 가능
네트워크 가상화, 고속 I/O 처리에 필수
대표 사례:

클라우드 VM에서의 고속 NIC 공유 (SR-IOV NIC)
GPU 가상화 환경에서 멀티 테넌시 지원
5. Adaptive Queue Depth Management(적응형 큐 깊이 관리)의 개념과 성능 최적화 효과는?
개념:
I/O 요청 큐의 Queue Depth(큐 길이)를 동적으로 조절하여, 과부하 상태에서 응답 지연을 줄이고, 경량 상태에서 성능을 극대화하는 기법.

성능 최적화 효과:

지연시간(Latency) 감소: 과도한 큐 적체 방지
처리량(Throughput) 증가: I/O 대역폭 활용 극대화
부하 기반 튜닝 가능 (e.g. NVMe SSD에서 큐 수 증가)
적용 사례:

SSD 장치 드라이버
데이터베이스 스토리지 엔진의 I/O 스케줄링
6. RDMA에서 On-Demand Paging의 개념과 메모리 관리 최적화 효과는?
개념:
RDMA(Remote Direct Memory Access)는 기본적으로 메모리가 미리 등록(registered)되어 있어야 직접 접근 가능함. On-Demand Paging은 필요할 때만 원격 메모리를 페이지 단위로 동적 등록/해제하는 기능.

최적화 효과:

메모리 사용량 절감: 전체 영역을 고정 등록할 필요 없음
대용량 데이터 처리 효율 향상
응답성과 지연 시간 개선 (필요한 시점에만 페이지 로딩)
대표 기술:

Mellanox ConnectX NIC에서 ODP 지원
고성능 DB/AI 시스템에서 RDMA 기반 메모리 액세스 최적화

- DPDK(Data Plane Development Kit)의 개념과 고속 네트워크 성능 최적화에 미치는 영향
개념:
DPDK는 사용자 공간(User Space)에서 네트워크 패킷을 처리할 수 있도록 하는 고속 네트워크 프레임워크입니다. 커널 우회를 통해 높은 처리 속도를 제공하며, Polling 기반의 방식으로 인터럽트 오버헤드를 제거합니다.

성능 최적화 영향:

Zero-Copy 전송으로 불필요한 복사 최소화
Batch 처리 및 Lock-Free 큐를 통해 병렬성 극대화
NIC 하드웨어 리소스를 직접 제어해 지연시간 감소
방화벽, IDS, 트래픽 분석 등의 고속 패킷 처리에 활용됨
2. Queue Pair(QP) 기반 I/O 성능 최적화 기법
개념:
RDMA와 같은 고속 네트워크에서는 Queue Pair (Send Queue + Receive Queue)를 통해 커널을 우회한 직접 통신을 수행합니다. QP 기반 처리 구조는 송수신 버퍼를 독립적으로 분리하여 효율적으로 관리할 수 있게 해줍니다.

최적화 기법:

멀티 QP 분산 처리: 다수의 QP를 다중 스레드에 바인딩하여 병렬성 확보
QP 재사용: QP를 재생성하지 않고 연결 재활용해 레이턴시 감소
CQ(Completion Queue) 최적화: Polling 빈도와 워터마크 조절로 처리 지연 최소화
3. SR-IOV와 MR-IOV의 차이점
SR-IOV (Single Root I/O Virtualization):

단일 물리 장치를 여러 가상 장치(Virtual Functions)로 분할하여 여러 VM이 직접 사용할 수 있도록 함
하나의 Root Complex(PCIe Controller) 하에서만 작동
MR-IOV (Multi-Root IOV):

여러 Root Complex 간에서 하나의 장치를 공유할 수 있음
클라우드 데이터센터에서 리소스를 유연하게 공유 가능
차이점 요약:

SR-IOV는 단일 호스트용, MR-IOV는 멀티 호스트 간 I/O 자원 공유에 적합
MR-IOV는 더 복잡한 인프라와 동기화 메커니즘 필요
4. Quantum Annealing과 Gate-Based Quantum Computing의 차이점
Quantum Annealing (양자 어닐링):

최적화 문제 해결에 특화된 방식으로, 에너지 최소화를 통한 해 찾기
D-Wave 시스템에서 채택
병렬적인 확률적 계산을 통해 국소 최소값 탈출 가능
Gate-Based Quantum Computing:

논리게이트 조합으로 양자 연산을 수행하는 방식
범용 계산이 가능하며, 알고리즘 기반 양자 프로그래밍이 가능 (e.g., Shor’s, Grover’s)
IBM, Google, Rigetti 등에서 사용
차이점 요약:

어닐링은 최적화 특화 / 게이트 기반은 범용 계산 지원
어닐링은 물리적 시스템 기반 / 게이트 기반은 논리 회로 기반
5. DNA-Based Data Storage의 개념과 기존 스토리지와의 차이점
개념:
DNA 염기서열(A, T, C, G)을 0과 1의 이진수로 변환하여 데이터를 저장하는 기술. 극소형 공간에 수십 기가바이트 이상을 저장 가능하며, 이론적으로 수천 년간 보존 가능.

기존 스토리지와의 차이점:

용량: 밀도가 매우 높아 수 cm³ 내에 TB급 저장 가능
속도: 쓰기/읽기 속도는 느림 (합성 및 해독 과정 필요)
지속성: 수천 년 동안 보존 가능 (기존 HDD, SSD보다 뛰어남)
전력 소모: 거의 없음 (비휘발성 저장)
활용 예:
박물관, 국립 기록보관소, 대용량 콜드 스토리지 등

6. Brain-Inspired Computing에서 Spiking Neural Network(SNN)의 개념과 활용 사례
개념:
SNN은 뉴런이 연속적으로 활성화되는 일반적인 DNN과 달리, 시간 기반으로 전기적 신호(스파이크)를 발생시켜 계산하는 생물학적 신경망에 가까운 형태.

특징:

이산적인 이벤트 기반 연산 수행
에너지 효율성 극대화
생물학적 시간 지연 요소 모델링 가능
활용 사례:

뉴로모픽 칩 (e.g., Intel Loihi)
실시간 환경 인식 로봇
초저전력 IoT 센서 장치

- Analog Computing의 개념과 디지털 컴퓨팅과의 차이점
개념:
아날로그 컴퓨팅은 연속적인 물리적 변수(전압, 전류 등)를 이용해 수학적 문제를 해결하는 방식으로, 전통적으로 미분 방정식이나 제어 시스템 시뮬레이션에 사용되었습니다.

디지털과의 차이점:

디지털은 이산적인 이진 값(0/1) 처리, 아날로그는 연속적인 값 처리
아날로그는 특정 계산에서 속도와 에너지 효율이 높음
디지털은 정확도와 재현성이 높고 범용성 뛰어남
최근에는 AI 연산, 센서 기반 처리에 아날로그 회로 접목이 다시 주목받고 있음
2. Near-Data Processing (NDP)의 개념과 기존 메모리 계층구조와의 차이점
개념:
NDP는 데이터가 저장된 메모리나 스토리지 근처에서 연산을 수행하여 CPU로 데이터를 이동시키는 비용을 줄이는 컴퓨팅 아키텍처입니다.

기존 메모리 구조와의 차이:

기존: CPU가 데이터를 메모리에서 가져와 연산 → 데이터 이동 비용(메모리 병목) 큼
NDP: 메모리 모듈 내 연산 로직 탑재 → 처리 지연 감소, 에너지 절약
AI/ML, 그래프 연산, 대용량 DB 처리 등에서 유용
3. Optane Persistent Memory와 DRAM/NAND Flash와의 차이점
Optane PMem 개요:
인텔의 3D XPoint 기술 기반 비휘발성 메모리로, DRAM과 NAND의 중간 성격을 지님.

차이점:

속도: NAND보다 빠르고, DRAM보다는 느림
휘발성 여부: DRAM은 휘발성 / Optane은 비휘발성
용량: DRAM보다 대용량 가능, 스토리지 대체 가능
활용: 대용량 인메모리 DB, 지속성 캐시, 빠른 재부팅 시스템 등
4. Compute Express Link (CXL)의 개념과 기존 PCIe 인터커넥트와의 차이점
CXL 개요:
CPU, GPU, 메모리, 가속기 간의 고속, 저지연 연결을 위한 개방형 인터커넥트 기술. PCIe 5.0 기반이지만 메모리 일관성(coherency)을 지원.

PCIe와의 차이점:

CXL은 메모리 일관성 유지 지원, PCIe는 데이터 전송만
CXL은 장치 간 공유 메모리 사용 가능
고성능 서버 및 메모리 확장형 시스템에 적합
5. Quantum-Classical Hybrid Computing의 개념과 활용 사례
개념:
양자 컴퓨터와 고전 컴퓨터를 결합하여 문제 해결을 최적화하는 방식. 양자는 병렬 계산 능력, 고전 컴퓨터는 제어 및 전처리 역할 수행.

활용 사례:

양자 머신러닝(QML): 특성 추출, 고전 전처리 + 양자 분류기
최적화 문제: 고전 알고리즘으로 부분 문제 분해 → 양자 컴퓨터로 계산
IBM Qiskit, D-Wave Ocean 등의 프레임워크로 실험 가능
6. Multi-Tenant Accelerator의 개념과 클라우드 환경에서의 활용
개념:
하나의 GPU, NPU, TPU 등 하드웨어 가속기를 여러 사용자가 공유하는 방식. 컨테이너/가상화 기술과 함께 사용되어 자원 효율성을 높임.

활용 사례:

클라우드 AI 서비스: 고객 별로 할당량을 나눠 GPU 사용
Kubernetes 기반 AI 작업 분산 처리
GPU/NPU 자원의 비용 절감, 스케줄링 최적화 가능
7. Space Computing(우주 컴퓨팅)의 개념과 지구 기반 컴퓨팅과의 차이점
개념:
우주 환경에서 사용하는 고신뢰성 컴퓨팅 아키텍처로, 방사선 내성, 극한 온도 대응, 제한된 에너지 환경 등 특수 조건에 최적화됨.

차이점:

내복구성(Fault Tolerance): 우주 방사선에 의한 오류를 감지·복구
저전력/소형화: 소형 위성(SmallSat), 탐사선에 적합
통신 지연 고려한 자율성 강화: 지상 제어 없이 판단 가능한 AI 내장 컴퓨팅 필요
NASA, ESA, SpaceX 등이 관련 기술 개발 중

- Instruction Packing(명령어 패킹)의 개념과 실행 효율성에 미치는 영향
개념:
Instruction Packing은 여러 명령어를 하나의 명령어 블록에 압축하거나 병렬로 배치해 처리하는 방식으로, 특히 VLIW(매우 긴 명령어 워드) 구조에서 활발하게 사용됩니다.

효율성 영향:

병렬 처리 가능한 명령어를 함께 묶어 명령어 처리율(IPC)을 향상
명령어 디코딩 단계의 효율성 증가
그러나 컴파일러가 병렬성 판단에 실패할 경우 성능 저하 가능
2. Dynamic Binary Translation(동적 바이너리 변환)의 개념과 성능 최적화 활용
개념:
실행 중인 프로그램의 바이너리 코드를 실시간으로 다른 ISA(Instruction Set Architecture)로 변환하는 기술로, 예를 들어 x86 프로그램을 ARM 기반에서 실행할 수 있게 함.

활용 예:

Apple의 Rosetta 2: x86 바이너리를 Apple Silicon(M1, M2)에서 실행
QEMU: 가상 머신에서 다른 플랫폼 코드 실행
최적화된 코드 캐시를 통해 재사용성 확보 및 속도 향상
3. Instruction Set Simulator(ISS)의 개념과 활용 사례
개념:
프로세서 명령어 세트를 소프트웨어로 시뮬레이션하여, 실제 하드웨어 없이도 명령어 실행을 테스트할 수 있는 환경을 제공합니다.

활용 사례:

SoC 개발 전 아키텍처 검증
교육용 아키텍처 실습 도구
펌웨어 디버깅 및 포팅 환경 구성
4. Operand Fetch Optimization(피연산자 가져오기 최적화) 기법
개념:
명령어 실행 시 필요한 피연산자를 더 빠르게 가져오기 위한 기법으로, 메모리 계층 및 레지스터 접근 효율을 개선합니다.

주요 기법:

피연산자 전방 전달(Forwarding)
레지스터 리네이밍으로 충돌 방지
프리페칭(prefetching): 메모리 피연산자를 미리 불러오기
피연산자 캐싱을 통해 반복 연산에서 재사용
5. Hardware Multithreading에서 Coarse-Grained vs Fine-Grained의 차이점
Coarse-Grained Multithreading:

한 스레드가 스톨 상태일 때만 전환
컨텍스트 스위칭 오버헤드 적음
단순한 구조, 하지만 스레드 간 자원 사용 비효율적
Fine-Grained Multithreading:

매 사이클마다 스레드 전환
스레드별 병렬 처리 효율 극대화
자원 활용은 좋지만, 복잡한 설계 필요
6. Loop Perforation(루프 생략) 기법과 성능-정확도 트레이드오프
개념:
반복문 내 일부 연산을 고의적으로 생략해 실행 시간을 줄이고 성능을 향상시키는 근사 컴퓨팅(Approximate Computing) 기법입니다.

트레이드오프:

성능: 루프 횟수 감소로 실행 시간 및 에너지 효율 향상
정확도: 계산 정밀도 저하 위험 → 이미지 처리, 영상 압축 등에서 유용
사용 조건: 결과의 정확도보다 빠른 처리 속도가 중요한 경우

- Conditional Move(조건부 이동) 명령어의 개념과 활용 사례
개념:
분기(branch)를 사용하지 않고, 조건에 따라 레지스터 값을 이동할지 말지를 결정하는 명령어입니다.
예: CMOV (x86), CSEL (ARM)

활용 사례:

분기 예측 실패로 인한 페널티 방지
짧은 조건문에서의 분기 제거로 파이프라인 효율 향상
보안적 측면에서도, 조건문을 통한 메모리 접근 분기를 줄여 측면 채널 공격 완화
2. Micro-Op Decomposition(마이크로 연산 분해)의 개념과 효율성 향상 방법
개념:
하나의 복합 명령어(CISC 명령어 등)를 여러 개의 단순한 마이크로 연산(Micro-Op)으로 분해하여 처리하는 기법입니다.

성능 향상 방법:

병렬 실행이 가능하도록 Out-of-Order Execution 유도
복잡한 명령어도 단순 연산으로 파이프라인에 적합하게 재구성
마이크로-OP 캐시(Micro-Op Cache)와 함께 사용하여 디코딩 비용 절감
3. Operand Bypass(피연산자 바이패스) 기법과 파이프라인 해저드 최소화
개념:
피연산자가 레지스터에 쓰이기 전에 ALU 결과를 다음 명령어로 직접 전달하여 데이터를 우회 전달하는 기법

효과:

데이터 해저드(특히 RAW, Read-After-Write) 방지
파이프라인 스톨(stall) 최소화
파이프라인 딜레이 없이 연속된 연산 처리 가능
4. Wide Issue Processor(광폭 발행 프로세서)의 개념과 기존 프로세서와의 차이점
개념:
한 사이클에 여러 명령어를 동시에 발행(issue)할 수 있는 프로세서로, ILP(Instruction Level Parallelism)를 극대화

차이점:

일반 프로세서보다 더 많은 디코더, 스케줄러, 실행 유닛 필요
명령어 간 종속성 분석 비용 증가
슈퍼스칼라 아키텍처의 확장형으로 볼 수 있음
5. NUMA 환경에서 Thread Affinity(스레드 친화성) 조정 방법
개념:
특정 스레드가 특정 CPU 또는 메모리 노드에 고정적으로 실행되도록 설정하여, NUMA 지연을 줄이는 전략

조정 방법:

numactl, taskset 명령어 또는 API (pthread_setaffinity_np)
운영체제 스케줄러 또는 사용자 수준에서 조정
데이터와 스레드의 물리적 근접성 유지 → 메모리 접근 지연 감소
6. PageRank 알고리즘과 메모리 접근 패턴의 관계
개념:
PageRank는 웹 그래프에서 노드의 중요도를 반복 계산하는 알고리즘으로, 희소 행렬 기반의 반복 연산을 수반

메모리 접근 패턴:

비순차적(random access) 접근이 빈번함
캐시 친화도가 낮아 캐시 미스율 증가
CSR(Compressed Sparse Row) 포맷 등의 압축 및 데이터 정렬을 통해 성능 최적화 가능
7. Hardware Managed Prefetching vs Software Managed Prefetching
Hardware Managed Prefetching:

CPU 내부 로직이 패턴을 감지하여 자동으로 프리페치 수행
반응이 빠르고 투명하지만 잘못된 프리페치는 오히려 낭비
Software Managed Prefetching:

개발자가 명시적으로 프리페치 명령어 삽입 (__builtin_prefetch 등)
하드웨어를 속일 수 없을 정도로 복잡한 패턴에서 유리
워크로드에 최적화된 프리페칭 가능

- Scratchpad Memory(스크래치패드 메모리)의 개념과 일반 캐시 메모리와의 차이점
개념:
프로그래머 또는 컴파일러가 명시적으로 관리하는 고속 로컬 메모리로, 임시 데이터를 저장하는 데 사용됨.

캐시와의 차이점:

캐시: 하드웨어가 자동으로 데이터를 교체·관리 (투명성 있음)
스크래치패드: 소프트웨어에서 직접 제어, 예측 가능한 성능 제공
캐시 미스 없음, 실시간 시스템에서 시간 결정성 보장
2. Multi-Banked DRAM vs Single-Bank DRAM의 차이점과 성능 영향
Single-Bank DRAM:

단일 메모리 뱅크로 구성
동시에 하나의 접근만 처리 가능 → 병렬성 부족
Multi-Banked DRAM:

여러 뱅크가 독립적으로 동작 → 동시 접근 처리 가능
뱅크 충돌(Bank Conflict)을 피하면 메모리 대역폭과 처리량 향상
3. Cache Coloring과 Heap Memory Allocation의 관계
Cache Coloring:

메모리 주소를 캐시 라인과 매핑되는 색상(color)으로 구분하여 충돌을 방지
Heap Allocation과의 관계:

잘못된 힙 할당은 동일한 캐시 색상으로 몰려 Conflict Miss 유발
색상 분산 정책(e.g. page coloring-aware allocator)을 사용하면 캐시 효율 개선
4. DRAM Access Granularity(접근 단위 크기)가 성능에 미치는 영향
Access Granularity: DRAM에서 한 번에 읽고 쓰는 최소 데이터 단위

성능 영향:

너무 작은 단위 → 과도한 오버헤드 발생
너무 큰 단위 → 불필요한 데이터 이동으로 메모리 낭비 및 대역폭 포화
워크로드에 맞는 최적 단위 선택 필요 (e.g., 스트리밍 vs 랜덤 접근)
5. Subthreshold Leakage(서브스레숄 누설 전류)가 메모리 설계에서 문제가 되는 이유
개념:
트랜지스터가 꺼진 상태에서도 전자가 흐르며 발생하는 전력 소모

문제점:

고밀도 DRAM/캐시 구조에서는 누설 전류 누적 → 정전류 소모 급증
모바일 및 IoT 장치에서 배터리 수명 단축, 발열 문제
해결책: 전력 게이팅, 멀티 Vt 설계, FinFET 등 누설 억제 기술 필요
6. Out-of-Bounds Memory Access(경계 초과 메모리 접근) 문제 방지 하드웨어 기술
개념:
버퍼나 배열의 유효 범위를 벗어난 메모리 접근

방지 기술:

Intel MPX(Memory Protection Extensions): 경계 레지스터 사용
ARM MTE(Memory Tagging Extension): 각 메모리 블록에 태그 부여
스택 보호 기법: Stack Canary, NX-Bit, AddressSanitizer 등 하드웨어/소프트웨어 연계 보호
7. Page Frame Reclamation(페이지 프레임 회수) 기법이란 무엇이며, 성능에 미치는 영향
개념:
사용 빈도가 낮은 페이지 프레임을 회수하여 새로운 페이지에 재할당하는 메모리 관리 전략

기법 종류:

LRU (Least Recently Used)
CLOCK
Working Set Model 기반 알고리즘
성능 영향:

잘못된 회수 → 페이지 폴트 증가 → 디스크 접근 → 심각한 성능 저하
효율적인 회수 정책은 스왑 최소화, 메모리 활용률 증가에 기여

- Dynamic Voltage and Frequency Scaling(DVFS)와 병렬 처리 성능 간의 관계
DVFS란:
CPU의 전압과 클럭 주파수를 동적으로 조절해 전력 소모를 줄이는 기술.

병렬 처리 성능과의 관계:

낮은 부하의 작업에서는 클럭을 낮춰 전력 절감 가능
병렬 스레드 수가 많을수록 성능 병목 지점이 DVFS에 의해 제한될 수 있음
멀티코어 DVFS 제어가 가능한 구조에서는 코어별로 효율 조절 가능 → 성능-전력 균형 최적화
2. Spinning vs Blocking 동기화 기법의 차이점과 활용 사례
Spinning (Busy Waiting):

스레드가 락을 획득할 때까지 계속 루프를 돌며 기다림
오버헤드 작지만 CPU 자원을 계속 사용
짧은 대기 시간에 적합 (e.g., Spinlock, 경량 락)
Blocking:

락을 얻지 못하면 커널 수준에서 sleep 상태로 진입
CPU 자원을 아끼지만 컨텍스트 스위칭 비용 존재
긴 대기 시간이나 OS 수준 스케줄링 필요 시 적합
3. Load-Linked / Store-Conditional (LL/SC) 명령어의 개념과 원자적 연산에서의 역할
LL/SC란:

LL: 메모리 주소에서 값을 읽고, 그 주소를 모니터링 상태로 설정
SC: 해당 주소가 다른 쓰기로 변경되지 않은 경우에만 쓰기 성공
역할:

락 없이 원자적 CAS(Compare-And-Swap)와 유사한 동작 가능
ABA 문제 회피, RISC 아키텍처에서 널리 사용
4. Non-Blocking Synchronization(비차단 동기화) vs 락 기반 동기화
비차단 동기화:

시스템이 어떤 스레드의 지연이나 중단에도 계속 진전 가능
대표적으로 CAS, LL/SC, Lock-free, Wait-free 기법 사용
데드락·기아 상태 방지, 병렬성 극대화
락 기반 동기화:

리소스 보호를 위해 공유 자원 접근 제한
데드락, 컨텍스트 스위칭, 우선순위 역전 등의 문제 존재
5. Vector Processing Unit(VPU)와 일반 SIMD의 차이점
VPU:

전용 벡터 레지스터를 통해 연속된 데이터 스트림 처리 최적화
벡터 명령어와 레지스터 기반, 고속 벡터 연산 특화
일반 SIMD:

범용 CPU 또는 GPU에서 제공되는 병렬 처리 단위
VPU보다 유연성은 높지만 연산 효율은 다소 떨어질 수 있음
차이 요약:

VPU는 연산 집약적 고정된 데이터 구조에 적합
SIMD는 범용 연산에서 병렬성 활용
6. Hardware Transactional Memory(HTM)에서 Conflict Detection(충돌 감지) 기법
Eager Conflict Detection:

트랜잭션 도중 실시간 감지, 충돌 발생 즉시 중단
빠른 롤백 가능하지만 불필요한 중단 가능성 있음
Lazy Conflict Detection:

트랜잭션 종료 시점에 충돌 여부 확인
성공률은 높지만, 롤백 비용 증가 가능
기타 구현 방식:

주소 기반 충돌 추적, 캐시 라인 모니터링 등을 통해 수행
7. Deep Learning Accelerator에서 Dataflow Optimization(데이터 흐름 최적화) 개념
개념:

딥러닝 연산 중 데이터 이동 경로를 최소화하여 연산 처리 효율을 높이는 설계 방법론
주요 기법:

Weight Stationary: 가중치 고정, 입력만 교체
Output Stationary: 출력 위치 고정 후 누적
Row/Column Stationary: DRAM 접근 줄이고 on-chip 데이터 재사용 극대화
효과:

에너지 소비 감소, 성능 향상, 온칩 버퍼 활용 극대화

- Fine-Grained Synchronization(세밀한 동기화)의 개념과 성능 최적화 효과
개념:

공유 자원의 일부만을 보호하여 병렬 처리를 세밀하게 제어하는 기법
예: 자료구조의 각 노드 또는 버킷마다 별도의 락 적용
성능 최적화 효과:

락 충돌 최소화 및 병렬성 향상
락 범위를 좁혀 컨텍스트 스위칭 및 스레드 대기 시간 감소
다중 코어에서 스레드 간 독립적 작업 병렬 처리 가능
2. Data Dependency Graph를 활용한 병렬 실행 최적화 기법
개념:

명령어나 작업 간의 의존 관계를 그래프로 표현하여 병렬 실행 가능성을 분석
노드 = 작업 / 엣지 = 의존성
활용 기법:

Topological Sort 기반 실행 스케줄링
동적 병렬 실행: 의존이 없는 노드들 병렬 실행
컴파일러 최적화나 스케줄러에서 활용하여 ILP 및 TLP 최대화
3. Dynamic Thread Migration(동적 스레드 이동) 기술과 성능 최적화 방법
개념:

실행 중인 스레드를 다른 코어로 이동시켜 자원 사용률을 최적화하는 기법
성능 최적화 방법:

부하 균형 유지: 과부하 코어에서 저부하 코어로 스레드 이동
NUMA 최적화: 데이터 접근 위치에 따라 가까운 메모리 노드 쪽 코어로 이동
전력 관리: 사용률 낮은 코어는 슬립 모드로 전환
4. DMA에서 Descriptor Ring Buffer 방식 vs Linked List 방식의 차이점
Descriptor Ring Buffer:

고정 크기의 순환 버퍼 구조
빠른 인덱스 처리, 예측 가능한 성능
고정된 큐 크기 = 확장성 제한
Linked List 방식:

동적으로 연결된 구조, 크기 제한 없음
유연하지만 오버헤드가 더 큼
고성능 장비에서 확장 필요 시 주로 사용
5. Non-Volatile Main Memory (NVMM)의 개념과 DRAM 대비 장점
개념:

전원이 꺼져도 데이터가 유지되는 주 메모리
예: Intel Optane, ReRAM, PCM
장점:

데이터 영속성: DRAM 대비 데이터 복원성 우수
빠른 액세스 속도: SSD보단 빠르고, DRAM보단 느림
데이터 무결성 및 시스템 재시작 시 빠른 복구 가능
6. USB-C와 Thunderbolt 4의 차이점과 성능 비교
USB-C:

물리적 포트 형태이며, USB 3.2/4 등 다양한 프로토콜 지원
최대 20~40Gbps (USB4 기준)
Thunderbolt 4:

USB-C 기반 고속 인터페이스, 모든 TB4는 USB4 호환
40Gbps 고정 속도, 다중 모니터/PCIe 전송 지원
더 강력한 보안, 전력, 호환성 요건 포함
7. NVMe Namespace의 개념과 멀티 테넌트 환경에서의 활용 사례
개념:

NVMe 장치 내에서 논리적으로 분리된 저장 공간 단위
하나의 컨트롤러에서 여러 Namespace로 나눠 관리 가능
활용 사례:

멀티 테넌트 클라우드에서 테넌트별 분리된 저장 공간 제공
보안, 성능 격리 가능하며, 컨테이너 또는 VM에 개별 할당
8. PCIe Switch Fabric의 개념과 클라우드 환경에서의 활용 사례
개념:

여러 PCIe 장치를 하나의 Switch Fabric으로 연결하여 고속 통신 가능
스토리지, 네트워크, GPU 등 리소스를 동적으로 할당
활용 사례:

클라우드 데이터센터에서 자원 가상화 및 동적 리소스 구성
하나의 서버에 다수 GPU/SSD 연결, 리소스 공유 최적화

- Interrupt Coalescing(인터럽트 합병) 기법이 Network I/O 성능에 미치는 영향
개념:

네트워크 카드에서 수많은 패킷 수신 시 인터럽트를 패킷마다 발생시키는 대신, 여러 패킷을 모아 한 번에 처리하여 인터럽트 빈도를 줄이는 기법.
성능 영향:

CPU 부하 감소 및 인터럽트 오버헤드 최소화
Throughput(처리량) 향상
단점으로는 Latency(지연 시간) 증가 가능
2. SR-IOV에서 Physical Function(PF)과 Virtual Function(VF)의 차이점
Physical Function (PF):

전체 하드웨어 기능을 제어할 수 있는 실제 하드웨어 장치의 기능.
VF를 생성/관리하는 권한 보유
Virtual Function (VF):

PF에서 파생된 가상화된 하드웨어 기능
각 VF는 VM이나 컨테이너에 할당되어 직접 네트워크 장치에 접근 가능
PF는 VF보다 더 많은 권한과 설정 기능을 가짐
3. Hardware Queueing(하드웨어 큐잉) 기법이 Storage I/O 성능에 미치는 영향
개념:

SSD나 NVMe 장치 내부에서 다중 명령어 큐를 병렬로 처리하여, I/O 병목을 줄이고 스루풋을 향상시키는 기술
성능 영향:

고속 SSD에서 수천 개의 IOPS를 효율적으로 스케줄링
CPU 대기 시간 줄이고 동시 I/O 처리량 향상
대용량 처리 시 지연 시간 감소
4. Multi-Path I/O(MPIO)와 Storage Load Balancing의 개념과 차이점
Multi-Path I/O (MPIO):

저장장치까지 여러 개의 경로(Path)를 제공하여, 장애 시 자동 대체, 경로 병렬화로 성능 향상
Storage Load Balancing:

여러 저장장치나 경로에 부하를 분산하여 I/O 트래픽을 최적화
차이점:

MPIO는 장애 대비 및 다중 경로 중복성 확보 중심
Load Balancing은 성능 최적화에 중점
5. RDMA에서 Zero Copy Data Transfer의 개념과 기존 TCP 전송과의 차이점
Zero Copy:

데이터 복사 없이 직접 메모리 간 전송하는 방식
CPU 개입 없이, NIC가 메모리에서 바로 읽고 씀
기존 TCP 전송과의 차이점:

TCP는 커널 버퍼 ↔ 사용자 공간 간 여러 번 복사
RDMA는 낮은 지연 시간과 CPU 사용률 절감, 고성능 네트워킹 실현
6. Spintronics(스핀트로닉스) 기반 컴퓨팅의 개념과 기존 CMOS 기술과의 차이점
개념:

전자의 전하가 아닌 스핀(Spin) 상태를 활용한 정보 처리 기술
자기저항(Magnetoresistance)을 기반으로 동작
기존 CMOS와 차이점:

비휘발성 특성, 저전력 소모, 열 발생 적음
CMOS는 전하 기반, Spintronics는 스핀 기반
아직은 속도와 집적도 측면에서 CMOS에 비해 미성숙
7. 3D-IC(3D 집적 회로)의 개념과 기존 2D IC 대비 장점과 단점
개념:

여러 개의 반도체 다이를 수직으로 쌓아 연결한 형태
TSV(Through-Silicon Via)를 통해 층 간 통신
장점:

공간 절약 및 고대역폭, 저지연 연결
전력 효율 및 성능 향상
단점:

열 방출 문제, 공정 복잡성, 테스트 및 수율 저하
생산 비용 상승

1. Probabilistic Computing(확률적 컴퓨팅)과 기존 불확정성 기반 컴퓨팅의 차이

개념:
확률적 컴퓨팅은 연산 결과가 단일한 결정값이 아닌 확률 분포 형태로 나타나는 컴퓨팅 패러다임이다. 이는 물리적 노이즈나 양자적 현상 등 자연적으로 발생하는 불확실성을 연산에 적극 활용한다.

기존 불확정성 기반 컴퓨팅과의 차이:
불확정성 기반 컴퓨팅은 시스템 내의 잡음, 전자적 오류 등을 제어하거나 보정하는 방식이 중심이며, 불확실성을 극복 대상으로 본다. 반면 확률적 컴퓨팅은 이러한 불확정성을 연산 자원으로 간주하여 계산에 활용하며, 특히 AI 및 머신러닝 분야에서 확률 분포 기반 학습에 적합하다.

2. Ising Model을 활용한 최적화 문제 해결

개념:
Ising 모델은 자성 입자의 상호작용을 기반으로 한 물리학 모델로, 각 입자는 스핀(±1)을 가지며 인접한 입자들과 에너지를 최소화하는 방향으로 배열된다.

활용:
이러한 특성을 이용해 조합 최적화 문제를 해결할 수 있으며, 에너지 함수가 최소가 되는 스핀 조합을 찾는 과정은 최적 해를 탐색하는 것과 동일하다. 이 접근은 양자 어닐링이나 뉴로모픽 하드웨어 기반 시스템에서 자주 사용된다.

3. Bio-Inspired Computing(생체 모방 컴퓨팅)의 개념과 활용 사례

개념:
생체 모방 컴퓨팅은 자연계에서 발견되는 생물학적 구조나 행동 양식을 모사한 알고리즘 또는 컴퓨팅 구조를 말한다. 대표적인 예로 신경망, 진화 알고리즘, 군집 지능(Swarm Intelligence) 등이 있다.

활용 사례:
	•	신경망(NN): 인간 두뇌 구조를 모사한 인공지능 시스템
	•	유전 알고리즘: 생물의 진화 원리를 최적화에 적용
	•	개미 군집 알고리즘: 최단 경로 탐색 문제 해결
	•	면역 알고리즘: 바이러스 탐지 및 사이버보안

4. Compute-in-Memory(CIM) vs Processing-in-Memory(PIM)

Compute-in-Memory(CIM):
메모리 셀 내부 또는 주변에서 데이터 연산이 직접 수행되는 구조이다. 곱셈 및 누적 같은 단순 연산을 메모리에서 직접 처리하여 에너지 효율과 속도를 극대화한다.

Processing-in-Memory(PIM):
메모리와 연산 유닛을 메모리 모듈 내부 또는 근처에 통합하여, 데이터 이동 없이 연산이 가능하도록 설계된 구조이다.

차이점:
	•	CIM은 연산을 메모리 셀 내에서 수행, 아날로그 회로 기반이 많다.
	•	PIM은 별도의 연산 유닛이 메모리에 병합, 디지털 방식이 많고 범용성도 높다.

5. Edge TPU의 개념과 기존 AI 가속기와의 차이

개념:
Edge TPU는 구글이 개발한 경량형 AI 전용 프로세서로, 엣지 디바이스에서 저전력으로 머신러닝 모델을 빠르게 실행하도록 설계되었다.

기존 AI 가속기 대비 차이점:
	•	GPU나 TPU 대비 연산 성능은 낮지만, 소형화, 저전력, 실시간 처리에 최적화되어 있다.
	•	클라우드가 아닌 엣지 환경에서 AI 모델을 구동함으로써 지연 시간 최소화 및 데이터 프라이버시 확보에 유리하다.

6. Resistive Switching Device(RRAM, Memristor)의 개념과 NAND 플래시와의 차이

개념:
Resistive RAM(RRAM)과 Memristor는 저항의 상태를 조절하여 정보를 저장하는 비휘발성 메모리 소자이다. 전압에 따라 저항값이 변화하며, 이를 통해 0과 1을 표현한다.

기존 NAND 플래시와의 차이:
	•	속도: RRAM은 쓰기/지우기 속도가 훨씬 빠름
	•	내구성: 더 많은 쓰기-지우기 사이클을 견딤
	•	집적도: 더 높은 집적 가능
	•	구조: 플로팅 게이트 기반의 NAND와 달리, 단순한 Crossbar 구조

7. Energy-Efficient AI Accelerator(저전력 AI 가속기)의 개념과 설계 원리

개념:
저전력 AI 가속기는 전력 소모를 최소화하면서도 인공지능 연산에 최적화된 프로세서이다. 모바일, 웨어러블, 엣지 디바이스 등에 적합하다.

설계 원리:
	•	전용 연산 유닛(NPU, MAC array) 탑재
	•	저전력 SRAM 사용
	•	데이터 재사용과 로컬 버퍼 활용
	•	압축, 양자화 기법 적용
	•	Power Gating, DVFS 활용

8. DNA Computing에서 Hybrid Molecular Electronics의 개념과 활용 가능성

개념:
Hybrid Molecular Electronics는 DNA 분자와 전자 소자를 결합한 형태로, DNA의 자기조립 특성과 전자의 흐름을 이용한 계산을 가능하게 한다.

활용 가능성:
	•	논리 회로 설계: DNA-전자소자 조합으로 나노미터 단위의 논리 연산 구현
	•	고밀도 저장: DNA의 정보 밀도를 활용한 초고밀도 메모리
	•	바이오 센서: 생화학 반응을 감지하고 전기 신호로 전환하는 생물-전자 인터페이스

1. Triple Modular Redundancy(TMR)와 신뢰성 향상

개념:
TMR은 동일한 연산 장치를 세 개 사용하여 동일한 작업을 병렬로 수행하고, 그 결과를 다수결 투표로 결정하는 방식의 장애 허용 기법이다.

신뢰성 향상 원리:
	•	하나의 모듈에 오류가 발생해도 나머지 두 모듈이 올바른 결과를 제공함으로써 결과의 정확성을 보장할 수 있다.
	•	주로 항공우주, 군수, 안전성 중요한 임베디드 시스템에서 사용된다.
	•	단점으로는 자원 소모가 크고 비용이 높다는 점이 있다.

2. FPGA의 Dynamic Partial Reconfiguration(DPR)

개념:
DPR은 FPGA의 특정 영역만을 동적으로 재구성할 수 있는 기능으로, 전체 시스템을 중단하지 않고 기능을 업데이트하거나 교체할 수 있게 한다.

활용 사례:
	•	멀티모드 통신: 한 부분을 다른 통신 프로토콜로 전환
	•	자원 최적화: 메모리 절약과 전력 절감을 위해 사용
	•	보안: 특정 시점에만 필요한 기능만 로드함으로써 역공학을 어렵게 함
	•	AI 가속기: 여러 AI 모델을 교대로 재구성하여 유연한 처리 가능

⸻

3. VLIW 아키텍처의 Bundled Execution

개념:
VLIW(Very Long Instruction Word)는 하나의 명령어에 여러 개의 연산 명령을 포함하고, 이들을 병렬로 실행하는 방식이다. Bundled Execution은 이러한 연산들을 묶음 단위로 정적 스케줄링하여 동시에 실행하는 구조다.

특징:
	•	하드웨어 복잡도 감소: 명령어 병렬화는 컴파일러가 담당
	•	성능 향상: 독립적인 명령어들을 병렬 실행
	•	제약: 동적 분기나 의존성 발생 시 성능 저하 가능
4. SIMD Vectorization과 Loop Unrolling을 통한 성능 최적화

SIMD(Vectorization):
하나의 명령으로 여러 데이터를 동시에 처리하는 방식으로, 벡터 레지스터를 이용하여 데이터 병렬성을 확보한다.

Loop Unrolling:
루프 반복 횟수를 줄이고 루프 내부의 연산을 확장하여 루프 오버헤드를 줄이는 최적화 기법이다.

조합 효과:
두 기술을 함께 사용하면 루프 실행 횟수를 줄이면서도 병렬 실행 효율을 높여 CPU의 명령어 처리율을 극대화할 수 있다. 벡터화 가능한 연산을 Unroll된 구조에 적용함으로써 SIMD 유닛의 활용도를 높인다.

5. Wavefront Scheduling과 GPU 활용

개념:
Wavefront Scheduling은 GPU에서 명령어를 그룹 단위(Wavefront 또는 Warp)로 실행 스케줄링하는 방식이다. 동일한 명령을 다수의 스레드에 동시에 적용한다.

GPU 활용:
	•	SIMT 구조 최적화: 같은 명령어를 여러 스레드에 적용할 때 효율적
	•	메모리 접근 분산: 메모리 병목을 줄이기 위한 실행 순서 조절
	•	레지스터 및 캐시 자원 최적 분배

Wavefront는 AMD, Warp는 NVIDIA에서 사용되는 개념이며, 스레드 묶음의 동기화 및 효율적 자원 사용에 핵심적인 역할을 한다.

6. Cache Pipeline Stalls의 원인과 해결 방안

원인:
	•	캐시 미스: 필요한 데이터가 캐시에 없을 때 메모리 접근 지연
	•	의존성 충돌: 이전 명령어 결과를 기다리는 경우
	•	TLB 미스: 가상 주소 변환 실패 시 추가 지연
	•	프리패처 부정확: 잘못된 선fetch로 인한 데이터 오염

해결 방법:
	•	하드웨어 프리패칭 향상
	•	Out-of-Order 실행 및 Speculative Execution
	•	Multi-Level Cache 구성
	•	Loop Blocking 등 캐시 친화적 코드 작성

7. Speculative Store Bypass(SB)와 보안 취약점의 관계

개념:
Speculative Store Bypass는 CPU가 명령어 순서와 무관하게 예측 실행(투기 실행)을 통해 성능을 높이기 위한 기술이다. 메모리 접근 시, store 명령보다 load 명령이 먼저 실행되는 것을 허용한다.

보안 취약점:
Meltdown/Spectre 계열의 취약점 중 하나로, 잘못된 투기 실행 결과가 캐시에 반영되면 사이드 채널 공격을 통해 민감한 정보 유출이 가능해진다.

대응 방안:
	•	CPU 마이크로코드 업데이트
	•	컴파일러 레벨 보안 패치 삽입
	•	OS 커널 보안 설정 강화

1. Hardware Prefetcher Throttling의 성능 영향

개념:
Hardware Prefetcher는 CPU가 예상되는 데이터를 미리 캐시에 불러오는 기능이다. Throttling은 이 기능을 조절하거나 제한하는 행위를 의미한다.

성능 영향:
	•	과도한 프리패칭: 불필요한 데이터를 캐시에 불러와 캐시 오염 및 대역폭 낭비 초래
	•	적절한 제한(Throttling): 메모리 병목 완화, 캐시 효율 향상
	•	워크로드에 따라 프리패처의 이득과 오버헤드가 다르므로, 동적 조절(Adaptive Throttling)이 중요하다.

2. Out-of-Order Execution과 Load-Store Queue(LSQ)의 역할

개념:
Out-of-Order Execution은 명령어의 순서를 재배열하여 CPU 자원을 효율적으로 사용하는 기술이다. 이 때 Load-Store Queue(LSQ)는 메모리 명령어들의 순서를 추적하고 의존성 충돌을 방지하는 역할을 한다.

LSQ의 역할:
	•	Load와 Store 명령 사이의 주소 의존성 확인
	•	Store 연산의 결과가 확정되기 전에도 Load가 진행 가능하게 함
	•	투명성 유지: Out-of-Order 실행 중에도 프로그램이 본래 순서대로 작동한 것처럼 보장

3. Address Generation Interlock(주소 생성 인터록)과 최적화 방법

개념:
주소 생성 인터록은 메모리 주소가 계산되기 전까지 Load/Store 명령이 지연되는 현상이다. 이는 레지스터 의존성 또는 연산 지연에 의해 발생한다.

최적화 방법:
	•	명령어 재배치: 주소 계산이 완료되기 전 다른 독립적인 명령 실행
	•	루프 전개(Unrolling): 연산 간 간격 확보
	•	주소 계산 병렬화: AGU(Address Generation Unit) 활용 극대화
	•	컴파일러 최적화: 스케줄링 및 의존성 분석을 통한 코드 최적화

4. Hybrid Cache Architecture(하이브리드 캐시 아키텍처)의 개념과 장점

개념:
하이브리드 캐시는 서로 다른 특성을 가진 메모리 기술(예: SRAM + STT-MRAM, DRAM + PCM 등)을 조합한 캐시 구조다.

장점:
	•	속도와 용량의 균형: SRAM의 고속성과 NVM의 고밀도 특성을 결합
	•	전력 효율: 저전력 비휘발성 메모리를 통해 소비전력 감소
	•	데이터 특성 기반 계층화: 자주 사용하는 데이터는 빠른 계층, 덜 사용되는 데이터는 느리지만 큰 계층에 배치

5. Direct Segment 기반 가상 메모리 구조와 기존 페이지 기반 방식의 차이

개념:
Direct Segment는 큰 연속 가상 주소 영역을 직접 물리 메모리에 매핑하는 구조로, 페이지 테이블 없이 주소 변환이 가능하다.

차이점:
	•	Direct Segment: 대용량 연속 메모리 사용 시 효율적, 변환 속도 빠름, TLB 미스 감소
	•	페이지 기반 메모리: 작은 단위로 분할, 메모리 단편화 줄이고 보호 기능이 강함

활용:
메모리 집약적인 HPC나 그래픽 처리 시스템에서 효율적이다.

6. Cache Miss Penalty 최소화를 위한 최적화 기법

기법들:
	•	멀티레벨 캐시 구성: L1, L2, L3 캐시 계층화
	•	하드웨어 프리패칭: 예상되는 데이터 사전 로딩
	•	소프트웨어 최적화: 데이터 지역성(locality) 고려한 코드 작성
	•	Victim Cache: L1 캐시에서 제거된 블록을 보존하여 재사용 가능성 확보
	•	Non-blocking Cache: 캐시 미스 중에도 다른 명령 처리 가능

7. Virtual Address Space Fragmentation의 원인과 해결 방법

원인:
	•	동적 할당 및 해제 반복 시, 가상 주소 공간에 비연속적인 빈 공간이 생성됨
	•	다양한 크기의 메모리 블록 할당으로 인해 큰 연속 공간 확보 불가

해결 방법:
	•	메모리 풀(pooling): 유사 크기의 메모리 블록을 묶어 관리
	•	슬래브 할당자(slab allocator): 커널 메모리 할당에 적합
	•	메모리 압축/스왑 기술: 공간 회수 및 재사용 촉진
	•	히트맵 기반 할당 전략: 연속 공간 우선 할당

1. Decoupled Access-Execute Memory Architecture(분리형 접근-실행 메모리 구조)의 개념과 활용 사례

개념:
Decoupled Access-Execute 아키텍처는 메모리 접근(access) 단계와 명령어 실행(execute) 단계를 분리하여 병렬 수행하는 방식이다. 각 단계는 별도의 파이프라인으로 구성되어, 메모리 병목을 줄이고 연산 자원의 활용을 극대화한다.

활용 사례:
	•	고성능 임베디드 시스템: 메모리 지연을 줄여 실시간 처리 성능 향상
	•	에너지 효율 최적화: access 단계는 저전력 코어, execute는 고성능 코어 활용
	•	AI 가속기: 데이터 fetch와 연산 분리를 통해 연산 자원 idle 최소화

2. Last-Level Cache(LLC)의 개념과 Multi-Core CPU에서의 역할

개념:
LLC는 보통 L3 캐시에 해당하며, CPU 코어들이 공유하는 마지막 계층의 캐시이다. 메인 메모리 접근을 최소화하기 위한 최후의 방어선 역할을 한다.

역할:
	•	코어 간 캐시 일관성 유지
	•	공유 데이터 접근 효율화
	•	캐시 누락 시 메모리 접근 지연 완화
	•	QoS 정책 구현 가능: 중요 스레드에 LLC 우선 할당

3. Bank-Level Parallelism(BLP)이 DRAM 성능에 미치는 영향

개념:
BLP는 DRAM 내부의 여러 은행(bank)을 병렬로 활용하여 동시에 다수의 메모리 요청을 처리할 수 있도록 하는 기능이다.

성능 영향:
	•	메모리 대역폭 향상
	•	대기 시간 감소
	•	메모리 명령어 충돌 회피 가능성 증가

효과적인 메모리 접근 패턴 설계와 스케줄링 알고리즘을 통해 BLP를 극대화할 수 있다.

4. Soft Errors(소프트 오류)의 메모리 안정성에 대한 영향 및 보완 기술

개념:
Soft Error는 방사선, 전자기 간섭 등에 의해 발생하는 일시적인 데이터 비트 오류로, 하드웨어 손상 없이 발생한다.

영향:
	•	메모리 데이터의 신뢰도 저하
	•	계산 결과 오류, 시스템 장애 가능성

보완 기술:
	•	ECC(Error Correction Code): 비트 오류 탐지 및 복구
	•	RAID-like 메모리 구조: 중복 저장을 통한 복구
	•	Shielding 기술: 외부 방사선 차단
	•	Parity Bit 기반 검사

5. Read Disturb Issue가 NAND Flash 수명에 미치는 영향과 해결 방법

개념:
Read Disturb는 인접 셀의 데이터를 반복적으로 읽는 과정에서, 해당 셀에 전기적 간섭이 발생해 의도치 않게 전하가 변하는 현상이다.

수명 영향:
	•	데이터 무결성 손상
	•	셀의 전기적 열화 가속화
	•	전체 블록의 조기 마모

해결 방법:
	•	에러 정정 코드(ECC) 강화
	•	리프레시(refresh) 기법: 일정 주기마다 재기록
	•	읽기 횟수 제한 관리
	•	웨어 레벨링(Wear Leveling) 강화

6. Transparent Memory Compression(투명 메모리 압축)의 개념과 최적화 방법

개념:
투명 메모리 압축은 운영체제나 애플리케이션의 개입 없이 하드웨어 또는 시스템 소프트웨어 수준에서 실시간으로 메모리 데이터를 압축하여 사용 가능한 메모리 용량을 증가시키는 기술이다.

최적화 방법:
	•	압축 알고리즘 선택: LZ4, Zstandard 등 속도-압축률 균형
	•	캐시/버퍼와 연계: 압축 데이터에 대한 접근을 빠르게 처리
	•	압축 히트율 모니터링: 압축 효율이 낮을 경우 비활성화
	•	압축 블록 크기 조절: 페이지 단위 또는 객체 단위 압축

7. Multi-Threaded Processor에서 TLP와 ILP의 관계

개념:
	•	TLP(Thread-Level Parallelism): 여러 스레드를 동시에 실행하는 병렬성
	•	ILP(Instruction-Level Parallelism): 하나의 스레드 내에서 여러 명령을 병렬 처리

관계 및 병행 활용:
	•	두 개념은 상호 보완적이다. ILP는 하나의 스레드 내 성능 최적화, TLP는 여러 스레드를 통한 자원 활용도 향상
	•	ILP가 한계에 부딪히면 TLP로 스루풋을 향상시키고, 반대로 스레드 수가 제한되면 ILP로 세밀한 성능 개선을 추구한다.

1. Graph Analytics에서 PageRank Algorithm의 병렬 실행 특수성

개념:
PageRank는 웹 페이지의 중요도를 계산하는 그래프 알고리즘으로, 각 노드의 순위를 인접 노드의 값에 따라 반복적으로 업데이트한다.

병렬 실행의 특수성:
	•	고르게 분산된 작업이 아님: 일부 노드는 연결성이 높아 계산량이 집중됨
	•	비동기 갱신 불안정성: 노드 순위 업데이트 순서에 따라 수렴 속도가 달라짐
	•	동기화 필요성: 전체 그래프의 일정 수준 동기화 없이는 정확한 수렴 어려움
	•	분산 환경에서는 메시지 오버헤드가 큼: 성능 병목 유발

이를 해결하기 위해 비동기 알고리즘, 그래프 분할 최적화, sparse matrix 구조 등을 병렬화 설계에 활용한다.

2. Global Synchronization이 대규모 병렬 시스템에서 성능 저하를 유발하는 이유

개념:
Global Synchronization은 병렬 프로세서 또는 스레드들이 특정 지점에서 서로 기다리며 동기화를 맞추는 과정이다.

성능 저하 요인:
	•	느린 노드 하나가 전체 성능을 지연시키는 병목 발생
	•	불필요한 대기 시간 증가로 자원 활용률 저하
	•	대규모 시스템에서는 네트워크 트래픽 과다 발생
	•	빈번한 동기화는 캐시 무효화, 메모리 액세스 비용 상승

이를 개선하기 위해 지역 동기화(Local Barrier), 비동기 프로그래밍 모델, 데이터 의존성 최소화 전략 등이 사용된다.

3. Scalable Weak Memory Consistency Model(확장 가능한 약한 메모리 일관성 모델)

개념:
약한 메모리 일관성(Weak Memory Consistency)은 다중 프로세서 시스템에서 성능을 위해 메모리 접근 순서를 완전히 보장하지 않는 모델이다. Scalable 모델은 이를 시스템 규모 증가에도 유지 가능한 구조로 확장한 것이다.

특징:
	•	연산 재배열 허용: 성능 향상을 위해 순서 완화
	•	동기화 연산 필요: 일관성 확보 시점 명시
	•	스케일 확장 시 데이터 레이스 방지 어려움
	•	하드웨어와 컴파일러가 협력하여 보장

사용 예시로 ARM, POWER 아키텍처의 weak consistency 모델이 있으며, 이를 관리하기 위한 메모리 장벽(memory barrier)이 필요하다.

4. Persistent Memory 기반 시스템에서 데이터 일관성 유지 방법

개념:
Persistent Memory(NVRAM 등)는 전원이 꺼져도 데이터를 유지하는 메모리로, DRAM과 스토리지의 중간 형태이다.

일관성 유지 방법:
	•	Atomic Write: 부분 쓰기로 인한 중간 상태 방지
	•	Write-Ahead Logging(WAL): 변경 내용을 로그에 먼저 기록 후 적용
	•	Copy-on-Write(COW): 기존 데이터 보존 후 새로운 버전 생성
	•	Flush 및 Memory Barrier: CPU 캐시 데이터를 PMEM에 강제로 반영
	•	PMDK 같은 라이브러리: 고수준 일관성 제공

5. Cooperative Cache Management를 통한 Cache Contention 완화 기법

개념:
Cooperative Cache Management는 멀티코어 환경에서 각 코어의 캐시가 충돌(cache contention)하지 않도록, 협력적으로 캐시 사용 정책을 조절하는 방식이다.

기법:
	•	캐시 파티셔닝: 각 코어에 캐시 영역 분리 할당
	•	우선순위 기반 캐시 교체: 중요 프로세스 캐시 보존
	•	교차 모니터링: 코어 간 캐시 사용량을 공유하여 동적으로 조절
	•	QoS-aware 캐시 정책: 실시간 시스템에서 일정 수준 캐시 성능 보장

이 방식은 공유 LLC 환경에서 성능을 안정화시키는 데 유용하다.

6. Hierarchical Coherency Domains(계층적 일관성 도메인)의 개념과 활용

개념:
계층적 일관성 도메인은 멀티코어 또는 멀티소켓 시스템에서 캐시 일관성을 유지하는 범위를 계층적으로 분리하여 처리하는 방식이다.

활용:
	•	로컬 도메인: 동일 코어 또는 동일 클러스터 내 일관성 유지 (속도 빠름)
	•	글로벌 도메인: 클러스터 간 일관성 유지 (속도 느리나 범위 넓음)
	•	NUMA 구조 최적화: 노드 간 일관성 유지 비용 감소
	•	ARM’s AMBA, Intel의 CXL 등에서 지원

효율적인 트래픽 관리, 확장성 향상, 성능과 일관성의 균형 유지에 기여한다.

1. Dynamic Voltage and Frequency Scaling(DVFS)과 캐시 성능의 관계

개념:
DVFS는 프로세서의 전압과 클록 주파수를 동적으로 조절하여 전력 소비를 줄이고 발열을 제어하는 기술이다.

캐시 성능과의 관계:
	•	클록 속도 저하 시: 캐시 접근 지연(latency)이 길어져 메모리 병목 발생 가능
	•	전압 저하 시: 캐시 동작 안정성, 신호 무결성에 영향
	•	주파수 변화에 따른 미스 패턴 변화: 낮은 주파수에서는 미스율 증가 가능
	•	전력 절감을 위한 캐시 크기 축소나 프리패처 비활성화는 성능에 직접적 영향

따라서 DVFS는 성능-전력 균형을 고려한 설계 및 제어 알고리즘이 중요하다.

2. Hardware Barrier Synchronization(하드웨어 장벽 동기화) 개념과 성능 향상 효과

개념:
Hardware Barrier Synchronization은 병렬 처리 시 다수의 스레드가 특정 지점에서 동기화될 수 있도록 하드웨어 수준에서 빠르게 동기화 처리하는 방식이다.

성능 효과:
	•	소프트웨어보다 빠른 동기화 시간
	•	낮은 지연 및 오버헤드로 높은 처리량 유지
	•	캐시 일관성 유지에 기여
	•	특정 연산 종료 후 다음 단계 일괄 실행 가능

주로 고성능 컴퓨팅(HPC), GPGPU, 병렬 프로세서에서 효율적이다.

3. NUMA와 GPU Unified Memory의 차이점과 활용 사례

NUMA(Non-Uniform Memory Access):
CPU가 접근하는 메모리 위치에 따라 지연 시간이 달라지는 구조로, 다중 노드 시스템에서 로컬 메모리 접근이 빠르고, 원격 노드 접근은 느림.

GPU Unified Memory:
CPU와 GPU 간 메모리 주소 공간을 공유하는 기술로, 명시적 데이터 복사 없이 자동으로 메모리 동기화를 지원한다.

차이점 및 활용:
	•	NUMA는 CPU 메모리 구조 최적화에 활용, OS와 앱 수준에서 데이터 배치 중요
	•	Unified Memory는 GPU 연산의 코드 간결화, 데이터 이동 자동화, CUDA 환경에서 유리
	•	NUMA는 분산 메모리 구조, Unified Memory는 공유 메모리 구조에 가까움

4. PCIe Resizable BAR(Resizable Base Address Register)의 개념과 성능 향상 효과

개념:
Resizable BAR는 PCIe 장치(특히 GPU)가 한 번에 접근 가능한 메모리 주소 범위를 확장할 수 있도록 하는 기능이다.

성능 효과:
	•	기존에는 GPU가 BAR를 통해 시스템 메모리를 256MB씩만 접근
	•	Resizable BAR 활성화 시 전체 VRAM 혹은 큰 데이터 블록 접근 가능
	•	대규모 텍스처, 모델 데이터 연산 속도 향상
	•	I/O 병목 완화 및 연속 데이터 처리 효율 증가

특히 게임, 영상 처리, AI 추론 등에 실질적인 성능 개선을 가져올 수 있다.

5. Adaptive Routing이 NoC(Network-on-Chip) 성능에 미치는 영향

개념:
Adaptive Routing은 NoC에서 패킷 전송 시 현재 네트워크 혼잡 상황을 고려하여 경로를 동적으로 선택하는 기법이다.

성능 영향:
	•	혼잡 회피: 병목 구간 우회하여 전송 지연 최소화
	•	데이터 흐름 균형 유지: 병렬 처리량 향상
	•	에너지 효율 증대: 불필요한 대기 및 재전송 감소
	•	신뢰성 향상: 고장난 경로 회피 가능

Static Routing에 비해 설계 복잡도는 증가하지만, 대규모 멀티코어 환경에서 필수적이다.

6. Optically Interconnected Memory Systems(광 연결 메모리 시스템)의 개념과 전자 기반 연결과의 차이점

개념:
광 연결 메모리 시스템은 CPU와 메모리, 또는 메모리끼리의 통신을 광학적 신호(레이저 등)를 사용해 연결하는 구조다.

기존 전자 기반과 차이점:
	•	높은 전송 대역폭: 전자보다 광이 훨씬 넓은 주파수 대역 사용
	•	낮은 지연 시간: 거리 증가에 따른 신호 감쇠가 적음
	•	발열 감소: 전류 대신 광 신호 사용으로 발열 적음
	•	간섭과 크로스토크 최소화

단점은 아직까지 비용이 높고, 집적도 및 상용화 기술이 제한적이라는 점이다. 향후 고성능 서버, 데이터센터, 메모리 중심 컴퓨팅에서 유망하다.

1. RDMA에서 Memory Registration과 Key Caching이 성능에 미치는 영향

개념:
RDMA(Remote Direct Memory Access)는 CPU 개입 없이 메모리 간 직접 데이터 전송을 가능하게 하는 기술이다. 이때 Memory Registration은 RDMA에서 사용할 메모리 버퍼를 등록하여 메모리 키(Memory Key)를 발급받는 과정이고, Key Caching은 이 키를 재사용하여 등록 오버헤드를 줄이는 기법이다.

성능 영향:
	•	Memory Registration은 고비용 작업으로, 빈번할수록 성능 저하 발생
	•	Key Caching을 통해 등록 재사용이 가능하여 지연 시간 및 CPU 사용률 감소
	•	실시간 처리 및 고빈도 전송 시스템에서는 캐싱이 성능 유지에 핵심 역할

2. Zoned Namespaces(ZNS)의 개념과 기존 블록 스토리지와의 차이점

개념:
ZNS는 SSD를 Zone 단위로 나누어 순차 쓰기 방식으로 운용하는 차세대 스토리지 기술이다. 일반 블록 스토리지는 랜덤 쓰기 기반으로, 플래시의 수명 및 성능에 영향을 준다.

차이점:
	•	ZNS는 순차 쓰기 강제로 NAND 수명 연장
	•	GC(Garbage Collection) 비용 최소화
	•	호스트가 Zone 쓰기 순서를 직접 관리
	•	기존 방식은 SSD가 쓰기 및 GC를 내부적으로 수행하여 비효율 발생

ZNS는 저장 효율과 예측 가능한 성능 유지에 강점이 있으며, 대규모 로그 저장 시스템이나 데이터센터에 적합하다.

3. I/O Stack Bypassing의 개념과 성능 최적화 사례

개념:
I/O Stack Bypassing은 운영체제의 일반적인 파일 시스템 및 I/O 스택을 우회하여 사용자 공간에서 직접 디바이스와 통신하는 방식이다.

성능 최적화 사례:
	•	SPDK(Storage Performance Development Kit): 사용자 공간에서 NVMe SSD 직접 제어
	•	DPDK(Data Plane Development Kit): 네트워크 패킷 처리의 스택 우회
	•	CPU 개입, context switching, system call 오버헤드가 줄어들어 지연 시간 및 처리량 대폭 개선

이 방식은 고성능 스토리지 및 네트워크 시스템에서 특히 효과적이다.

4. Zero Copy Networking과 TCP/IP Offloading Engine(TOE)의 차이점

Zero Copy Networking:
데이터가 애플리케이션 ↔ 커널 ↔ 네트워크 장치 간 이동 시, 복사 없이 하나의 버퍼에서 직접 전송되도록 하는 기법. CPU와 메모리 대역폭을 절약할 수 있다.

TCP/IP Offloading Engine(TOE):
네트워크 카드(NIC)가 TCP/IP 프로토콜 스택 일부 또는 전체를 하드웨어에서 처리함으로써 CPU 부하를 줄이는 기술이다.

차이점:
	•	Zero Copy는 데이터 이동 최적화, TOE는 프로토콜 처리 최적화
	•	Zero Copy는 소프트웨어 구조에 초점, TOE는 하드웨어 기능에 초점
	•	두 기술을 병행하면 고성능 네트워크 처리에 시너지 효과 가능

5. Persistent Memory 기반 스토리지의 Mixed Workload Optimization

개념:
Mixed Workload는 읽기/쓰기, 랜덤/순차, 대/소용량 요청이 혼합된 형태를 의미하며, Persistent Memory(NVDIMM, Intel Optane 등)는 이들에 모두 대응 가능한 스토리지다.

최적화 기법:
	•	쓰기 집중형 워크로드: 버퍼링 + WAL(Write-Ahead Logging) 적용
	•	읽기 중심: 캐싱, 미리 읽기(Prefetching)
	•	혼합형: 접근 패턴 분석 후 쓰기-읽기 분리 경로 설계
	•	파일 시스템 단 최적화(FS-DAX 등): CPU ↔ 메모리 직접 경로 제공

지속 메모리는 낮은 지연성과 높은 IOPS 특성 덕분에 혼합 워크로드 처리에 특히 적합하다.

6. Hardware vs Software Message Passing의 차이점과 활용 사례

Hardware Message Passing:
하드웨어 수준에서 처리되는 메시지 전달 방식으로, 네트워크 인터페이스 컨트롤러(NIC)나 전용 메시지 버스에서 바로 메시지를 전송/수신한다.

Software Message Passing:
운영체제나 미들웨어(예: MPI, gRPC)를 통해 메시지를 송수신하는 방식으로, 유연성은 높지만 CPU 개입 및 오버헤드 존재.

차이점 및 활용:
	•	하드웨어 방식: 고성능 컴퓨팅, 실시간 시스템, FPGA 통신 등에 적합
	•	소프트웨어 방식: 분산 시스템, 클라우드 마이크로서비스, 일반 IPC 등에서 사용
	•	하드웨어는 지연이 짧고 효율적, 소프트웨어는 이식성과 유연성이 강점

1. Inline Data Deduplication(인라인 데이터 중복 제거)이 SSD 성능에 미치는 영향

개념:
인라인 중복 제거는 데이터를 저장하기 전에 실시간으로 중복 여부를 확인하고 중복된 데이터는 저장하지 않고 참조로 대체하는 방식이다.

SSD 성능 영향:
	•	장점: 저장 공간 절약, 쓰기 횟수 감소로 수명 연장
	•	단점: 실시간 비교 및 해시 처리로 인한 쓰기 지연 증가 가능
	•	고성능 컨트롤러와 병렬 해시 계산을 통해 지연을 최소화하면 효과적
	•	인라인 방식은 오프라인(배치) 방식에 비해 I/O 처리 경로 최적화가 핵심

2. 3D TSV(Through-Silicon Via) 기술과 2.5D 패키징 기술 비교

3D TSV:
칩을 수직으로 적층하고, 실리콘을 관통하는 미세 구멍을 통해 층간 연결하는 기술이다. 실질적인 3차원 적층 회로로 전송 거리를 줄인다.

2.5D 패키징:
여러 칩을 실리콘 인터포저 위에 나란히 배치하고, 인터포저를 통해 신호를 주고받는 구조이다. 물리적으로는 평면 배치이나 연결은 고밀도다.

장단점 비교:
	•	3D TSV: 고집적, 고속 전송, 공간 절약 / 발열 관리 어려움, 비용 높음
	•	2.5D: 설계 유연성, 발열 분산 용이 / 면적 증가, 성능은 TSV보다 낮음
	•	활용: TSV는 고대역폭 메모리(HBM), AI 칩에, 2.5D는 GPU, FPGA 인터페이스에 적합

3. Coarse-Grained Reconfigurable Architecture(CGRA)와 FPGA의 차이점

CGRA:
연산 블록을 coarse한 단위로 구성하고, 데이터 경로를 동적으로 설정하여 프로그래밍 가능한 구조이다. 고정된 ALU와 레지스터 그룹을 반복적으로 연결한 구조로 소프트웨어 제어를 통해 동작 방식이 바뀐다.

FPGA와의 차이점:
	•	FPGA는 게이트 수준(미세)에서 회로를 직접 구성하는 반면
	•	CGRA는 연산 단위(거칠게)에서 기능 블록을 설정하고 연결
	•	CGRA는 재구성 속도가 빠르고 에너지 효율이 높으며 특정 도메인(영상, 신호처리 등)에 최적화됨
	•	FPGA는 범용성이 높으나 복잡하고 구성 시간이 오래 걸림

4. Near-Sensor Computing(센서 근접 컴퓨팅)의 개념과 엣지 AI 활용 가능성

개념:
센서 근접 컴퓨팅은 데이터가 생성되는 센서 가까이에서 전처리 또는 추론을 수행하여, 원거리 전송을 줄이고 실시간성을 확보하는 방식이다.

엣지 AI 활용:
	•	영상, 음성, 생체 신호 등 대용량 데이터의 불필요한 전송 감소
	•	지연 최소화 및 프라이버시 보호 강화
	•	저전력 MCU 기반 AI 추론 연산 수행 가능
	•	활용 예시: 스마트 카메라, 헬스케어 웨어러블, IoT 디바이스 등

5. Reversible Computing(가역 컴퓨팅)의 개념과 에너지 효율성과의 관계

개념:
가역 컴퓨팅은 모든 연산이 입력을 유실하지 않고 되돌릴 수 있는 계산 모델로, 이론적으로 열적 에너지 소모가 거의 없다. 이는 랜더의 열역학적 이론에 근거한다.

에너지 효율성:
	•	정보 소실 없이 계산하면 에너지 손실도 없어진다는 원리
	•	CMOS처럼 불필요한 전하 방출이 없는 계산 회로 설계
	•	양자 컴퓨팅과 밀접한 개념으로, 초저전력 시스템에 활용 가능성
	•	단점은 회로 복잡성 증가, 논리 설계의 어려움

6. Quantum Tunneling Transistor(양자 터널링 트랜지스터)와 CMOS의 차이점

개념:
Quantum Tunneling Transistor는 전자가 장벽을 터널링 효과를 통해 통과하는 양자 현상을 이용한 소자이다.

CMOS와의 차이점:
	•	구조: 터널링 기반으로 동작, 전압이 아니라 양자 확률에 의존
	•	장점: 스위칭 에너지 극소화, 초저전력
	•	단점: 공정 기술 난이도 높고 상온에서 안정성 확보 어려움
	•	CMOS는 전하 축적과 방전을 통한 전통적 스위칭 방식이며, 현재는 대부분의 디지털 회로에 사용됨

7. DNA Storage에서 Information Density 증가가 HDD/SSD 대비 갖는 이점

개념:
DNA Storage는 데이터를 4개의 염기서열(ATCG)을 조합하여 분자 수준으로 저장하는 차세대 저장 기술이다.

Information Density 측면 이점:
	•	초고밀도 저장: 수 mm³당 수 페타바이트 이상 저장 가능
	•	장기 보존: DNA는 수천 년간 안정적 보관 가능
	•	물리적 공간 극소화: 서버실 전체가 test tube 하나로 대체 가능
	•	HDD/SSD 대비 1,000배 이상 밀도 우수

단점은 아직 쓰기/읽기 속도, 비용, 에러율이 상용화에 걸림돌이다.

1. Stochastic Computing(확률적 컴퓨팅)의 개념과 AI 모델 학습에서의 활용

개념:
Stochastic Computing은 데이터를 이진수 대신 확률 분포로 표현하고, 연산을 확률 기반 논리로 수행하는 계산 방식이다. 예를 들어, 0.75라는 값을 100비트 중 75비트가 1인 시퀀스로 표현한다.

AI 학습에서의 활용:
	•	곱셈, 누산 등 연산을 단순한 AND, MUX로 구현할 수 있어 하드웨어 비용 절감
	•	노이즈 내성 강함 → 딥러닝의 확률적 특성과 잘 어울림
	•	저전력/고속 계산 가능
	•	제약이 많은 엣지 AI 또는 초경량 뉴로모픽 칩에 적용 가능성

다만 정확도와 표현 정밀도가 낮아 정밀 제어가 필요한 연산에는 부적합할 수 있다.

2. Self-Healing Hardware(자가 치유 하드웨어)의 개념과 고장 허용 컴퓨팅에서의 활용

개념:
자가 치유 하드웨어는 회로 내에서 결함이 발생했을 때 자동으로 대체 경로를 형성하거나 오류를 복구하여 기능을 유지하는 시스템이다.

활용:
	•	리던던시(중복 회로)를 활용해 결함 감지 시 즉시 우회
	•	기생 경로 활성화 또는 재배선을 통해 오류 대응
	•	고장 허용 시스템(Fault-Tolerant Systems)에서 생존성 향상
	•	우주, 군사, 의료 등 극한 환경에서의 신뢰성 확보에 매우 중요

미래에는 AI와 결합해 예측적 자가 치유 시스템으로 진화 가능성도 크다.

3. Photonic Neuromorphic Computing(광 뉴로모픽 컴퓨팅)의 개념과 기존 디지털 뉴로모픽과의 차이점

개념:
광 뉴로모픽 컴퓨팅은 뉴런과 시냅스를 광학 소자(레이저, 위상 변조기 등)로 구현하여 빛의 간섭, 위상, 세기를 활용해 정보를 처리하는 방식이다.

기존 디지털 뉴로모픽과 차이점:
	•	속도: 빛은 전자보다 빠르게 전파되어 계산 속도 우수
	•	병렬성: 광학 경로는 교차 없이 다중 신호 전달 가능
	•	전력 효율: 낮은 발열, 고효율
	•	복잡성: 회로 구현이 어려워 기술 난이도 높음

빛 기반 연산은 초고속 신호처리와 AI 추론 가속기에 적합하며, 미래형 AI 하드웨어로 주목받고 있다.

4. Molecular Electronics(분자 전자공학)의 개념과 기존 반도체 소자와의 차이점

개념:
Molecular Electronics는 개별 분자 또는 분자 집합체를 논리 게이트, 트랜지스터, 메모리 셀 등 전자 소자로 활용하는 기술이다.

차이점:
	•	크기: 나노미터 이하의 초미세 단위로 소자 집적 가능
	•	제어 방식: 전자 수송, 양자 터널링, 분자 진동 등 새로운 물리 기반
	•	집적도: 기존 CMOS의 한계를 넘어 초고밀도 회로 구현 가능
	•	전통 반도체와 달리 상온 안정성 확보와 신뢰성 확보가 관건

궁극적으로는 모든 전자 회로를 화학적으로 합성하는 기술로 이어질 수 있으며, 유연 전자기기, 생체이식 디바이스 등에서 가능성을 보이고 있다.